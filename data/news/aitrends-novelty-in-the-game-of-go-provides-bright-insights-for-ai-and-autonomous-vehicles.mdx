---
title: 'Novelty In The Game Of Go Provides Bright Insights For AI And Autonomous Vehicles'
date: 2021-10-28
categories: ['AI Trends Insider on Autonomy', 'Robotics', 'Self Driving Cars', 'AI Trends Insider', 'autonomous cars', 'robot cars', 'robot taxis', 'robotics', 'self-driving cars']
url: https://www.aitrends.com/ai-insider/novelty-in-the-game-of-go-provides-bright-insights-for-ai-and-autonomous-vehicles/
company: AITrends
authors: ['Allison Proffitt']
summary: 'By Lance Eliot, the AI Trends Insider   We already expect that humans to exhibit flashes of brilliance. It might not happen all the time, but the act itself is welcomed and not altogether disturbing when it occurs.    What about when Artificial Intelligence (AI) seems to display an act of novelty? Any such instance is bound to get our attention; […]'
---


![](https://www.aitrends.com/wp-
content/uploads/2021/10/10-29GameofGo-2-100x70.jpeg)

_By Lance Eliot, the AI Trends Insider_

We already expect that humans to exhibit flashes of brilliance. It might not
happen all the time, but the act itself is welcomed and not altogether
disturbing when it occurs.

What about when Artificial Intelligence (AI) seems to display an act of
novelty? Any such instance is bound to get our attention; questions arise
right away.

How did the AI come up with the apparent out-of-the-blue insight or novel
indication? Was it a mistake, or did it fit within the parameters of what the
AI was expected to produce? There is also the immediate consideration of
whether the AI somehow is slipping toward the precipice of becoming sentient.

Please be aware that no AI system in existence is anywhere close to reaching
sentience, despite the claims and falsehoods tossed around in the media. As
such, if today’s AI seems to do something that appears to be a novel act, you
should not leap to the conclusion that this is a sign of human insight within
technology or the emergence of human ingenuity among AI.

That’s an anthropomorphic bridge too far.

The reality is that any such AI “insightful” novelties are based on various
concrete computational algorithms and tangible data-based pattern matching.

In today’s column, we’ll be taking a close look at an example of an AI-powered
novel act, illustrated via the game of Go, and relate these facets to the
advent of AI-based true self-driving cars as a means of understanding the AI-
versus-human related ramifications.

Realize that the capacity to spot or suggest a novelty is being done
methodically by an AI system, while, in contrast, no one can say for sure how
humans can devise novel thoughts or intuitions.

Perhaps we too are bound by some internal mechanistic-like facets, or maybe
there is something else going on. Someday, hopefully, we will crack open the
secret inner workings of the mind and finally know how we think. I suppose it
might undercut the mystery and magical aura that oftentimes goes along with
those of us that have moments of outside-the-box visions, though I’d trade
that enigma to know how the cups-and-balls trickery truly functions (going
behind the curtain, as it were).

Speaking of novelty, a famous game match involving the playing of Go can
provide useful illumination on this overall topic.

Go is a popular board game in the same complexity category as chess. Arguments
are made about which is tougher, chess or Go, but I’m not going to get mired
into that morass. For the sake of civil discussion, the key point is that Go
is highly complex and requires intense mental concentration especially at the
tournament level.

Generally, Go consists of trying to capture territory on a standard Go board,
consisting of a 19 by 19 grid of intersecting lines. For those of you that
have never tried playing Go, the closest similar kind of game might be the
connect-the-dots that you played in childhood, which involves grabbing up
territory, though Go is magnitudes more involved.

There is no need for you to know anything in particular about Go to get the
gist of what will be discussed next regarding the act of human novelty and the
act of AI novelty.

A famous Go competition took place about four years ago that pitted one of the
world’s top professional Go players, Lee Sedol, against an AI program that had
been crafted to play Go, coined as AlphaGo. There is a riveting documentary
about the contest and plenty of write-ups and online videos that have in
detail covered the match, including post-game analysis.

Put yourself back in time to 2016 and relive what happened.

Most AI developers did not anticipate that the AI of that time would be
proficient enough to beat a top Go player. Sure, AI had already been able to
best some top chess players, and thus offered a glimmer of expectation that Go
would eventually be equally undertaken, but there weren’t any Go programs that
had been able to compete at the pinnacle levels of human Go players. Most
expected that it would probably be around the year 2020 or so before the
capabilities of AI would be sufficient to compete in world-class Go
tournaments.

**DeepMind Created AlphaGo Using Deep Learning, Machine Learning**

A small-sized tech company named DeepMind Technologies devised the AlphaGo AI
playing system (the firm was later acquired by Google). Using techniques from
Machine Learning and Deep Learning, the AlphaGo program was being revamped and
adjusted right up to the actual tournament, a typical kind of last-ditch
developer contortions that many of us have done when trying to get the last
bit of added edge into something that is about to be demonstrated.

This was a monumental competition that had garnered global interest.

Human players of Go were doubtful that the AlphaGo program would win. Many AI
techies were doubtful that AlphaGo would win. Even the AlphaGo developers were
unsure of how well the program would do, including the stay-awake-at-night
fears that the AlphaGo program would hit a bug or go into a kind of delusional
mode and make outright mistakes and play foolishly.

A million dollars in prize money was put into the pot for the competition.
There would be five Go games played, one per day, along with associated rules
about taking breaks, etc. Some predicted that Sedol would handily win all five
games, doing so without cracking a sweat. AI pundits were clinging to the hope
that AlphaGo would win at least one of the five games, and otherwise, present
itself as a respectable level of Go player throughout the contest.

In the first match, AlphaGo won.

This was pretty much a worldwide shocker. Sedol was taken aback. Lots of Go
players were surprised that a computer program could compete and beat someone
at Sedol’s level of play. Everyone began to give some street cred to the
AlphaGo program and the efforts by the AI developers.

Tension grew for the next match.

For the second game, it was anticipated that Sedol might significantly change
his approach to the contest. Perhaps he had been overconfident coming into the
competition, some harshly asserted, and the loss of the first game would
awaken him to the importance of putting all his concentration into the
tournament. Or, possibly he had played as though he was competing with a
lesser capable player and thus was not pulling out all the stops to try and
win the match.

What happened in the second game?

Turns out that AlphaGo prevailed, again, and also did something that was
seemingly remarkable for those that avidly play Go. On the 37th move of the
match, the AlphaGo program opted to make placement onto the Go board in a spot
that nobody especially anticipated. It was a surprise move, coming partway
through a match that otherwise was relatively conventional in the nature of
the moves being made by both Sedol and AlphaGo.

At the time, in real-time, rampant speculation was that the move was an utter
gaffe on the part of the AlphaGo program.

Instead, it became famous as a novel move, known now as “Move 37” and heralded
in Go and used colloquially overall to suggest any instance when AI does
something of a novel or unexpected manner.

In the third match, AlphaGo won again, now having successfully beaten Sedol in
a 3-out-of-5 winner competition. They continued though to play a fourth and a
fifth game.

During the fourth game, things were tight as usual and the match play was
going head-to-head (well, head versus AI). Put yourself into the shoes of
Sedol. In one sense, he wasn’t just a Go player, he was somehow representing
all of humanity (an unfair and misguided viewpoint, but pervasive anyway), and
the pressure was on him to win at least one game. Just even one game would be
something to hang your hat on, and bolster faith in mankind (again, a
nonsensical way to look at it).

At the seventy-eighth move of the fourth game, Sedol made a so-called “wedge”
play that was not conventional and surprised onlookers. The next move by
AlphaGo was rotten and diminished the likelihood of a win by the AI system.
After additional play, ultimately AlphaGo tossed in the towel and resigned
from the match, thus Sedol finally had a win against the AI in his belt. He
ended-up losing the fifth game, so AlphaGo won four games, Sedol won one). His
move also became famous, generally known as “Move 78” in the lore of Go
playing.

Something else that is worthwhile to know about involves the overarching
strategy that AlphaGo was crafted to utilize.

When you play a game, let’s say connect-the-dots, you can aim to grab as many
squares at each moment of play, doing so under the belief that inevitably you
will then win by the accumulation of those tactically-oriented successes.
Human players of Go are often apt to play that way, as it can be said too of
chess players, and nearly any kind of game playing altogether.

Another approach involves playing to win, even if only by the thinnest of
margins, as long as you win. In that case, you might not be motivated for each
tactical move to gain near-term territory or score immediate points, and be
willing instead to play a larger scope game per se. The proverbial mantra is
that if you are shortsighted, you might win some of the battles, but could
eventually lose the war. Therefore, it might be a better strategy to keep your
eye on the prize, winning the war, albeit if it means that there are battles
and skirmishes to be lost along the way.

The AI developers devised AlphaGo with that kind of macro-perspective
underlying how the AI system functioned.

Humans can have an especially hard time choosing at the moment to make a move
that might look bad or ill-advised, such as giving up territory, finding
themselves to be unable to grit their teeth, and taking a lump or two during
play. The embarrassment at the instant is difficult to offset by betting that
it is going to ultimately be okay, and you will prevail in the end.

For an AI system, there is no semblance of that kind of sentiment involved,
and it is all about calculated odds and probabilities.

Now that we’ve covered the legendary Go match, let’s consider some lessons
learned about novelty.

The “Move 38” made by the AI system was not magical. It was an interesting
move, for sure, and the AI developers later indicated that the move was one
that the AI had calculated would rarely be undertaken by a human player.

This can be interpreted in two ways (at least).

One interpretation is that a human player would not make that move because
humans are right and know that it would be a lousy move.

Another interpretation is that humans would not make that move due to a belief
that the move is unwise, but this could be a result of the humans
insufficiently assessing the ultimate value of the move, in the long-run, and
getting caught up in a shorter time frame semblance of play.

In this instance, it turned out to be a good move—maybe a brilliant move—and
turned the course of the game to the advantage of the AI. Thus, what looked
like brilliance was in fact a calculated move that few humans would have
imagined as valuable and for which jostled humans to rethink how they think
about such matters.

Some useful recap lessons:

**Showcasing Human Self-Limited Insight.** When the AI does something
seemingly novel, it might be viewed as novel simply because humans have
already predetermined what is customary and anything beyond that is blunted by
the assumption that it is unworthy or mistaken. You could say that we are
mentally trapped by our own drawing of the lines of what is considered as
inside versus outside the box.

**Humans Exploiting AI For Added Insight**. Humans can gainfully assess an AI-
powered novelty to potentially re-calibrate human thinking on a given topic,
enlarging our understanding via leveraging something that the AI, via its vast
calculative capacity, might detect or spot that we have not yet so
ascertained. Thus, besides admiring the novelty, we ought to seek to improve
our mental prowess by whatever source shines brightly including an AI system.

**AI Novelty Is A Dual-Edged Sword.** We need to be mindful of all AI systems
and their possibility of acting in a novel way, which could be good or could
be bad. In the Go game, it worked out well. In other circumstances, the AI
exploiting the novelty route might go off the tracks, as it were.

Let’s see how this can be made tangible via exploring the advent of AI-based
true self-driving cars.

For my framework about AI autonomous cars, see the link here:
[https://aitrends.com/ai-insider/framework-ai-self-driving-driverless-cars-
big-picture/](https://aitrends.com/ai-insider/framework-ai-self-driving-
driverless-cars-big-picture/)

Why this is a moonshot effort, see my explanation here:
[https://aitrends.com/ai-insider/self-driving-car-mother-ai-projects-
moonshot/](https://aitrends.com/ai-insider/self-driving-car-mother-ai-
projects-moonshot/)

For more about the levels as a type of Richter scale, see my discussion here:
[https://aitrends.com/ai-insider/richter-scale-levels-self-driving-
cars/](https://aitrends.com/ai-insider/richter-scale-levels-self-driving-
cars/)

For the argument about bifurcating the levels, see my explanation here:
[https://aitrends.com/ai-insider/reframing-ai-levels-for-self-driving-cars-
bifurcation-of-autonomy/](https://aitrends.com/ai-insider/reframing-ai-levels-
for-self-driving-cars-bifurcation-of-autonomy/)

**Understanding The Levels Of Self-Driving Cars**

As a clarification, true self-driving cars are ones where the AI drives the
car entirely on its own and there isn’t any human assistance during the
driving task.

These driverless vehicles are considered a Level 4 and Level 5, while a car
that requires a human driver to co-share the driving effort is usually
considered at a Level 2 or Level 3. The cars that co-share the driving task
are described as being semi-autonomous, and typically contain a variety of
automated add-on’s that are referred to as ADAS (Advanced Driver-Assistance
Systems).

There is not yet a true self-driving car at Level 5, which we don’t yet even
know if this will be possible to achieve, and nor how long it will take to get
there.

Meanwhile, the Level 4 efforts are gradually trying to get some traction by
undergoing very narrow and selective public roadway trials, though there is
controversy over whether this testing should be allowed per se (we are all
life-or-death guinea pigs in an experiment taking place on our highways and
byways, some contend).

For why remote piloting or operating of self-driving cars is generally
eschewed, see my explanation here: [https://aitrends.com/ai-insider/remote-
piloting-is-a-self-driving-car-crutch/](https://aitrends.com/ai-
insider/remote-piloting-is-a-self-driving-car-crutch/)

To be wary of fake news about self-driving cars, see my tips here:
[https://aitrends.com/ai-insider/ai-fake-news-about-self-driving-
cars/](https://aitrends.com/ai-insider/ai-fake-news-about-self-driving-cars/)

The ethical implications of AI driving systems are significant, see my
indication here: [http://aitrends.com/selfdrivingcars/ethically-ambiguous-
self-driving-cars/](http://aitrends.com/selfdrivingcars/ethically-ambiguous-
self-driving-cars/)

Be aware of the pitfalls of normalization of deviance when it comes to self-
driving cars, here’s my call to arms: [https://aitrends.com/ai-
insider/normalization-of-deviance-endangers-ai-self-driving-
cars/](https://aitrends.com/ai-insider/normalization-of-deviance-endangers-ai-
self-driving-cars/)

**Self-Driving Cars And Acts Of Novelty**

For Level 4 and Level 5 true self-driving vehicles, there won’t be a human
driver involved in the driving task. All occupants will be passengers; the AI
is doing the driving.

You could say that the AI is playing a game, a driving game, requiring
tactical decision-making and strategic planning, akin to when playing Go or
chess, though in this case involving life-or-death matters driving a multi-ton
car on our public roadways.

Our base assumption is that the AI driving system is going to always take a
tried-and-true approach to any driving decisions. This assumption is somewhat
shaped around a notion that AI is a type of robot or automata that is bereft
of any human biases or human foibles.

In reality, there is no reason to make this kind of assumption. Yes, we can
generally rule out the aspect that the AI is not going to display the emotion
of a human ilk, and we also know that the AI will not be drunk or DUI in its
driving efforts. Nonetheless, if the AI has been trained using Machine
Learning (ML) and Deep Learning (DL), it can pick up subtleties of human
behavioral patterns in the data about human driving, out of which it will
likewise utilize or mimic in choosing its driving actions (for example, see my
column postings involving an analysis of potential racial biases in AI and the
possibility of gender biases).

Turning back to the topic of novelty, let’s ponder a specific use case.

A few years ago, I was driving on an open highway, going at the prevailing
speed of around 65 miles per hour, and something nearly unimaginable occurred.
A car coming toward me in the opposing lane, and likely traveling at around 60
to 70 miles per hour, suddenly and unexpectedly veered into my lane. It was
one of those moments that you cannot anticipate.

There did not appear to be any reason for the other driver to be headed toward
me, in my lane of traffic, and coming at me for an imminent and bone-
chillingly terrifying head-on collision. If there had been debris on the other
lane, it might have been a clue that perhaps this other driver was simply
trying to swing around the obstruction. No debris. If there was a slower
moving car, the driver might have wanted to do a fast end-around to get past
it. Nope, there was absolutely no discernible basis for this radical and life-
threatening maneuver.

What would you do?

Come on, hurry, the clock is ticking, and you have just a handful of split
seconds to make a life-or-death driving decision.

You could stay in your lane and hope that the other driver realizes the error
of their ways, opting to veer back into their lane at the last moment. Or, you
could proactively go into the opposing lane, giving the other driver a clear
path in your lane, but this could be a chancy game of chicken whereby the
other driver chooses to go back into their lane (plus, there was other traffic
further behind that driver, so going into the opposing lane was quite dicey).

Okay, so do you stay in your lane or veer away into the opposing lane?

I dare say that most people would be torn between those two options. Neither
one is palatable.

Suppose the AI of a self-driving car was faced with the same circumstance.

What would the AI do?

The odds are that even if the AI had been fed with thousands upon thousands of
miles of driving via a database about human driving while undergoing the ML/DL
training, there might not be any instances of a head-to-head nature and thus
no prior pattern to utilize for making this onerous decision.

Anyway, here’s a twist.

Imagine that the AI calculated the probabilities involving which way to go,
and in some computational manner came to the conclusion that the self-driving
car should go into the ditch that was at the right of the roadway. This was
intended to avoid entirely a collision with the other car (the AI estimated
that a head-on collision would be near-certain death for the occupants). The
AI estimated that going into the ditch at such high speed would indisputably
wreck the car and cause great bodily injury to the occupants, but the odds of
assured death were (let’s say) calculated as lower than the head-on option
possibilities (this is a variant of the infamous Trolley Problem, as covered
in my columns).

I’m betting that you would concede that most humans would be relatively
unwilling to aim purposely into that ditch, which they know for sure is going
to be a wreck and potential death, while instead willing (reluctantly) to take
a hoped-for chance of either veering into the other lane or staying on course
and wishing for the best.

In some sense, the AI might seem to have made a novel choice. It is one that
(we’ll assume) few humans would have given any explicit thought toward.

Returning to the earlier recap of the points about AI novelty, you could
suggest that in this example, the AI has exceeded a human self-imposed
limitation by the AI having considered otherwise “unthinkable” options. From
this, perhaps we can learn to broaden our view for options that otherwise
don’t seem apparent.

The other recap element was that the AI novelty can be a dual-edged sword.

If the AI did react by driving into the ditch, and you were inside the self-
driving car, and you got badly injured, would you later believe that the AI
acted in a novel manner or that it acted mistakenly or adversely?

Some might say that if you lived to ask that question, apparently the AI made
the right choice. The counter-argument is that if the AI had gone with one of
the other choices, perhaps you would have sailed right past the other car and
not gotten a single scratch.

For more details about ODDs, see my indication at this link here:
[https://www.aitrends.com/ai-insider/amalgamating-of-operational-design-
domains-odds-for-ai-self-driving-cars/](https://www.aitrends.com/ai-
insider/amalgamating-of-operational-design-domains-odds-for-ai-self-driving-
cars/)

On the topic of off-road self-driving cars, here’s my details elicitation:
[https://www.aitrends.com/ai-insider/off-roading-as-a-challenging-use-case-
for-ai-autonomous-cars/](https://www.aitrends.com/ai-insider/off-roading-as-a-
challenging-use-case-for-ai-autonomous-cars/)

I’ve urged that there must be a Chief Safety Officer at self-driving car
makers, here’s the scoop: [https://www.aitrends.com/ai-insider/chief-safety-
officers-needed-in-ai-the-case-of-ai-self-driving-
cars/](https://www.aitrends.com/ai-insider/chief-safety-officers-needed-in-ai-
the-case-of-ai-self-driving-cars/)

Expect that lawsuits are going to gradually become a significant part of the
self-driving car industry, see my explanatory details here:
[http://aitrends.com/selfdrivingcars/self-driving-car-lawsuits-bonanza-
ahead/](http://aitrends.com/selfdrivingcars/self-driving-car-lawsuits-bonanza-
ahead/)

**Conclusion**

For those of you wondering what actually did happen, my lucky stars were
looking over me that day, and I survived with nothing more than a close call.
I decided to remain in my lane, though it was tempting to veer into the
opposing lane, and by some miracle, the other driver suddenly went back into
the opposing lane.

When I tell the story, my heart still gets pumping, and I begin to sweat.

Overall, AI that appears to engage in novel approaches to problems can be
advantageous and in some circumstances such as playing a board game can be
right or wrong, for which being wrong does not especially put human lives at
stake.

For AI-based true self-driving cars, lives are at stake.

We’ll need to proceed mindfully and with our eyes wide open about how we want
AI driving systems to operate, including calculating odds and deriving choices
while at the wheel of the vehicle.

_Copyright 2021 Dr. Lance Eliot_

[_http://ai-selfdriving-cars.libsyn.com/website_](http://ai-selfdriving-
cars.libsyn.com/website)

