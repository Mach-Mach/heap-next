---
title: 'SaSR-Net: Source-Aware Semantic Representation Network for Enhancing Audio-Visual Question Answering'
date: 2024-11-08
tags: ['Music-AVQA dataset', 'spatial and temporal attention mechanisms', 'Audio-Visual Question Answering', 'source-wise learnable tokens', 'fusion of audio and visual information', 'multi-modal scenes', 'AVQA-Yang dataset']
categories: ['cs.CV']
problem: Source-aware Semantic Representation Network
solution: interpreting complex multi-modal scenes
pdf_url: https://arxiv.org/pdf/2411.04933
arx_url: https://arxiv.org/abs/2411.04933
score: 5
authors: ['ianyu Yang', 'Yiyang Nan', 'Lisen Dai', 'Zhenwen Liang', 'Yapeng Tian', 'Xiangliang Zhang', 'Tianyu Yang', 'Ianyu Yang']
affiliations_aligned: ['', 'University of Notre Dame', 'University of Notre Dame', 'University of Notre Dame', 'University of Notre Dame', 'University of Notre Dame', 'University of Notre Dame', 'University of Notre Dame']
affiliations: ['', 'University of Notre Dame']
---


Audio-Visual Question Answering (AVQA) is a challenging task that involves answering questions based on both auditory and visual information in videos. A significant challenge is interpreting complex multi-modal scenes, which include both visual objects and sound sources, and connecting them to the given question. In this paper, we introduce the Source-aware Semantic Representation Network (SaSR-Net), a novel model designed for AVQA. SaSR-Net utilizes source-wise learnable tokens to efficiently capture and align audio-visual elements with the corresponding question. It streamlines the fusion of audio and visual information using spatial and temporal attention mechanisms to identify answers in multi-modal scenes. Extensive experiments on the Music-AVQA and AVQA-Yang datasets show that SaSR-Net outperforms state-of-the-art AVQA methods.