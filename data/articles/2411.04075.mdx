---
title: M3SciQA: A Multi-Modal Multi-Document Scientific QA Benchmark for
  Evaluating Foundation Models
date: 2024_11_06
tags: ['scientific question answering', 'scientific literature analysis', 'foundation models', 'information retrieval', 'multi-document', 'multi-modal', 'natural language processing']
categories: ['cs.CL', 'cs.AI']
problem: M3SciQA benchmark for comprehensive evaluation of foundation models
solution: ['inadequate evaluation of foundation models in multi-modal and multi-document contexts']
pdf_url: http://arxiv.org/pdf/2411.04075v1
arx_url: http://arxiv.org/abs/2411.04075v1
score: 5
authors: ['Chuhan Li', 'Ziyao Shangguan', 'Yilun Zhao', 'Deyuan Li', 'Yixin Liu', 'Arman Cohan']
affiliations_aligned: ['Yale University', 'Yale University', 'Yale University', 'Yale University', 'Yale University', 'Allen Institute for AI']
affiliations: ['Yale University', 'Allen Institute for AI']
---


Existing benchmarks for evaluating foundation models mainly focus on
single-document, text-only tasks. However, they often fail to fully capture the
complexity of research workflows, which typically involve interpreting
non-textual data and gathering information across multiple documents. To
address this gap, we introduce M3SciQA, a multi-modal, multi-document
scientific question answering benchmark designed for a more comprehensive
evaluation of foundation models. M3SciQA consists of 1,452 expert-annotated
questions spanning 70 natural language processing paper clusters, where each
cluster represents a primary paper along with all its cited documents,
mirroring the workflow of comprehending a single paper by requiring multi-modal
and multi-document data. With M3SciQA, we conduct a comprehensive evaluation of
18 foundation models. Our results indicate that current foundation models still
significantly underperform compared to human experts in multi-modal information
retrieval and in reasoning across multiple scientific documents. Additionally,
we explore the implications of these findings for the future advancement of
applying foundation models in multi-modal scientific literature analysis.