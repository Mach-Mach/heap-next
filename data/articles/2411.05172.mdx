---
title: '''ImpScore: A Learnable Metric For Quantifying The Implicitness Level of Language'''
date: 2024-11-07
tags: ['implicitness', 'text understanding', 'natural language processing', 'large language models', 'pairwise contrastive learning', 'implicit language', 'pragmatic interpretation', 'semantic meaning', 'scalar metric', 'hate speech detection', 'comprehension capabilities']
categories: ['cs.CL']
problem: ImpScore, a novel reference-free metric formulated through an interpretable regression model
solution: a
pdf_url: http://arxiv.org/pdf/2411.05172v1
arx_url: http://arxiv.org/abs/2411.05172v1
score: 6
authors: ['Yuxin Wang', 'Xiaomeng Zhu', 'Weimin Lyu', 'Saeed Hassanpour', 'Soroush Vosoughi']
affiliations_aligned: ['Department of Computer Science, Dartmouth College', 'Department of Linguistics, Yale University', 'Department of Computer Science, Stony Brook University', 'Department of Computer Science, Dartmouth College', 'Department of Computer Science, Dartmouth College']
affiliations: ['Department of Linguistics, Yale University', 'Department of Computer Science, Dartmouth College', 'Department of Computer Science, Stony Brook University']
---


Handling implicit language is essential for natural language processing systems to achieve precise text understanding and facilitate natural interactions with users. Despite its importance, the absence of a robust metric for accurately measuring the implicitness of language significantly constrains the depth of analysis possible in evaluating models' comprehension capabilities. This paper addresses this gap by developing a scalar metric that quantifies the implicitness level of language without relying on external references. Drawing on principles from traditional linguistics, we define ''implicitness'' as the divergence between semantic meaning and pragmatic interpretation. To operationalize this definition, we introduce ImpScore, a novel, reference-free metric formulated through an interpretable regression model. This model is trained using pairwise contrastive learning on a specially curated dataset comprising $112,580$ (implicit sentence, explicit sentence) pairs. We validate ImpScore through a user study that compares its assessments with human evaluations on out-of-distribution data, demonstrating its accuracy and strong correlation with human judgments. Additionally, we apply ImpScore to hate speech detection datasets, illustrating its utility and highlighting significant limitations in current large language models' ability to understand highly implicit content. The metric model and its training data are available at https://github.com/audreycs/ImpScore.