---
title: Classifier-guided Gradient Modulation for Enhanced Multimodal Learning
date: 2024-11-03
tags: ['segmentation', 'multimodal learning', 'multimodal datasets', 'loss functions', 'classification', 'optimizers', 'classifier-guided methods', 'gradient modulation', 'regression']
categories: ['cs.CL', 'cs.CV', 'cs.LG']
problem: Classifier-Guided Gradient Modulation (CGGM)
solution: ['inadequate use of modalities during multimodal training']
pdf_url: http://arxiv.org/pdf/2411.01409v1
arx_url: http://arxiv.org/abs/2411.01409v1
score: 5
authors: ['Zirun Guo', 'Tao Jin', 'Jingyuan Chen', 'Zhou Zhao']
affiliations_aligned: ['Zhejiang University, Shanghai AI Lab', 'Zhejiang University', 'Zhejiang University', 'Zhejiang University, Shanghai AI Lab']
affiliations: ['Zhejiang University, Shanghai AI Lab', 'Zhejiang University']
---


Multimodal learning has developed very fast in recent years. However, during the multimodal training process, the model tends to rely on only one modality based on which it could learn faster, thus leading to inadequate use of other modalities. Existing methods to balance the training process always have some limitations on the loss functions, optimizers and the number of modalities and only consider modulating the magnitude of the gradients while ignoring the directions of the gradients. To solve these problems, in this paper, we present a novel method to balance multimodal learning with Classifier-Guided Gradient Modulation (CGGM), considering both the magnitude and directions of the gradients. We conduct extensive experiments on four multimodal datasets: UPMC-Food 101, CMU-MOSI, IEMOCAP and BraTS 2021, covering classification, regression and segmentation tasks. The results show that CGGM outperforms all the baselines and other state-of-the-art methods consistently, demonstrating its effectiveness and versatility. Our code is available at https://github.com/zrguo/CGGM.