---
title: 'Moving Off-the-Grid: Scene-Grounded Video Representations'
date: 2024-11-12
tags: ['latent tokens', 'video representation', 'scene tracking', 'positional embeddings', 'self-supervised learning', 'cross-attention', 'downstream tasks']
categories: ['cs.AI', 'cs.CV', 'cs.LG']
problem: M
solution: ['fixed correspondence between representation structure and image space']
pdf_url: https://arxiv.org/pdf/2411.05927
arx_url: https://arxiv.org/abs/2411.05927
score: 4
authors: ['Sjoerd van Steenkiste', 'Daniel Zoran', 'Yi Yang', 'Yulia Rubanova', 'Rishabh Kabra', 'Carl Doersch', 'Dilara Gokay', 'Joseph Heyward', 'Etienne Pot', 'Klaus Greff', 'Drew A. Hudson', 'Thomas Albert Keck', 'Joao Carreira', 'Alexey Dosovitskiy', 'Mehdi S. M. Sajjadi', 'Thomas Kipf']
affiliations_aligned: ['Google Research', 'Google DeepMind', 'Google DeepMind', 'Google DeepMind', 'Google DeepMind', 'Google DeepMind', 'Google DeepMind', 'Google DeepMind', 'Google DeepMind', 'Google DeepMind', 'Google DeepMind', 'Google DeepMind', 'Google DeepMind', 'Inceptive', 'Google DeepMind', 'Google DeepMind']
affiliations: ['Google DeepMind', 'Google Research', 'Inceptive']
---


Current vision models typically maintain a fixed correspondence between their representation structure and image space. Each layer comprises a set of tokens arranged "on-the-grid," which biases patches or tokens to encode information at a specific spatio(-temporal) location. In this work we present Moving Off-the-Grid (MooG), a self-supervised video representation model that offers an alternative approach, allowing tokens to move "off-the-grid" to better enable them to represent scene elements consistently, even as they move across the image plane through time. By using a combination of cross-attention and positional embeddings we disentangle the representation structure and image structure. We find that a simple self-supervised objective--next frame prediction--trained on video data, results in a set of latent tokens which bind to specific scene structures and track them as they move. We demonstrate the usefulness of MooG's learned representation both qualitatively and quantitatively by training readouts on top of the learned representation on a variety of downstream tasks. We show that MooG can provide a strong foundation for different vision tasks when compared to "on-the-grid" baselines.