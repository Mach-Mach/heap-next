---
title: GRSQA -- Graph Reasoning-Structured Question Answering Dataset
date: 2024_11_08
tags: ['reasoning graphs', 'Large Language Models', 'multi-hop question-answering', 'evaluation of LLM reasoning capabilities', 'reasoning structures', 'reasoning pathways', 'QA datasets', 'semantic contexts', 'Graph Reasoning-Structured Question Answering Dataset', 'reasoning abilities']
categories: ['cs.CL']
problem: Graph Reasoning-Structured Question Answering Dataset
solution: ['impact of reasoning structures on LLM M-QA performance']
pdf_url: https://arxiv.org/pdf/2411.00369
arx_url: https://arxiv.org/abs/2411.00369
score: 6
authors: ['Anish Pahilajani', 'Devasha Trivedi', 'Jincen Shuai', 'Khin S. Yone', 'Samyak Rajesh Jain', 'Namyong Park', 'Ryan A. Rossi', 'Nesreen K. Ahmed', 'Franck Dernoncourt', 'Yu Wang']
affiliations_aligned: ['University of California Santa Cruz', 'University of California Santa Cruz', 'University of California Santa Cruz', 'University of California Santa Cruz', 'University of California Santa Cruz', '', 'Adobe Research', 'Cisco Outshift', 'Adobe Research', 'University of Oregon']
affiliations: ['', 'Adobe Research', 'University of Oregon', 'University of California Santa Cruz', 'Cisco Outshift']
---


Large Language Models (LLMs) have excelled in multi-hop question-answering (M-QA) due to their advanced reasoning abilities. However, the impact of the inherent reasoning structures on LLM M-QA performance remains unclear, largely due to the absence of QA datasets that provide fine-grained reasoning structures. To address this gap, we introduce the Graph Reasoning-Structured Question Answering Dataset (GRS-QA), which includes both semantic contexts and reasoning structures for QA pairs. Unlike existing M-QA datasets, where different reasoning structures are entangled together, GRS-QA explicitly captures intricate reasoning pathways by constructing reasoning graphs, where nodes represent textual contexts and edges denote logical flows. These reasoning graphs of different structures enable a fine-grained evaluation of LLM reasoning capabilities across various reasoning structures. Our empirical analysis reveals that LLMs perform differently when handling questions with varying reasoning structures. This finding facilitates the exploration of textual structures as compared with semantics.