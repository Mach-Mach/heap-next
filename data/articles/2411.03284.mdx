---
title: 'SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents'
date: 2024-11-05
tags: ['multi-agent systems', 'hyper-parameter optimization', 'Response Selection', 'expert diversity', 'Large Language Models', 'Early Stopping', 'fairness benchmarks', 'workload balance', 'alignment benchmarks', 'computational costs', 'sparse mixture-of-agents', 'diversity', 'efficiency', 'reasoning benchmarks']
categories: ['cs.CL', 'cs.AI', 'cs.MA']
problem: sparse mixture-of-agents framework
solution: inefficiency and lack of diversity in multi-agent LLMs
pdf_url: http://arxiv.org/pdf/2411.03284v1
arx_url: http://arxiv.org/abs/2411.03284v1
score: 4
authors: ['Dawei Li', 'Zhen Tan', 'Peijia Qian', 'Yifan Li', 'Kumar Satvik Chaudhary', 'Lijie Hu', 'Jiayi Shen']
affiliations_aligned: ['', '', '', '', '', '', '']
affiliations: ['']
---


While multi-agent systems have been shown to significantly enhance the performance of Large Language Models (LLMs) across various tasks and applications, the dense interaction between scaling agents potentially hampers their efficiency and diversity. To address these challenges, we draw inspiration from the sparse mixture-of-agents (SMoE) and propose a sparse mixture-of-agents (SMoA) framework to improve the efficiency and diversity of multi-agent LLMs. Unlike completely connected structures, SMoA introduces novel Response Selection and Early Stopping mechanisms to sparsify information flows among individual LLM agents, striking a balance between performance and efficiency. Additionally, inspired by the expert diversity principle in SMoE frameworks for workload balance between experts, we assign distinct role descriptions to each LLM agent, fostering diverse and divergent thinking. Extensive experiments on reasoning, alignment, and fairness benchmarks demonstrate that SMoA achieves performance comparable to traditional mixture-of-agents approaches but with significantly lower computational costs. Further analysis reveals that SMoA is more stable, has a greater capacity to scale, and offers considerable potential through hyper-parameter optimization. Code and data will be available at: https://github.com/David-Li0406/SMoA.