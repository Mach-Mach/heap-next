---
title: End-to-End Navigation with Vision Language Models: Transforming Spatial Reasoning into Question-Answering
date: 2024-11-11
tags: ['planning', 'navigation policy', 'zero-shot learning', 'design analysis', 'end-to-end navigation', 'Vision-Language Model', 'control', 'baseline prompting methods', 'perception']
categories: ['cs.RO', 'cs.CV', 'cs.CL']
problem: VLMnav framework for end-to-end navigation using Vision-Language Models
solution: ['separation between perception, planning, and control in navigation']
pdf_url: https://arxiv.org/pdf/2411.05755
arx_url: https://arxiv.org/abs/2411.05755
score: 4
authors: ['Dylan Goetting', 'Himanshu Gaurav Singh', 'Antonio Loquercio']
affiliations_aligned: ['University of California Berkeley', 'University of California Berkeley', 'University of Pennsylvania']
affiliations: ['University of Pennsylvania', 'University of California Berkeley']
---


We present VLMnav, an embodied framework to transform a Vision-Language Model (VLM) into an end-to-end navigation policy. In contrast to prior work, we do not rely on a separation between perception, planning, and control; instead, we use a VLM to directly select actions in one step. Surprisingly, we find that a VLM can be used as an end-to-end policy zero-shot, i.e., without any fine-tuning or exposure to navigation data. This makes our approach open-ended and generalizable to any downstream navigation task. We run an extensive study to evaluate the performance of our approach in comparison to baseline prompting methods. In addition, we perform a design analysis to understand the most impactful design decisions. Visual examples and code for our project can be found at https://jirl-upenn.github.io/VLMnav/