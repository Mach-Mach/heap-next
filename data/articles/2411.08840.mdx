---
title: Multimodal Instruction Tuning with Hybrid State Space Models
date: 2024_11_14
tags: ['high frame rate videos', 'high-resolution images', 'multimodal large language models', 'self-attention mechanism', 'hybrid transformer-MAMBA model', 'computational efficiency', 'context handling']
categories: ['cs.CV']
problem: hybrid transformer-MAMBA model for efficient long context processing
solution: ['handling lengthy context in multimodal applications']
pdf_url: https://arxiv.org/pdf/2411.08840
arx_url: https://arxiv.org/abs/2411.08840
score: 4
authors: ['Jianing Zhou', 'Han Li', 'Shuai Zhang', 'Ning Xie', 'Ruijie Wang', 'Xiaohan Nie', 'Sheng Liu', 'Lingyun Wang']
affiliations_aligned: ['University of Illinois at Urbana-Champaign', 'Amazon, Seattle, WA', 'Amazon, Seattle, WA', 'Amazon, Seattle, WA', 'Amazon, Seattle, WA', 'Amazon, Seattle, WA', 'Amazon, Seattle, WA', 'Amazon, Seattle, WA']
affiliations: ['University of Illinois at Urbana-Champaign', 'Amazon, Seattle, WA']
---


Handling lengthy context is crucial for enhancing the recognition and understanding capabilities of multimodal large language models (MLLMs) in applications such as processing high-resolution images or high frame rate videos. The rise in image resolution and frame rate substantially increases computational demands due to the increased number of input tokens. This challenge is further exacerbated by the quadratic complexity with respect to sequence length of the self-attention mechanism. Most prior works either pre-train models with long contexts, overlooking the efficiency problem, or attempt to reduce the context length via downsampling (e.g., identify the key image patches or frames) to decrease the context length, which may result in information loss. To circumvent this issue while keeping the remarkable effectiveness of MLLMs, we propose a novel approach using a hybrid transformer-MAMBA model to efficiently handle long contexts in multimodal applications. Our multimodal model can effectively process long context input exceeding 100k tokens, outperforming existing models across various benchmarks. Remarkably, our model enhances inference efficiency for high-resolution images and high-frame-rate videos by about 4 times compared to current models, with efficiency gains increasing as image resolution or video frames rise. Furthermore, our model is the first to be trained on low-resolution images or low-frame-rate videos while being capable of inference on high-resolution images and high-frame-rate videos, offering flexibility for inference in diverse scenarios.