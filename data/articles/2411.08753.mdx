---
title: 'Which Viewpoint Shows it Best? Language for Weakly Supervising View Selection in Multi-view Videos'
date: 2024-11-14
tags: ['multi-view video', 'view-dependent caption predictions', 'weakly supervised learning', 'language supervision', 'best viewpoint identification', 'camera pose prediction', 'view selection']
categories: ['cs.CV']
problem: a
solution: ['identifying the most informative viewpoint in multi-view videos']
pdf_url: https://arxiv.org/pdf/2411.08753
arx_url: https://arxiv.org/abs/2411.08753
score: 4
authors: ['Sagnik Majumder', 'Tushar Nagarajan', 'Ziad Al-Halah', 'Reina Pradhan', 'Kristen Grauman']
affiliations_aligned: ['UT Austin', 'FAIR, Meta', 'University of Utah', 'UT Austin', 'UT Austin, FAIR, Meta']
affiliations: ['UT Austin, FAIR, Meta', 'University of Utah', 'FAIR, Meta', 'UT Austin']
---


Given a multi-view video, which viewpoint is most informative for a human observer? Existing methods rely on heuristics or expensive ``best-view" supervision to answer this question, limiting their applicability. We propose a weakly supervised approach that leverages language accompanying an instructional multi-view video as a means to recover its most informative viewpoint(s). Our key hypothesis is that the more accurately an individual view can predict a view-agnostic text summary, the more informative it is. To put this into action, we propose a framework that uses the relative accuracy of view-dependent caption predictions as a proxy for best view pseudo-labels. Then, those pseudo-labels are used to train a view selector, together with an auxiliary camera pose predictor that enhances view-sensitivity. During inference, our model takes as input only a multi-view video -- no language or camera poses -- and returns the best viewpoint to watch at each timestep. On two challenging datasets comprised of diverse multi-camera setups and how-to activities, our model consistently outperforms state-of-the-art baselines, both with quantitative metrics and human evaluation.