---
title: 'Leveraging Large Language Models in Code Question Answering: Baselines and Issues'
date: 2024-11-05
tags: ['error analysis', 'Python', 'BERTScore F1', 'large language models', 'question answering', 'BLEU-4', 'dataset preprocessing', 'Exact Match', 'source code', 'grammar correction', 'BLEURT']
categories: ['cs.CL', 'cs.AI']
problem: poor quality of public genuine question-answering datasets
solution: fine-tuning a large language model on a unified dataset of questions and answers for Python code
pdf_url: http://arxiv.org/pdf/2411.03012v1
arx_url: http://arxiv.org/abs/2411.03012v1
score: 7
authors: ['Georgy Andryushchenko', 'Vladimir Ivanov', 'Vladimir Makharev', 'Elizaveta Tukhtina', 'Aidar Valeev']
affiliations_aligned: ['Institute of Applied Physics, Russian Academy of Sciences', 'Moscow Institute of Physics and Technology', 'Kazan Federal University', 'National Research University Higher School of Economics', 'Kazan Federal University']
affiliations: ['National Research University Higher School of Economics', 'Institute of Applied Physics, Russian Academy of Sciences', 'Moscow Institute of Physics and Technology', 'Kazan Federal University']
---


Question answering over source code provides software engineers and project managers with helpful information about the implemented features of a software product. This paper presents a work devoted to using large language models for question answering over source code in Python. The proposed method for implementing a source code question answering system involves fine-tuning a large language model on a unified dataset of questions and answers for Python code. To achieve the highest quality answers, we tested various models trained on datasets preprocessed in different ways: a dataset without grammar correction, a dataset with grammar correction, and a dataset augmented with the generated summaries. The model answers were also analyzed for errors manually. We report BLEU-4, BERTScore F1, BLEURT, and Exact Match metric values, along with the conclusions from the manual error analysis. The obtained experimental results highlight the current problems of the research area, such as poor quality of the public genuine question-answering datasets. In addition, the findings include the positive effect of the grammar correction of the training data on the testing metric values. The addressed findings and issues could be important for other researchers who attempt to improve the quality of source code question answering solutions. The training and evaluation code is publicly available at https://github.com/IU-AES-AI4Code/CodeQuestionAnswering.