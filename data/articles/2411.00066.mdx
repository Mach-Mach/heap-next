---
title: '''Interpretable Language Modeling via Induction-head Ngram Models'''
date: 2024-10-31
tags: ['large language models', 'Induction-head ngram models', 'interpretability', 'efficiency', 'natural-language neuroscience', 'next-word prediction', 'language selectivity', 'speculative decoding', 'neural similarity metric', 'fMRI response prediction']
categories: ['cs.CL', 'cs.AI', 'cs.LG']
problem: Induction-head ngram models
solution: d
pdf_url: http://arxiv.org/pdf/2411.00066v1
arx_url: http://arxiv.org/abs/2411.00066v1
score: 7
authors: ['Eunji Kim', 'Sriya Mantena', 'Weiwei Yang', 'Chandan Singh', 'Sungroh Yoon', 'Jianfeng Gao']
affiliations_aligned: ['Department of Electrical and Computer Engineering, Seoul National University', 'Stanford University', 'Microsoft Research', 'Microsoft Research', 'Interdisciplinary Program in Artificial Intelligence, Seoul National University', 'Microsoft Research']
affiliations: ['Microsoft Research', 'Stanford University', 'Department of Electrical and Computer Engineering, Seoul National University', 'Interdisciplinary Program in Artificial Intelligence, Seoul National University']
---


Recent large language models (LLMs) have excelled across a wide range of tasks, but their use in high-stakes and compute-limited settings has intensified the demand for interpretability and efficiency. We address this need by proposing Induction-head ngram models (Induction-Gram), a method that builds an efficient, interpretable LM by bolstering modern ngram models with a hand-engineered "induction head". This induction head uses a custom neural similarity metric to efficiently search the model's input context for potential next-word completions. This process enables Induction-Gram to provide ngram-level grounding for each generated token. Moreover, experiments show that this simple method significantly improves next-word prediction over baseline interpretable models (up to 26%p) and can be used to speed up LLM inference for large models through speculative decoding. We further study Induction-Gram in a natural-language neuroscience setting, where the goal is to predict the next fMRI response in a sequence. It again provides a significant improvement over interpretable models (20% relative increase in the correlation of predicted fMRI responses), potentially enabling deeper scientific investigation of language selectivity in the brain. The code is available at https://github.com/ejkim47/induction-gram.