---
title: Do LLMs Know to Respect Copyright Notice?
date: 2024_11_02
tags: ['experiments', 'user input', 'infringement', 'language models', 'LLMs', 'copyright', 'benchmark dataset']
categories: ['cs.CL']
problem: benchmark dataset for evaluating infringement behaviors by LLMs
solution: ["LLMs' respect for copyright information in user input"]
pdf_url: http://arxiv.org/pdf/2411.01136v1
arx_url: http://arxiv.org/abs/2411.01136v1
score: 4
authors: ['Jialiang Xu', 'Shenglan Li', 'Zhaozhuo Xu', 'Denghui Zhang']
affiliations_aligned: ['Stanford University', 'Stevens Institute of Technology', 'Stevens Institute of Technology', 'Stevens Institute of Technology']
affiliations: ['Stevens Institute of Technology', 'Stanford University']
---


Prior study shows that LLMs sometimes generate content that violates
copyright. In this paper, we study another important yet underexplored problem,
i.e., will LLMs respect copyright information in user input, and behave
accordingly? The research problem is critical, as a negative answer would imply
that LLMs will become the primary facilitator and accelerator of copyright
infringement behavior. We conducted a series of experiments using a diverse set
of language models, user prompts, and copyrighted materials, including books,
news articles, API documentation, and movie scripts. Our study offers a
conservative evaluation of the extent to which language models may infringe
upon copyrights when processing user input containing protected material. This
research emphasizes the need for further investigation and the importance of
ensuring LLMs respect copyright regulations when handling user input to prevent
unauthorized use or reproduction of protected content. We also release a
benchmark dataset serving as a test bed for evaluating infringement behaviors
by LLMs and stress the need for future alignment.