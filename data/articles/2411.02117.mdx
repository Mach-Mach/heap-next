---
title: 'AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis'
date: 2024-11-04
tags: ['layer importance evaluation', 'large language models', 'question answering', 'sentiment classification', 'model performance', 'activation distribution', 'language modeling', 'interpretability', 'efficient architectures', 'model optimization', 'deep learning', 'activation variance-sparsity score']
categories: ['cs.CL']
problem: Activation Variance-Sparsity Score (AVSS)
solution: functional importance and performance contributions of individual layers within large language models
pdf_url: http://arxiv.org/pdf/2411.02117v1
arx_url: http://arxiv.org/abs/2411.02117v1
score: 5
authors: ['Zichen Song', 'Yuxin Wu', 'Sitan Huang', 'Zhongfeng Kang']
affiliations_aligned: ['Lanzhou University, Lanzhou, Gansu, China', 'Lanzhou University, Lanzhou, Gansu, China', 'Lanzhou University, Lanzhou, Gansu, China', 'Lanzhou University, Lanzhou, Gansu, China']
affiliations: ['Lanzhou University, Lanzhou, Gansu, China']
---


The evaluation of layer importance in deep learning has been an active area of research, with significant implications for model optimization and interpretability. Recently, large language models (LLMs) have gained prominence across various domains, yet limited studies have explored the functional importance and performance contributions of individual layers within LLMs, especially from the perspective of activation distribution. In this work, we propose the Activation Variance-Sparsity Score (AVSS), a novel metric combining normalized activation variance and sparsity to assess each layer's contribution to model performance. By identifying and removing approximately the lowest 25% of layers based on AVSS, we achieve over 90% of original model performance across tasks such as question answering, language modeling, and sentiment classification, indicating that these layers may be non-essential. Our approach provides a systematic method for identifying less critical layers, contributing to efficient large language model architectures.