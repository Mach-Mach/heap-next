---
title: MM-Embed: Universal Multimodal Retrieval with Multimodal LLMs
date: 2024-11-04
tags: ['zero-shot reranking', 'hard negative mining', 'text-image queries', 'bi-encoder retriever', 'modality bias', 'information retrieval', 'large language models', 'multimodal retrieval']
categories: ['cs.CL', 'cs.CV', 'cs.AI', 'cs.LG', 'cs.IR']
problem: modality-aware hard negative mining
solution: ['modality bias in multimodal retrieval']
pdf_url: http://arxiv.org/pdf/2411.02571v1
arx_url: http://arxiv.org/abs/2411.02571v1
score: 9
authors: ['Sheng-Chieh Lin', 'Chankyu Lee', 'Mohammad Shoeybi', 'Jimmy Lin', 'Bryan Catanzaro', 'Wei Ping']
affiliations_aligned: ['NVIDIA', 'NVIDIA', 'NVIDIA', 'University of Waterloo', 'NVIDIA', 'NVIDIA']
affiliations: ['University of Waterloo', 'NVIDIA']
---


State-of-the-art retrieval models typically address a straightforward search scenario, where retrieval tasks are fixed (e.g., finding a passage to answer a specific question) and only a single modality is supported for both queries and retrieved results. This paper introduces techniques for advancing information retrieval with multimodal large language models (MLLMs), enabling a broader search scenario, termed universal multimodal retrieval, where multiple modalities and diverse retrieval tasks are accommodated. To this end, we first study fine-tuning an MLLM as a bi-encoder retriever on 10 datasets with 16 retrieval tasks. Our empirical results show that the fine-tuned MLLM retriever is capable of understanding challenging queries, composed of both text and image, but underperforms a smaller CLIP retriever in cross-modal retrieval tasks due to modality bias from MLLMs. To address the issue, we propose modality-aware hard negative mining to mitigate the modality bias exhibited by MLLM retrievers. Second, we propose to continually fine-tune the universal multimodal retriever to enhance its text retrieval capability while maintaining multimodal retrieval capability. As a result, our model, MM-Embed, achieves state-of-the-art performance on the multimodal retrieval benchmark M-BEIR, which spans multiple domains and tasks, while also surpassing the state-of-the-art text retrieval model, NV-Embed-v1, on MTEB retrieval benchmark. Finally, we explore to prompt the off-the-shelf MLLMs as the zero-shot rerankers to refine the ranking of the candidates from the multimodal retriever. We find that through prompt-and-reranking, MLLMs can further improve multimodal retrieval when the user queries (e.g., text-image composed queries) are more complex and challenging to understand. These findings also pave the way to advance universal multimodal retrieval in the future.