---
title: 'Dynamic Subset Tuning: Expanding the Operational Range of Parameter-Efficient Training for Large Language Models'
date: 2024-11-14
tags: ['prompt tuning', 'NLP tasks', 'model performance', 'large language models', 'dynamic parameter selection', 'parameter-efficient training', 'LoRA']
categories: ['cs.CL', 'cs.LG']
problem: novel parameter-efficient training method with dynamic subset tuning
solution: limited operational range of parameter-efficient training methods
pdf_url: https://arxiv.org/pdf/2411.08610
arx_url: https://arxiv.org/abs/2411.08610
score: 5
authors: ['Felix Stahlberg', 'Jared Lichtarge', 'Shankar Kumar']
affiliations_aligned: ['Google Research', 'Google Research', 'Google Research']
affiliations: ['Google Research']
---


We propose a novel parameter-efficient training (PET) method for large language models that adapts models to downstream tasks by optimizing a small subset of the existing model parameters. Unlike prior methods, this subset is not fixed in location but rather which parameters are modified evolves over the course of training. This dynamic parameter selection can yield good performance with many fewer parameters than extant methods. Our method enables a seamless scaling of the subset size across an arbitrary proportion of the total model size, while popular PET approaches like prompt tuning and LoRA cover only a small part of this spectrum. We match or outperform prompt tuning and LoRA in most cases on a variety of NLP tasks (MT, QA, GSM8K, SuperGLUE) for a given parameter budget across different model families and sizes.