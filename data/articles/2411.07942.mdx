---
title: Towards Low-bit Communication for Tensor Parallel LLM Inference
date: 2024_11_13
tags: ['large language model (LLM) inference', 'tensor parallelism', 'feature communication', 'performance preservation', 'communication cost', 'quantization']
categories: ['cs.LG', 'cs.AI']
problem: quantization method for reducing communicated values
solution: ['increased communication cost in distributed large language models']
pdf_url: https://arxiv.org/pdf/2411.07942
arx_url: https://arxiv.org/abs/2411.07942
score: 5
authors: ['Harry Dong', 'Tyler Johnson', 'Minsik Cho', 'Emad Soroush']
affiliations_aligned: ['Carnegie Mellon University', 'Apple', 'Apple', 'Apple']
affiliations: ['Apple', 'Carnegie Mellon University']
---


Tensor parallelism provides an effective way to increase server large language model (LLM) inference efficiency despite adding an additional communication cost. However, as server LLMs continue to scale in size, they will need to be distributed across more devices, magnifying the communication cost. One way to approach this problem is with quantization, but current methods for LLMs tend to avoid quantizing the features that tensor parallelism needs to communicate. Taking advantage of consistent outliers in communicated features, we introduce a quantization method that reduces communicated values on average from 16 bits to 4.2 bits while preserving nearly all of the original performance. For instance, our method maintains around 98.0% and 99.5% of Gemma 2 27B's and Llama 2 13B's original performance, respectively, averaged across all tasks we evaluated on.