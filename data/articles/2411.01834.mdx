---
title: Align-SLM: Textless Spoken Language Models with Reinforcement Learning
  from AI Feedback
date: 2024_11_04
tags: ['AI Feedback', 'semantic coherence', 'Spoken Language Models', 'preference optimization', 'Reinforcement Learning', 'Direct Preference Optimization', 'speech generation metrics']
categories: ['cs.CL', 'eess.AS']
problem: Align-SLM framework leveraging preference optimization with AI Feedback
solution: ['lack of semantic coherence and relevance in textless Spoken Language Models']
pdf_url: http://arxiv.org/pdf/2411.01834v1
arx_url: http://arxiv.org/abs/2411.01834v1
score: 5
authors: ['Guan-Ting Lin', 'Prashanth Gurunath Shivakumar', 'Aditya Gourav', 'Yile Gu', 'Ankur Gandhe', 'Hung-yi Lee', 'Ivan Bulyko']
affiliations_aligned: ['Graduate Institute of Communication Engineering, National Taiwan University, Taiwan', 'Amazon AGI, USA', 'Amazon AGI, USA', 'Amazon AGI, USA', 'Amazon AGI, USA', 'Graduate Institute of Communication Engineering, National Taiwan University, Taiwan', 'Amazon AGI, USA']
affiliations: ['Graduate Institute of Communication Engineering, National Taiwan University, Taiwan', 'Amazon AGI, USA']
---


While textless Spoken Language Models (SLMs) have shown potential in
end-to-end speech-to-speech modeling, they still lag behind text-based Large
Language Models (LLMs) in terms of semantic coherence and relevance. This work
introduces the Align-SLM framework, which leverages preference optimization
inspired by Reinforcement Learning with AI Feedback (RLAIF) to enhance the
semantic understanding of SLMs. Our approach generates multiple speech
continuations from a given prompt and uses semantic metrics to create
preference data for Direct Preference Optimization (DPO). We evaluate the
framework using ZeroSpeech 2021 benchmarks for lexical and syntactic modeling,
the spoken version of the StoryCloze dataset for semantic coherence, and other
speech generation metrics, including the GPT4-o score and human evaluation.
Experimental results show that our method achieves state-of-the-art performance
for SLMs on most benchmarks, highlighting the importance of preference
optimization to improve the semantics of SLMs.