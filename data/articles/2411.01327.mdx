---
title: 'Visual Fourier Prompt Tuning'
date: 2024-11-02
tags: ['finetuning', 'empirical results', 'Visual prompt tuning', 'parameter-efficient finetuning', 'Fast Fourier Transform', 'spatial domain', 'performance degradation', 'frequency domain', 'state-of-the-art baselines', 'prompt embeddings', 'vision Transformer', 'datasets']
categories: ['cs.CV', 'cs.AI']
problem: Visual Fourier Prompt Tuning method
solution: performance degradation due to dataset disparity
pdf_url: http://arxiv.org/pdf/2411.01327v1
arx_url: http://arxiv.org/abs/2411.01327v1
score: 6
authors: ['Runjia Zeng', 'Cheng Han', 'Qifan Wang', 'Chunshu Wu', 'Tong Geng', 'Lifu Huang', 'Ying Nian Wu', 'Dongfang Liu']
affiliations_aligned: ['Rochester Institute of Technology', 'University of Missouri - Kansas City', 'Meta AI', 'University of Rochester', 'University of Rochester', 'Virginia Tech', 'University of California, Los Angeles', 'Rochester Institute of Technology']
affiliations: ['Rochester Institute of Technology', 'Meta AI', 'University of Rochester', 'University of Missouri - Kansas City', 'Virginia Tech', 'University of California, Los Angeles']
---


With the scale of vision Transformer-based models continuing to grow, finetuning these large-scale pretrained models for new tasks has become increasingly parameter-intensive. Visual prompt tuning is introduced as a parameter-efficient finetuning (PEFT) method to this trend. Despite its successes, a notable research challenge persists within almost all PEFT approaches: significant performance degradation is observed when there is a substantial disparity between the datasets applied in pretraining and finetuning phases. To address this challenge, we draw inspiration from human visual cognition, and propose the Visual Fourier Prompt Tuning (VFPT) method as a general and effective solution for adapting large-scale transformer-based models. Our approach innovatively incorporates the Fast Fourier Transform into prompt embeddings and harmoniously considers both spatial and frequency domain information. Apart from its inherent simplicity and intuitiveness, VFPT exhibits superior performance across all datasets, offering a general solution to dataset challenges, irrespective of data disparities. Empirical results demonstrate that our approach outperforms current state-of-the-art baselines on two benchmarks, with low parameter usage (e.g., 0.57% of model parameters on VTAB-1k) and notable performance enhancements (e.g., 73.20% of mean accuracy on VTAB-1k). Our code is avaliable at https://github.com/runtsang/VFPT.