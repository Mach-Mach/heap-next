---
title: '''Online and Offline Evaluations of Collaborative Filtering and Content Based Recommender Systems'''
date: 2024-11-02
tags: ['e-commerce', 'hybrid approaches', 'hit-rate@k', 'A/B testing', 'user-based recommendations', 'news broadcasting', 'offline evaluations', 'popularity bias', 'manual evaluation', 'item-based recommendations', 'collaborative filtering', 'nDCG', 'digital publishing', 'user satisfaction', 'content-based methods', 'algorithms', 'information retrieval', 'trend-based methods', 'media streaming', 'Persian language', 'click-through rate', 'online evaluations', 'datasets', 'accuracy metrics', 'recommender systems', 'cold-start']
categories: ['cs.CL', 'cs.IR', 'cs.AI', 'cs.LG']
problem: comparative analysis of evaluation methods for recommender systems
solution: u
pdf_url: http://arxiv.org/pdf/2411.01354v1
arx_url: http://arxiv.org/abs/2411.01354v1
score: 6
authors: ['Ali Elahi', 'Armin Zirak']
affiliations_aligned: ['University of Illinois at Chicago, Chicago, Illinois, USA', 'University of Illinois at Chicago, Chicago, Illinois, USA']
affiliations: ['University of Illinois at Chicago, Chicago, Illinois, USA']
---


Recommender systems are widely used AI applications designed to help users efficiently discover relevant items. The effectiveness of such systems is tied to the satisfaction of both users and providers. However, user satisfaction is complex and cannot be easily framed mathematically using information retrieval and accuracy metrics. While many studies evaluate accuracy through offline tests, a growing number of researchers argue that online evaluation methods such as A/B testing are better suited for this purpose. We have employed a variety of algorithms on different types of datasets divergent in size and subject, producing recommendations in various platforms, including media streaming services, digital publishing websites, e-commerce systems, and news broadcasting networks. Notably, our target websites and datasets are in Persian (Farsi) language.   This study provides a comparative analysis of a large-scale recommender system that has been operating for the past year across about 70 websites in Iran, processing roughly 300 requests per second collectively. The system employs user-based and item-based recommendations using content-based, collaborative filtering, trend-based methods, and hybrid approaches. Through both offline and online evaluations, we aim to identify where these algorithms perform most efficiently and determine the best method for our specific needs, considering the dataset and system scale. Our methods of evaluation include manual evaluation, offline tests including accuracy and ranking metrics like hit-rate@k and nDCG, and online tests consisting of click-through rate (CTR). Additionally we analyzed and proposed methods to address cold-start and popularity bias.