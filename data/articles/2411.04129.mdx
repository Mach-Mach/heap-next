---
title: AmazonQAC: A Large-Scale, Naturalistic Query Autocomplete Dataset
date: 2024-11-08
tags: ['search engines', 'finetuning', 'Query Autocomplete', 'user-typed prefixes', 'Prefix Trees', 'semantic retrieval', 'Large Language Models', 'user needs', 'context-dependent modeling', 'AmazonQAC dataset', 'large-scale datasets']
categories: ['cs.AI', 'cs.IR', 'cs.LG']
problem: introduction of AmazonQAC dataset
solution: ['absence of large-scale realistic datasets for QAC']
pdf_url: https://arxiv.org/pdf/2411.04129
arx_url: https://arxiv.org/abs/2411.04129
score: 5
authors: ['Dante Everaert', 'Rohit Patki', 'Tianqi Zheng', 'Christopher Potts']
affiliations_aligned: ['Amazon Search', 'Amazon Search', 'Amazon Search', 'Stanford University']
affiliations: ['Amazon Search', 'Stanford University']
---


Query Autocomplete (QAC) is a critical feature in modern search engines, facilitating user interaction by predicting search queries based on input prefixes. Despite its widespread adoption, the absence of large-scale, realistic datasets has hindered advancements in QAC system development. This paper addresses this gap by introducing AmazonQAC, a new QAC dataset sourced from Amazon Search logs, comprising 395M samples. The dataset includes actual sequences of user-typed prefixes leading to final search terms, as well as session IDs and timestamps that support modeling the context-dependent aspects of QAC. We assess Prefix Trees, semantic retrieval, and Large Language Models (LLMs) with and without finetuning. We find that finetuned LLMs perform best, particularly when incorporating contextual information. However, even our best system achieves only half of what we calculate is theoretically possible on our test data, which implies QAC is a challenging problem that is far from solved with existing systems. This contribution aims to stimulate further research on QAC systems to better serve user needs in diverse environments. We open-source this data on Hugging Face at https://huggingface.co/datasets/amazon/AmazonQAC.