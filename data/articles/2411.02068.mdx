---
title: 'Model Integrity when Unlearning with T2I Diffusion Models'
date: 2024-11-04
tags: ['model integrity', 'text-to-image Diffusion Models', 'FID', 'unlearning algorithms', 'CLIPScore', 'retain distribution', 'Machine Unlearning algorithms', 'forget distribution', 'retention metric']
categories: ['cs.CV', 'cs.LG']
problem: n
solution: ['compromised model integrity during unlearning']
pdf_url: http://arxiv.org/pdf/2411.02068v1
arx_url: http://arxiv.org/abs/2411.02068v1
score: 4
authors: ['Andrea Schioppa', 'Emiel Hoogeboom', 'Jonathan Heek']
affiliations_aligned: ['Google DeepMind', 'Google DeepMind', 'Google DeepMind']
affiliations: ['Google DeepMind']
---


The rapid advancement of text-to-image Diffusion Models has led to their widespread public accessibility. However these models, trained on large internet datasets, can sometimes generate undesirable outputs. To mitigate this, approximate Machine Unlearning algorithms have been proposed to modify model weights to reduce the generation of specific types of images, characterized by samples from a ``forget distribution'', while preserving the model's ability to generate other images, characterized by samples from a ``retain distribution''. While these methods aim to minimize the influence of training data in the forget distribution without extensive additional computation, we point out that they can compromise the model's integrity by inadvertently affecting generation for images in the retain distribution. Recognizing the limitations of FID and CLIPScore in capturing these effects, we introduce a novel retention metric that directly assesses the perceptual difference between outputs generated by the original and the unlearned models. We then propose unlearning algorithms that demonstrate superior effectiveness in preserving model integrity compared to existing baselines. Given their straightforward implementation, these algorithms serve as valuable benchmarks for future advancements in approximate Machine Unlearning for Diffusion Models.