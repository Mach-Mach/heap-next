---
title: JudgeRank: Leveraging Large Language Models for Reasoning-Intensive   Reranking
date: 2024-10-31
tags: ['document retrieval', 'reasoning-intensive tasks', 'large language models', 'query analysis', 'BRIGHT benchmark', 'BEIR benchmark', 'ablation studies', 'zero-shot generalization', 'reranking', 'document analysis', 'retrieval-augmented generation', 'relevance judgment']
categories: ['cs.CL', 'cs.AI']
problem: JudgeRank, a novel agentic reranker
solution: ['lack of nuanced analysis in judging document relevance']
pdf_url: http://arxiv.org/pdf/2411.00142v1
arx_url: http://arxiv.org/abs/2411.00142v1
score: 5
authors: ['Tong Niu', 'Shafiq Joty', 'Ye Liu', 'Caiming Xiong', 'Yingbo Zhou', 'Semih Yavuz']
affiliations_aligned: ['Salesforce AI Research', 'Salesforce AI Research', 'Salesforce AI Research', 'Salesforce AI Research', 'Salesforce AI Research', 'Salesforce AI Research']
affiliations: ['Salesforce AI Research']
---


Accurate document retrieval is crucial for the success of retrieval-augmented generation (RAG) applications, including open-domain question answering and code completion. While large language models (LLMs) have been employed as dense encoders or listwise rerankers in RAG systems, they often struggle with reasoning-intensive tasks because they lack nuanced analysis when judging document relevance. To address this limitation, we introduce JudgeRank, a novel agentic reranker that emulates human cognitive processes when assessing document relevance. Our approach consists of three key steps: (1) query analysis to identify the core problem, (2) document analysis to extract a query-aware summary, and (3) relevance judgment to provide a concise assessment of document relevance. We evaluate JudgeRank on the reasoning-intensive BRIGHT benchmark, demonstrating substantial performance improvements over first-stage retrieval methods and outperforming other popular reranking approaches. In addition, JudgeRank performs on par with fine-tuned state-of-the-art rerankers on the popular BEIR benchmark, validating its zero-shot generalization capability. Through comprehensive ablation studies, we demonstrate that JudgeRank's performance generalizes well across LLMs of various sizes while ensembling them yields even more accurate reranking than individual models.