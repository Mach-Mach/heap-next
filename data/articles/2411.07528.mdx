---
title: SecEncoder: Logs are All You Need in Security
date: 2024_11_13
tags: ['Generalization Capabilities', 'Pretraining', 'Small Language Models', 'Large Language Models', 'Threat Intelligence Document Retrieval', 'Security Logs', 'Domain-Specific Tasks', 'Incident Prioritization']
categories: ['cs.LG', 'cs.AI', 'cs.CL', 'cs.CR']
problem: specialized small language model pretrained using security logs
solution: ['domain-specific limitations of general language models']
pdf_url: https://arxiv.org/pdf/2411.07528
arx_url: https://arxiv.org/abs/2411.07528
score: 5
authors: ['Muhammed Fatih Bulut', 'Yingqi Liu', 'Naveed Ahmad', 'Maximilian Turner', 'Sami Ait Ouahmane', 'Cameron Andrews', 'Lloyd Greenwald']
affiliations_aligned: ['Microsoft Security AI Research', 'Microsoft Security AI Research', 'Microsoft Security AI Research', 'Microsoft Security AI Research', 'Microsoft Security AI Research', 'Microsoft Security AI Research', 'Microsoft Security AI Research']
affiliations: ['Microsoft Security AI Research']
---


Large and Small Language Models (LMs) are typically pretrained using extensive volumes of text, which are sourced from publicly accessible platforms such as Wikipedia, Book Corpus, or through web scraping. These models, due to their exposure to a wide range of language data, exhibit impressive generalization capabilities and can perform a multitude of tasks simultaneously. However, they often fall short when it comes to domain-specific tasks due to their broad training data. This paper introduces SecEncoder, a specialized small language model that is pretrained using security logs. SecEncoder is designed to address the domain-specific limitations of general LMs by focusing on the unique language and patterns found in security logs. Experimental results indicate that SecEncoder outperforms other LMs, such as BERTlarge, DeBERTa-v3-large and OpenAI's Embedding (textembedding-ada-002) models, which are pretrained mainly on natural language, across various tasks. Furthermore, although SecEncoder is primarily pretrained on log data, it outperforms models pretrained on natural language for a range of tasks beyond log analysis, such as incident prioritization and threat intelligence document retrieval. This suggests that domain specific pretraining with logs can significantly enhance the performance of LMs in security. These findings pave the way for future research into security-specific LMs and their potential applications.