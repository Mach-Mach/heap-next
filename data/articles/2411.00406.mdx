---
title: 'MoD: A Distribution-Based Approach for Merging Large Language Models'
date: 2024-11-01
tags: ['Mixture of Distributions (MoD)', 'mathematical reasoning benchmarks', 'operational efficiency', 'output probability distributions', 'weight-averaging methods', 'Qwen2.5 models', 'resource utilization', 'large language models', 'knowledge sharing', 'model merging techniques', 'task-specific variants']
categories: ['cs.LG']
problem: M
solution: ['maintenance and deployment of individual models']
pdf_url: http://arxiv.org/pdf/2411.00406v1
arx_url: http://arxiv.org/abs/2411.00406v1
score: 4
authors: ['Quy-Anh Dang', 'Chris Ngo']
affiliations_aligned: ['VNU University of Science, Vietnam', 'Knovel Engineering Lab, Singapore']
affiliations: ['Knovel Engineering Lab, Singapore', 'VNU University of Science, Vietnam']
---


Large language models (LLMs) have enabled the development of numerous specialized, task-specific variants. However, the maintenance and deployment of these individual models present substantial challenges in terms of resource utilization and operational efficiency. In this work, we propose the \textit{Mixture of Distributions (MoD)} framework, a novel approach for merging LLMs that operates directly on their output probability distributions, rather than on model weights. Unlike traditional weight-averaging methods, MoD effectively preserves the specialized capabilities of individual models while enabling efficient knowledge sharing across tasks. Through extensive experimentation on mathematical reasoning benchmarks using Qwen2.5 models, we demonstrate that MoD significantly outperforms existing model merging techniques across multiple benchmarks. All code, data, and experimental materials are published at https://github.com/knovel-eng/mod.