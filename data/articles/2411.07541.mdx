---
title: HiCoM: Hierarchical Coherent Motion for Streamable Dynamic Scene with 3D Gaussian Splatting
date: 2024-11-13
tags: ['real-time rendering', 'learning speed', 'multi-view streaming videos', 'Hierarchical Coherent Motion', 'motion learning', '3D Gaussian Splatting', 'free-viewpoint video synthesis', 'dynamic scene reconstruction', 'parallel learning', 'data storage reduction', 'perturbation smoothing strategy', 'storage efficiency']
categories: ['cs.CV']
problem: HiCoM framework for efficient dynamic scene reconstruction
solution: ['inefficiency in storage and overfitting in 3D Gaussian Splatting']
pdf_url: https://arxiv.org/pdf/2411.07541
arx_url: https://arxiv.org/abs/2411.07541
score: 4
authors: ['Qiankun Gao', 'Jiarui Meng', 'Chengxiang Wen', 'Jie Chen', 'Jian Zhang']
affiliations_aligned: ['School of Electronic and Computer Engineering, Peking University', 'School of Electronic and Computer Engineering, Peking University', 'School of Electronic and Computer Engineering, Peking University', 'School of Electronic and Computer Engineering, Peking University', 'School of Electronic and Computer Engineering, Peking University']
affiliations: ['School of Electronic and Computer Engineering, Peking University']
---


The online reconstruction of dynamic scenes from multi-view streaming videos faces significant challenges in training, rendering and storage efficiency. Harnessing superior learning speed and real-time rendering capabilities, 3D Gaussian Splatting (3DGS) has recently demonstrated considerable potential in this field. However, 3DGS can be inefficient in terms of storage and prone to overfitting by excessively growing Gaussians, particularly with limited views. This paper proposes an efficient framework, dubbed HiCoM, with three key components. First, we construct a compact and robust initial 3DGS representation using a perturbation smoothing strategy. Next, we introduce a Hierarchical Coherent Motion mechanism that leverages the inherent non-uniform distribution and local consistency of 3D Gaussians to swiftly and accurately learn motions across frames. Finally, we continually refine the 3DGS with additional Gaussians, which are later merged into the initial 3DGS to maintain consistency with the evolving scene. To preserve a compact representation, an equivalent number of low-opacity Gaussians that minimally impact the representation are removed before processing subsequent frames. Extensive experiments conducted on two widely used datasets show that our framework improves learning efficiency of the state-of-the-art methods by about $20\%$ and reduces the data storage by $85\%$, achieving competitive free-viewpoint video synthesis quality but with higher robustness and stability. Moreover, by parallel learning multiple frames simultaneously, our HiCoM decreases the average training wall time to $<2$ seconds per frame with negligible performance degradation, substantially boosting real-world applicability and responsiveness.