---
title: VQA$^2$:Visual Question Answering for Video Quality Assessment
date: 2024_11_06
tags: ['visual question answering', 'video quality understanding', 'state-of-the-art performance', 'VQA2-Assistant', 'video quality assessment', 'low-level visual quality evaluation', 'video quality scoring', 'instruction dataset', 'VQA2 series models', 'large multi-modal models']
categories: ['cs.CV', 'cs.AI']
problem: VQA2 Instruction Dataset and VQA2 series models
solution: ['lack of visual question answering methods in the video domain']
pdf_url: http://arxiv.org/pdf/2411.03795v1
arx_url: http://arxiv.org/abs/2411.03795v1
score: 5
authors: ['Ziheng Jia', 'Zicheng Zhang', 'Jiaying Qian', 'Haoning Wu', 'Wei Sun', 'Chunyi Li', 'Xiaohong Liu', 'Weisi Lin', 'Guangtao Zhai', 'Xiongkuo Min']
affiliations_aligned: ['Shanghai Jiaotong University', 'Shanghai Jiaotong University', 'Shanghai Jiaotong University', 'Nanyang Technological University', 'Shanghai Jiaotong University', 'Shanghai Jiaotong University', 'Shanghai Jiaotong University', 'Nanyang Technological University', 'Shanghai Jiaotong University', 'Shanghai Jiaotong University']
affiliations: ['Shanghai Jiaotong University', 'Nanyang Technological University']
---


The advent and proliferation of large multi-modal models (LMMs) have
introduced a new paradigm to video-related computer vision fields, including
training and inference methods based on visual question answering (VQA). These
methods enable models to handle multiple downstream tasks robustly. Video
Quality Assessment (VQA), a classic field in low-level visual quality
evaluation, originally focused on quantitative video quality scoring. However,
driven by advances in LMMs, it is now evolving towards more comprehensive
visual quality understanding tasks. Visual question answering has significantly
improved low-level visual evaluation within the image domain recently. However,
related work is almost nonexistent in the video domain, leaving substantial
room for improvement. To address this gap, we introduce the VQA2 Instruction
Dataset the first visual question answering instruction dataset entirely
focuses on video quality assessment, and based on it, we propose the VQA2
series models The VQA2 Instruction Dataset consists of three stages and covers
various video types, containing 157,735 instruction question-answer pairs,
including both manually annotated and synthetic data. We conduct extensive
experiments on both video quality scoring and video quality understanding
tasks. Results demonstrate that the VQA2 series models achieve state-of-the-art
(SOTA) performance in quality scoring tasks, and their performance in visual
quality question answering surpasses the renowned GPT-4o. Additionally, our
final model, the VQA2-Assistant, performs well across both scoring and
question-answering tasks, validating its versatility.