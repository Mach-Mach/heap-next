---
title: '''Self-Evolved Reward Learning for LLMs'''
date: 2024-11-01
tags: ['biases', 'human preferences', 'training data', 'Reinforcement Learning from Human Feedback', 'reward model', 'self-feedback', 'large language models', 'experiments', 'language models', 'datasets']
categories: ['cs.CL', 'cs.AI']
problem: Self-Evolved Reward Learning
solution: t
pdf_url: http://arxiv.org/pdf/2411.00418v1
arx_url: http://arxiv.org/abs/2411.00418v1
score: 6
authors: ['Chenghua Huang', 'Zhizhen Fan', 'Lu Wang', 'Fangkai Yang', 'Pu Zhao', 'Zeqi Lin', 'Qingwei Lin', 'Dongmei Zhang', 'Saravan Rajmohan', 'Qi Zhang']
affiliations_aligned: ['School of Computer Science, Fudan University', 'School of Computer Science, Peking University', 'Microsoft', 'Microsoft', 'Microsoft', 'Microsoft', 'Microsoft', 'Microsoft', 'Microsoft', 'Microsoft']
affiliations: ['Microsoft', 'School of Computer Science, Fudan University', 'School of Computer Science, Peking University']
---


Reinforcement Learning from Human Feedback (RLHF) is a crucial technique for aligning language models with human preferences, playing a pivotal role in the success of conversational models like GPT-4, ChatGPT, and Llama 2. A core challenge in employing RLHF lies in training a reliable reward model (RM), which relies on high-quality labels typically provided by human experts or advanced AI system. These methods can be costly and may introduce biases that affect the language model's responses. As language models improve, human input may become less effective in further enhancing their performance. In this paper, we propose Self-Evolved Reward Learning (SER), a novel approach where the RM generates additional training data to iteratively improve itself. We conducted extensive experiments on multiple datasets such as HH-RLHF and UltraFeedback, using models like Mistral and Llama 3, and compare SER against various baselines. Our results demonstrate that even with limited human-annotated data, learning from self-feedback can robustly enhance RM performance, thereby boosting the capabilities of large language models (LLMs).