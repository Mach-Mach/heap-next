---
title: Predictor-Corrector Enhanced Transformers with Exponential Moving
  Average Coefficient Learning
date: 2024_11_05
tags: ['Ordinary Differential Equations', 'multistep methods', 'multi-particle dynamical systems', 'neural network design', 'predictor-corrector learning framework', 'exponential moving average', 'abstractive summarization', 'BLEU scores', 'high-order methods', 'machine translation', 'natural language understanding', 'LLama models', 'language modeling', 'SacreBLEU', 'Residual networks', 'Transformer architecture']
categories: ['cs.CL']
problem: predictor-corrector learning framework with exponential moving average-based coefficient learning
solution: ['minimizing truncation errors in Transformer architecture']
pdf_url: http://arxiv.org/pdf/2411.03042v1
arx_url: http://arxiv.org/abs/2411.03042v1
score: 5
authors: ['Bei Li', 'Tong Zheng', 'Rui Wang', 'Jiahao Liu', 'Qingyan Guo', 'Junliang Guo', 'Xu Tan', 'Tong Xiao', 'Jingbo Zhu', 'Jingang Wang', 'Xunliang Cai']
affiliations_aligned: ['', '', '', '', '', '', '', '', '', '', '']
affiliations: ['']
---


Residual networks, as discrete approximations of Ordinary Differential
Equations (ODEs), have inspired significant advancements in neural network
design, including multistep methods, high-order methods, and multi-particle
dynamical systems. The precision of the solution to ODEs significantly affects
parameter optimization, thereby impacting model performance. In this work, we
present a series of advanced explorations of Transformer architecture design to
minimize the error compared to the true ``solution.'' First, we introduce a
predictor-corrector learning framework to minimize truncation errors, which
consists of a high-order predictor and a multistep corrector. Second, we
propose an exponential moving average-based coefficient learning method to
strengthen our higher-order predictor. Extensive experiments on large-scale
machine translation, abstractive summarization, language modeling, and natural
language understanding benchmarks demonstrate the superiority of our approach.
On the WMT'14 English-German and English-French tasks, our model achieved BLEU
scores of 30.95 and 44.27, respectively. Furthermore, on the OPUS multilingual
machine translation task, our model surpasses a robust 3.8B DeepNet by an
average of 2.9 SacreBLEU, using only 1/3 parameters. Notably, it also beats
LLama models by 5.7 accuracy points on the LM Harness Evaluation.