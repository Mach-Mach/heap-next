---
title: 'Identifying Implicit Social Biases in Vision-Language Models'
date: 2024-11-01
tags: ['multimodal retrieval tasks', 'CLIP', 'social biases', 'demographic groups', 'fairness-aware curation', 'vision-language models', 'taxonomy of social biases', 'image-text datasets', 'harmful stereotypes', 'transparency']
categories: ['cs.CL', 'cs.CV', 'cs.AI', 'cs.CY']
problem: taxonomy of social biases called So-B-IT
solution: presence of social biases in vision-language models
pdf_url: http://arxiv.org/pdf/2411.00997v1
arx_url: http://arxiv.org/abs/2411.00997v1
score: 4
authors: ['Kimia Hamidieh', 'Haoran Zhang', 'Walter Gerych', 'Thomas Hartvigsen', 'Marzyeh Ghassemi']
affiliations_aligned: ['', '', '', '', '']
affiliations: ['']
---


Vision-language models, like CLIP (Contrastive Language Image Pretraining), are becoming increasingly popular for a wide range of multimodal retrieval tasks. However, prior work has shown that large language and deep vision models can learn historical biases contained in their training sets, leading to perpetuation of stereotypes and potential downstream harm. In this work, we conduct a systematic analysis of the social biases that are present in CLIP, with a focus on the interaction between image and text modalities. We first propose a taxonomy of social biases called So-B-IT, which contains 374 words categorized across ten types of bias. Each type can lead to societal harm if associated with a particular demographic group. Using this taxonomy, we examine images retrieved by CLIP from a facial image dataset using each word as part of a prompt. We find that CLIP frequently displays undesirable associations between harmful words and specific demographic groups, such as retrieving mostly pictures of Middle Eastern men when asked to retrieve images of a "terrorist". Finally, we conduct an analysis of the source of such biases, by showing that the same harmful stereotypes are also present in a large image-text dataset used to train CLIP models for examples of biases that we find. Our findings highlight the importance of evaluating and addressing bias in vision-language models, and suggest the need for transparency and fairness-aware curation of large pre-training datasets.