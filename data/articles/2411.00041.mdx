---
title: NeuroSym-BioCAT: Leveraging Neuro-Symbolic Methods for Biomedical
  Scholarly Document Categorization and Question Answering
date: 2024_10_29
tags: ['biomedical scholarly documents', 'MiniLM model', 'topic modeling', 'OVB-LDA', 'large language models', 'answer extraction', 'information retrieval', 'BI-POP CMA-ES', 'evaluation metrics']
categories: ['cs.AI', 'cs.CL', 'cs.DL', 'cs.IR']
problem: integrated approach using optimized topic modeling and fine-tuned MiniLM for document categorization and answer extraction
solution: ['efficient retrieval of accurate and relevant information from biomedical scholarly document abstracts']
pdf_url: http://arxiv.org/pdf/2411.00041v1
arx_url: http://arxiv.org/abs/2411.00041v1
score: 5
authors: ['Parvez Zamil', 'Gollam Rabby', 'Md. Sadekur Rahman', 'Sören Auer']
affiliations_aligned: ['TIB—Leibniz Information Centre for Science and Technology, Hannover, Germany', 'TIB—Leibniz Information Centre for Science and Technology, Hannover, Germany', 'TIB—Leibniz Information Centre for Science and Technology, Hannover, Germany', 'TIB—Leibniz Information Centre for Science and Technology, Hannover, Germany']
affiliations: ['TIB—Leibniz Information Centre for Science and Technology, Hannover, Germany']
---


The growing volume of biomedical scholarly document abstracts presents an
increasing challenge in efficiently retrieving accurate and relevant
information. To address this, we introduce a novel approach that integrates an
optimized topic modelling framework, OVB-LDA, with the BI-POP CMA-ES
optimization technique for enhanced scholarly document abstract categorization.
Complementing this, we employ the distilled MiniLM model, fine-tuned on
domain-specific data, for high-precision answer extraction. Our approach is
evaluated across three configurations: scholarly document abstract retrieval,
gold-standard scholarly documents abstract, and gold-standard snippets,
consistently outperforming established methods such as RYGH and bio-answer
finder. Notably, we demonstrate that extracting answers from scholarly
documents abstracts alone can yield high accuracy, underscoring the sufficiency
of abstracts for many biomedical queries. Despite its compact size, MiniLM
exhibits competitive performance, challenging the prevailing notion that only
large, resource-intensive models can handle such complex tasks. Our results,
validated across various question types and evaluation batches, highlight the
robustness and adaptability of our method in real-world biomedical
applications. While our approach shows promise, we identify challenges in
handling complex list-type questions and inconsistencies in evaluation metrics.
Future work will focus on refining the topic model with more extensive
domain-specific datasets, further optimizing MiniLM and utilizing large
language models (LLM) to improve both precision and efficiency in biomedical
question answering.