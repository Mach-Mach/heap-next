---
title: '''Enhancing Multimodal Query Representation via Visual Dialogues for End-to-End Knowledge Retrieval'''
date: 2024-11-14
tags: ['fine-tuning scenarios', 'end-to-end retrieval system', 'image comprehension', 'partial convolution mechanism', 'multimodal query representations', 'dynamic modality interaction', 'visual information', 'Visual Dialogue-to-Retrieval dataset', 'caption generators', 'information retrieval tasks', 'zero-shot settings', 'object detectors', 'multimodal retrieval systems']
categories: ['cs.IR', 'cs.MM', 'cs.AI', 'cs.CV']
problem: end-to-end retrieval system Ret-XKnow
solution: d
pdf_url: https://arxiv.org/pdf/2411.08334
arx_url: https://arxiv.org/abs/2411.08334
score: 6
authors: ['Yeong-Joon Ju', 'Ho-Joong Kim', 'Seong-Whan Lee']
affiliations_aligned: ['Department of Electrical Engineering, Pohang University of Science and Technology, Pohang, South Korea', 'Department of Electrical Engineering, Pohang University of Science and Technology, Pohang, South Korea', 'Department of Computer Science and Engineering, Pohang University of Science and Technology, Pohang, South Korea']
affiliations: ['Department of Electrical Engineering, Pohang University of Science and Technology, Pohang, South Korea', 'Department of Computer Science and Engineering, Pohang University of Science and Technology, Pohang, South Korea']
---


Existing multimodal retrieval systems often rely on disjointed models for image comprehension, such as object detectors and caption generators, leading to cumbersome implementations and training processes. To overcome this limitation, we propose an end-to-end retrieval system, Ret-XKnow, to endow a text retriever with the ability to understand multimodal queries via dynamic modality interaction. Ret-XKnow leverages a partial convolution mechanism to focus on visual information relevant to the given textual query, thereby enhancing multimodal query representations. To effectively learn multimodal interaction, we also introduce the Visual Dialogue-to-Retrieval (ViD2R) dataset automatically constructed from visual dialogue datasets. Our dataset construction process ensures that the dialogues are transformed into suitable information retrieval tasks using a text retriever. We demonstrate that our approach not only significantly improves retrieval performance in zero-shot settings but also achieves substantial improvements in fine-tuning scenarios. Our code is publicly available: https://github.com/yeongjoonJu/Ret_XKnow.