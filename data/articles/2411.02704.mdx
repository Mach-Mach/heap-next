---
title: 'RT-Affordance: Affordances are Versatile Intermediate Representations for Robot Manipulation'
date: 2024-11-05
tags: ['task language', 'hierarchical model', 'manipulation tasks', 'intermediate policy representations', 'affordance images', 'affordances', 'robot manipulation', 'supervision sources', 'knowledge transfer']
categories: ['cs.CL', 'cs.RO', 'cs.CV', 'cs.AI', 'cs.LG']
problem: RT-Affordance model conditioning policies on affordances
solution: insufficient context in existing representations for manipulation tasks
pdf_url: http://arxiv.org/pdf/2411.02704v1
arx_url: http://arxiv.org/abs/2411.02704v1
score: 6
authors: ['Soroush Nasiriany', 'Sean Kirmani', 'Tianli Ding', 'Laura Smith', 'Yuke Zhu', 'Danny Driess', 'Dorsa Sadigh', 'Ted Xiao']
affiliations_aligned: ['Google DeepMind, The University of Austin at Texas', 'Google DeepMind, The University of Austin at Texas', 'Google DeepMind, The University of Austin at Texas', 'Google DeepMind, The University of Austin at Texas', 'The University of Austin at Texas', 'Google DeepMind, The University of Austin at Texas', 'Google DeepMind, The University of Austin at Texas', 'Google DeepMind, The University of Austin at Texas']
affiliations: ['Google DeepMind, The University of Austin at Texas', 'The University of Austin at Texas']
---


We explore how intermediate policy representations can facilitate generalization by providing guidance on how to perform manipulation tasks. Existing representations such as language, goal images, and trajectory sketches have been shown to be helpful, but these representations either do not provide enough context or provide over-specified context that yields less robust policies. We propose conditioning policies on affordances, which capture the pose of the robot at key stages of the task. Affordances offer expressive yet lightweight abstractions, are easy for users to specify, and facilitate efficient learning by transferring knowledge from large internet datasets. Our method, RT-Affordance, is a hierarchical model that first proposes an affordance plan given the task language, and then conditions the policy on this affordance plan to perform manipulation. Our model can flexibly bridge heterogeneous sources of supervision including large web datasets and robot trajectories. We additionally train our model on cheap-to-collect in-domain affordance images, allowing us to learn new tasks without collecting any additional costly robot trajectories. We show on a diverse set of novel tasks how RT-Affordance exceeds the performance of existing methods by over 50%, and we empirically demonstrate that affordances are robust to novel settings. Videos available at https://snasiriany.me/rt-affordance