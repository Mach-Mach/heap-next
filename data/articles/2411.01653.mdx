---
title: 'Diagnosing Medical Datasets with Training Dynamics'
date: 2024-11-03
tags: ['Data Maps', 'medical question answering', 'training dynamics', 'automated annotation', 'medical knowledge', 'text comprehension', 'evaluation of frameworks', 'quality of training data']
categories: ['cs.CL', 'cs.AI', 'cs.LG']
problem: Data Maps framework
solution: unique challenges in answering medical questions
pdf_url: http://arxiv.org/pdf/2411.01653v1
arx_url: http://arxiv.org/abs/2411.01653v1
score: 5
authors: ['Laura Wenderoth']
affiliations_aligned: ['Newnham College, University of Cambridge']
affiliations: ['Newnham College, University of Cambridge']
---


This study explores the potential of using training dynamics as an automated alternative to human annotation for evaluating the quality of training data. The framework used is Data Maps, which classifies data points into categories such as easy-to-learn, hard-to-learn, and ambiguous (Swayamdipta et al., 2020). Swayamdipta et al. (2020) highlight that difficult-to-learn examples often contain errors, and ambiguous cases significantly impact model training. To confirm the reliability of these findings, we replicated the experiments using a challenging dataset, with a focus on medical question answering. In addition to text comprehension, this field requires the acquisition of detailed medical knowledge, which further complicates the task. A comprehensive evaluation was conducted to assess the feasibility and transferability of the Data Maps framework to the medical domain. The evaluation indicates that the framework is unsuitable for addressing datasets' unique challenges in answering medical questions.