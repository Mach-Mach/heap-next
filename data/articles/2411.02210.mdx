---
title: 'One VLM to Keep it Learning: Generation and Balancing for Data-free Continual Visual Question Answering'
date: 2024-11-04
tags: ['data-free method', 'benchmark evaluation', 'catastrophic forgetting', 'unsupervised clustering', 'question meta-statistics', 'pseudo-rehearsal data', 'Vision-Language Models', 'Visual Question Answering', 'rehearsal strategy', 'continual learning']
categories: ['cs.CV']
problem: d
solution: ['catastrophic forgetting in continual learning for Visual Question Answering']
pdf_url: http://arxiv.org/pdf/2411.02210v1
arx_url: http://arxiv.org/abs/2411.02210v1
score: 4
authors: ['Deepayan Das', 'Davide Talon', 'Massimiliano Mancini', 'Yiming Wang', 'Elisa Ricci']
affiliations_aligned: ['University of Trento', 'Fondazione Bruno Kessler', 'University of Trento', 'Fondazione Bruno Kessler', 'University of Trento']
affiliations: ['University of Trento', 'Fondazione Bruno Kessler']
---


Vision-Language Models (VLMs) have shown significant promise in Visual Question Answering (VQA) tasks by leveraging web-scale multimodal datasets. However, these models often struggle with continual learning due to catastrophic forgetting when adapting to new tasks. As an effective remedy to mitigate catastrophic forgetting, rehearsal strategy uses the data of past tasks upon learning new task. However, such strategy incurs the need of storing past data, which might not be feasible due to hardware constraints or privacy concerns. In this work, we propose the first data-free method that leverages the language generation capability of a VLM, instead of relying on external models, to produce pseudo-rehearsal data for addressing continual VQA. Our proposal, named as GaB, generates pseudo-rehearsal data by posing previous task questions on new task data. Yet, despite being effective, the distribution of generated questions skews towards the most frequently posed questions due to the limited and task-specific training data. To mitigate this issue, we introduce a pseudo-rehearsal balancing module that aligns the generated data towards the ground-truth data distribution using either the question meta-statistics or an unsupervised clustering method. We evaluate our proposed method on two recent benchmarks, \ie VQACL-VQAv2 and CLOVE-function benchmarks. GaB outperforms all the data-free baselines with substantial improvement in maintaining VQA performance across evolving tasks, while being on-par with methods with access to the past data.