---
title: Towards Vision Mixture of Experts for Wildlife Monitoring on the Edge
date: 2024_11_13
tags: ['conditional computation', 'multimodal networks', 'sustainable computing', 'mobile vision transformers', 'bird species discrimination', 'algorithm inference latency', 'IoT sensors', 'cloud storage costs', 'deep learning algorithms', 'communication bandwidth', 'TinyML', 'data privacy']
categories: ['cs.CV']
problem: per patch conditional computation for mobile vision transformers
solution: ['unprecedented demand for computing infrastructure to transmit and analyze petabytes of data']
pdf_url: https://arxiv.org/pdf/2411.07834
arx_url: https://arxiv.org/abs/2411.07834
score: 4
authors: ['Emmanuel Azuh Mensah', 'Anderson Lee', 'Haoran Zhang', 'Yitong Shan', 'Kurtis Heimerl']
affiliations_aligned: ['University of Washington', 'University of Washington', 'University of Washington', 'University of Washington', 'University of Washington']
affiliations: ['University of Washington']
---


The explosion of IoT sensors in industrial, consumer and remote sensing use cases has come with unprecedented demand for computing infrastructure to transmit and to analyze petabytes of data. Concurrently, the world is slowly shifting its focus towards more sustainable computing. For these reasons, there has been a recent effort to reduce the footprint of related computing infrastructure, especially by deep learning algorithms, for advanced insight generation. The `TinyML' community is actively proposing methods to save communication bandwidth and excessive cloud storage costs while reducing algorithm inference latency and promoting data privacy. Such proposed approaches should ideally process multiple types of data, including time series, audio, satellite images, and video, near the network edge as multiple data streams has been shown to improve the discriminative ability of learning algorithms, especially for generating fine grained results. Incidentally, there has been recent work on data driven conditional computation of subnetworks that has shown real progress in using a single model to share parameters among very different types of inputs such as images and text, reducing the computation requirement of multi-tower multimodal networks. Inspired by such line of work, we explore similar per patch conditional computation for the first time for mobile vision transformers (vision only case), that will eventually be used for single-tower multimodal edge models. We evaluate the model on Cornell Sap Sucker Woods 60, a fine grained bird species discrimination dataset. Our initial experiments uses $4X$ fewer parameters compared to MobileViTV2-1.0 with a $1$% accuracy drop on the iNaturalist '21 birds test data provided as part of the SSW60 dataset.