---
title: Attacking Vision-Language Computer Agents via Pop-ups
date: 2024_11_04
tags: ['pop-ups', 'OSWorld', 'autonomous agents', 'vision and language models', 'task success rate', 'adversarial attacks', 'agent testing environments', 'VisualWebArena', 'defense techniques']
categories: ['cs.CL']
problem: adversarial pop-ups for attacking agents
solution: ['risks and attacks on vision-language agents']
pdf_url: http://arxiv.org/pdf/2411.02391v1
arx_url: http://arxiv.org/abs/2411.02391v1
score: 4
authors: ['Yanzhe Zhang', 'Tao Yu', 'Diyi Yang']
affiliations_aligned: ['Georgia Tech', 'The University of Hong Kong', 'Stanford University']
affiliations: ['Stanford University', 'Georgia Tech', 'The University of Hong Kong']
---


Autonomous agents powered by large vision and language models (VLM) have
demonstrated significant potential in completing daily computer tasks, such as
browsing the web to book travel and operating desktop software, which requires
agents to understand these interfaces. Despite such visual inputs becoming more
integrated into agentic applications, what types of risks and attacks exist
around them still remain unclear. In this work, we demonstrate that VLM agents
can be easily attacked by a set of carefully designed adversarial pop-ups,
which human users would typically recognize and ignore. This distraction leads
agents to click these pop-ups instead of performing the tasks as usual.
Integrating these pop-ups into existing agent testing environments like OSWorld
and VisualWebArena leads to an attack success rate (the frequency of the agent
clicking the pop-ups) of 86% on average and decreases the task success rate by
47%. Basic defense techniques such as asking the agent to ignore pop-ups or
including an advertisement notice, are ineffective against the attack.