---
title: Typicalness-Aware Learning for Failure Detection
date: 2024_11_08
tags: ['cross-entropy loss', 'atypical samples', 'logit direction', 'Area Under the Risk-Coverage Curve', 'failure detection', 'metric for typicalness', 'logit magnitude', 'benchmark datasets', 'Typicalness-Aware Learning', 'Deep neural networks', 'overconfidence issue']
categories: ['cs.CV']
problem: Typicalness-Aware Learning approach
solution: ['overconfidence issue in deep neural networks']
pdf_url: https://arxiv.org/pdf/2411.01981
arx_url: https://arxiv.org/abs/2411.01981
score: 6
authors: ['Yijun Liu', 'Jiequan Cui', 'Zhuotao Tian', 'Senqiao Yang', 'Qingdong He', 'Xiaoling Wang', 'Jingyong Su']
affiliations_aligned: ['Harbin Institute of Technology (Shenzhen)', 'Nanyang Technological University', 'Harbin Institute of Technology (Shenzhen)', 'The Chinese University of Hong Kong', 'Tencent Youtu Lab', 'Harbin Institute of Technology (Shenzhen)', 'Harbin Institute of Technology (Shenzhen)']
affiliations: ['Harbin Institute of Technology (Shenzhen)', 'Tencent Youtu Lab', 'Nanyang Technological University', 'The Chinese University of Hong Kong']
---


Deep neural networks (DNNs) often suffer from the overconfidence issue, where incorrect predictions are made with high confidence scores, hindering the applications in critical systems. In this paper, we propose a novel approach called Typicalness-Aware Learning (TAL) to address this issue and improve failure detection performance. We observe that, with the cross-entropy loss, model predictions are optimized to align with the corresponding labels via increasing logit magnitude or refining logit direction. However, regarding atypical samples, the image content and their labels may exhibit disparities. This discrepancy can lead to overfitting on atypical samples, ultimately resulting in the overconfidence issue that we aim to address. To tackle the problem, we have devised a metric that quantifies the typicalness of each sample, enabling the dynamic adjustment of the logit magnitude during the training process. By allowing atypical samples to be adequately fitted while preserving reliable logit direction, the problem of overconfidence can be mitigated. TAL has been extensively evaluated on benchmark datasets, and the results demonstrate its superiority over existing failure detection methods. Specifically, TAL achieves a more than 5% improvement on CIFAR100 in terms of the Area Under the Risk-Coverage Curve (AURC) compared to the state-of-the-art. Code is available at https://github.com/liuyijungoon/TAL.