---
title: Training objective drives the consistency of representational similarity across datasets
date: 2024_11_11
tags: ['self-supervised vision models', 'foundation models', 'image-text models', 'Platonic Representation Hypothesis', 'image classification', 'systematic measurement of similarities', 'representational similarity', 'downstream task performance', 'dataset-dependent behavior']
categories: ['cs.LG', 'cs.CV', 'cs.AI']
problem: systematic way to measure representational similarity between models
solution: ['inconsistency of representational similarity across datasets']
pdf_url: https://arxiv.org/pdf/2411.05561
arx_url: https://arxiv.org/abs/2411.05561
score: 5
authors: ['Laure Ciernik', 'Lorenz Linhardt', 'Marco Morik', 'Jonas Dippel', 'Simon Kornblith', 'Lukas Muttenthaler']
affiliations_aligned: ['Machine Learning Group, Technische Universität Berlin, BIFOLD, Hector Fellow Academy, Berlin, Germany', 'Machine Learning Group, Technische Universität Berlin, BIFOLD, Berlin, Germany', 'Machine Learning Group, Technische Universität Berlin, BIFOLD, Berlin, Germany', 'Aignostics, Machine Learning Group, Technische Universität Berlin, BIFOLD, Berlin, Germany', 'Anthropic, San Francisco, CA, USA', 'Google DeepMind, Machine Learning Group, Technische Universität Berlin, BIFOLD, Berlin, Germany']
affiliations: ['Aignostics, Machine Learning Group, Technische Universität Berlin, BIFOLD, Berlin, Germany', 'Machine Learning Group, Technische Universität Berlin, BIFOLD, Hector Fellow Academy, Berlin, Germany', 'Google DeepMind, Machine Learning Group, Technische Universität Berlin, BIFOLD, Berlin, Germany', 'Machine Learning Group, Technische Universität Berlin, BIFOLD, Berlin, Germany', 'Anthropic, San Francisco, CA, USA']
---


The Platonic Representation Hypothesis claims that recent foundation models are converging to a shared representation space as a function of their downstream task performance, irrespective of the objectives and data modalities used to train these models. Representational similarity is generally measured for individual datasets and is not necessarily consistent across datasets. Thus, one may wonder whether this convergence of model representations is confounded by the datasets commonly used in machine learning. Here, we propose a systematic way to measure how representational similarity between models varies with the set of stimuli used to construct the representations. We find that the objective function is the most crucial factor in determining the consistency of representational similarities across datasets. Specifically, self-supervised vision models learn representations whose relative pairwise similarities generalize better from one dataset to another compared to those of image classification or image-text models. Moreover, the correspondence between representational similarities and the models' task behavior is dataset-dependent, being most strongly pronounced for single-domain datasets. Our work provides a framework for systematically measuring similarities of model representations across datasets and linking those similarities to differences in task behavior.