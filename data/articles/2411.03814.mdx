---
title: MRJ-Agent: An Effective Jailbreak Agent for Multi-Round Dialogue
date: 2024_11_06
tags: ['multi-round dialogue', 'psychological strategies', 'Large Language Models', 'jailbreak attacks', 'prompt engineering', 'risk decomposition', 'safety capabilities', 'attack success rate', 'vulnerabilities']
categories: ['cs.CL', 'cs.AI', 'cs.CR']
problem: multi-round dialogue jailbreaking agent
solution: ['jailbreak risks in multi-round dialogues']
pdf_url: http://arxiv.org/pdf/2411.03814v1
arx_url: http://arxiv.org/abs/2411.03814v1
score: 5
authors: ['Fengxiang Wang', 'Ranjie Duan', 'Peng Xiao', 'Xiaojun Jia', 'YueFeng Chen', 'Chongwen Wang', 'Jialing Tao', 'Hang Su', 'Jun Zhu', 'Hui Xue']
affiliations_aligned: ['Alibaba', 'Alibaba', 'Alibaba', '', 'Alibaba', '', 'Alibaba', '', '', 'Alibaba']
affiliations: ['', 'Alibaba']
---


Large Language Models (LLMs) demonstrate outstanding performance in their
reservoir of knowledge and understanding capabilities, but they have also been
shown to be prone to illegal or unethical reactions when subjected to jailbreak
attacks. To ensure their responsible deployment in critical applications, it is
crucial to understand the safety capabilities and vulnerabilities of LLMs.
Previous works mainly focus on jailbreak in single-round dialogue, overlooking
the potential jailbreak risks in multi-round dialogues, which are a vital way
humans interact with and extract information from LLMs. Some studies have
increasingly concentrated on the risks associated with jailbreak in multi-round
dialogues. These efforts typically involve the use of manually crafted
templates or prompt engineering techniques. However, due to the inherent
complexity of multi-round dialogues, their jailbreak performance is limited. To
solve this problem, we propose a novel multi-round dialogue jailbreaking agent,
emphasizing the importance of stealthiness in identifying and mitigating
potential threats to human values posed by LLMs. We propose a risk
decomposition strategy that distributes risks across multiple rounds of queries
and utilizes psychological strategies to enhance attack strength. Extensive
experiments show that our proposed method surpasses other attack methods and
achieves state-of-the-art attack success rate. We will make the corresponding
code and dataset available for future research. The code will be released soon.