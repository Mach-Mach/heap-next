---
title: 'Enhancing Multiple Dimensions of Trustworthiness in LLMs via Sparse Activation Control'
date: 2024-11-04
tags: ['safety', 'trustworthiness', 'Sparse Activation Control', 'Reinforcement Learning from Human Feedback', 'representation engineering', 'attention heads', 'bias', 'Large Language Models', 'factuality', 'semantic features']
categories: ['cs.CL', 'cs.AI']
problem: S
solution: ['difficulty in fulfilling multiple requirements simultaneously']
pdf_url: http://arxiv.org/pdf/2411.02461v1
arx_url: http://arxiv.org/abs/2411.02461v1
score: 5
authors: ['Yuxin Xiao', 'Chaoqun Wan', 'Yonggang Zhang', 'Wenxiao Wang', 'Binbin Lin', 'Xiaofei He', 'Xu Shen', 'Jieping Ye']
affiliations_aligned: ['State Key Lab of CAD&CG, Zhejiang University', 'Alibaba Cloud', 'Hong Kong Baptist University', 'School of Software Technology, Zhejiang University', 'School of Software Technology, Zhejiang University', 'State Key Lab of CAD&CG, Zhejiang University', 'Alibaba Cloud', 'Alibaba Cloud']
affiliations: ['Hong Kong Baptist University', 'School of Software Technology, Zhejiang University', 'Alibaba Cloud', 'State Key Lab of CAD&CG, Zhejiang University']
---


As the development and application of Large Language Models (LLMs) continue to advance rapidly, enhancing their trustworthiness and aligning them with human preferences has become a critical area of research. Traditional methods rely heavily on extensive data for Reinforcement Learning from Human Feedback (RLHF), but representation engineering offers a new, training-free approach. This technique leverages semantic features to control the representation of LLM's intermediate hidden states, enabling the model to meet specific requirements such as increased honesty or heightened safety awareness. However, a significant challenge arises when attempting to fulfill multiple requirements simultaneously. It proves difficult to encode various semantic contents, like honesty and safety, into a singular semantic feature, restricting its practicality. In this work, we address this issue through ``Sparse Activation Control''. By delving into the intrinsic mechanisms of LLMs, we manage to identify and pinpoint components that are closely related to specific tasks within the model, i.e., attention heads. These heads display sparse characteristics that allow for near-independent control over different tasks. Our experiments, conducted on the open-source Llama series models, have yielded encouraging results. The models were able to align with human preferences on issues of safety, factuality, and bias concurrently.