---
title: LLM-Ref: Enhancing Reference Handling in Technical Writing with Large
  Language Models
date: 2024_11_01
tags: ['writing assistant tool', 'Large Language Models', 'domain-specific tasks', 'retrieval-augmented generation', 'accuracy', 'Ragas score', 'contextual relevance', 'data synthesis', 'reference handling', 'iterative response generation']
categories: ['cs.CL', 'I.2.7']
problem: LLM-Ref writing assistant tool
solution: ['inaccuracy in domain-specific tasks of LLMs']
pdf_url: http://arxiv.org/pdf/2411.00294v2
arx_url: http://arxiv.org/abs/2411.00294v2
score: 5
authors: ['Kazi Ahmed Asif Fuad', 'Lizhong Chen']
affiliations_aligned: ['Oregon State University', 'Oregon State University']
affiliations: ['Oregon State University']
---


Large Language Models (LLMs) excel in data synthesis but can be inaccurate in
domain-specific tasks, which retrieval-augmented generation (RAG) systems
address by leveraging user-provided data. However, RAGs require optimization in
both retrieval and generation stages, which can affect output quality. In this
paper, we present LLM-Ref, a writing assistant tool that aids researchers in
writing articles from multiple source documents with enhanced reference
synthesis and handling capabilities. Unlike traditional RAG systems that use
chunking and indexing, our tool retrieves and generates content directly from
text paragraphs. This method facilitates direct reference extraction from the
generated outputs, a feature unique to our tool. Additionally, our tool employs
iterative response generation, effectively managing lengthy contexts within the
language model's constraints. Compared to baseline RAG-based systems, our
approach achieves a $3.25\times$ to $6.26\times$ increase in Ragas score, a
comprehensive metric that provides a holistic view of a RAG system's ability to
produce accurate, relevant, and contextually appropriate responses. This
improvement shows our method enhances the accuracy and contextual relevance of
writing assistance tools.