---
title: Multimodal Commonsense Knowledge Distillation for Visual Question
  Answering
date: 2024_11_05
tags: ['Visual Question Answering', 'commonsense knowledge', 'ScienceQA dataset', 'Multimodal Large Language Models', 'teacher-student environment', 'Graph Convolutional Network', 'Visual Language Pretrained Models', 'knowledge distillation']
categories: ['cs.CL', 'cs.AI']
problem: graph-based multimodal commonsense knowledge distillation framework
solution: ['VQA questions requiring external commonsense knowledge']
pdf_url: http://arxiv.org/pdf/2411.02722v1
arx_url: http://arxiv.org/abs/2411.02722v1
score: 6
authors: ['Shuo Yang', 'Siwen Luo', 'Soyeon Caren Han']
affiliations_aligned: ['', '', '']
affiliations: ['']
---


Existing Multimodal Large Language Models (MLLMs) and Visual Language
Pretrained Models (VLPMs) have shown remarkable performances in the general
Visual Question Answering (VQA). However, these models struggle with VQA
questions that require external commonsense knowledge due to the challenges in
generating high-quality prompts and the high computational costs of
fine-tuning. In this work, we propose a novel graph-based multimodal
commonsense knowledge distillation framework that constructs a unified
relational graph over commonsense knowledge, visual objects and questions
through a Graph Convolutional Network (GCN) following a teacher-student
environment. This proposed framework is flexible with any type of teacher and
student models without further fine-tuning, and has achieved competitive
performances on the ScienceQA dataset.