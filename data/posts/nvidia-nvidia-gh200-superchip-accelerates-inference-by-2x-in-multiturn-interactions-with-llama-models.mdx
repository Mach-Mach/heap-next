---
title: 'NVIDIA GH200 Superchip Accelerates Inference by 2x in Multiturn Interactions with Llama Models'
date: 2024-10-28
categories: ['Data Center / Cloud', 'Generative AI', 'Top Stories', 'featured', 'Inference Performance', 'LLMs', 'Retrieval Augmented Generation (RAG)']
url: https://developer.nvidia.com/blog/nvidia-gh200-superchip-accelerates-inference-by-2x-in-multiturn-interactions-with-llama-models/
company: NVIDIA
authors: ['Amr Elmeleegy']
summary: 'Deploying large language models (LLMs) in production environments often requires making hard trade-offs between enhancing user interactivity and increasing...'
---


![](https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/grace-
superchip-768x432.png)Deploying large language models (LLMs) in production
environments often requires making hard trade-offs between enhancing user
interactivity and increasing...![](https://developer-blogs.nvidia.com/wp-
content/uploads/2024/10/grace-superchip-768x432.png)

Deploying large language models (LLMs) in production environments often
requires making hard trade-offs between enhancing user interactivity and
increasing system throughput. While enhancing user interactivity requires
minimizing time to first token (TTFT), increasing throughput requires
increasing tokens per second. Improving one aspect often results in the
decline of the otherâ€¦

[Source](https://developer.nvidia.com/blog/nvidia-gh200-superchip-accelerates-
inference-by-2x-in-multiturn-interactions-with-llama-models/)

