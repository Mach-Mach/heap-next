---
title: 'Knowledge distillation for better convergence in multitask learning'
date: 2022-07-13
categories: []
url: https://www.amazon.science/blog/knowledge-distillation-for-better-convergence-in-multitask-learning
company: Amazon
authors: []
summary: 'Allowing separate tasks to converge on their own schedules and using knowledge distillation to maintain performance improves accuracy.'
---




