---
title: 'Boosting Llama 3.1 405B Throughput by Another 1.5x on NVIDIA H200 Tensor Core GPUs and NVLink Switch'
date: 2024-10-09
categories: ['Data Center / Cloud', 'Generative AI', 'Top Stories', 'AI Inference / Inference Microservices', 'featured', 'Inference Performance', 'Llama', 'LLMs']
url: https://developer.nvidia.com/blog/boosting-llama-3-1-405b-throughput-by-another-1-5x-on-nvidia-h200-tensor-core-gpus-and-nvlink-switch/
company: NVIDIA
authors: ['Nick Comly']
summary: 'The continued growth of LLMs capability, fueled by increasing parameter counts and support for longer contexts, has led to their usage in a wide variety of...'
---


![](https://developer-blogs.nvidia.com/wp-
content/uploads/2024/10/HGX-H200-product-photo-close-up-copy-2-768x432.jpg)The
continued growth of LLMs capability, fueled by increasing parameter counts and
support for longer contexts, has led to their usage in a wide variety
of...![](https://developer-blogs.nvidia.com/wp-
content/uploads/2024/10/HGX-H200-product-photo-close-up-copy-2-768x432.jpg)

The continued growth of LLMs capability, fueled by increasing parameter counts
and support for longer contexts, has led to their usage in a wide variety of
applications, each with diverse deployment requirements. For example, a
chatbot supports a small number of users at very low latencies for good
interactivity. Meanwhile, synthetic data generation requires high throughput
to process many itemsâ€¦

[Source](https://developer.nvidia.com/blog/boosting-llama-3-1-405b-throughput-
by-another-1-5x-on-nvidia-h200-tensor-core-gpus-and-nvlink-switch/)

