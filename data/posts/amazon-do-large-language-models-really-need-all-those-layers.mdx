---
title: 'Do large language models really need all those layers?'
date: 2023-07-09
categories: []
url: https://www.amazon.science/blog/do-large-language-models-really-need-all-those-layers
company: Amazon
authors: []
summary: 'Finding that 70% of attention heads and 20% of feed-forward networks can be excised with minimal effect on in-context learning suggests that large language models are undertrained.'
---




