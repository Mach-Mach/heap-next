---
title: 'Knowledge distillation method for better vision-language models'
date: 2024-02-22
categories: []
url: https://www.amazon.science/blog/knowledge-distillation-method-for-better-vision-language-models
company: Amazon
authors: []
summary: 'Method preserves knowledge encoded in teacher modelâ€™s attention heads even when student model has fewer of them.'
---




