---
title: 'NVIDIA NIM 1.4 Ready to Deploy with 2.4x Faster Inference'
date: 2024-11-16
categories: ['Data Center / Cloud', 'AI Inference / Inference Microservices', 'Inference Performance', 'LLMs']
url: https://developer.nvidia.com/blog/nvidia-nim-1-4-ready-to-deploy-with-2-4x-faster-inference/
company: NVIDIA
authors: ['Bethann Noble']
summary: 'The demand for ready-to-deploy high-performance inference is growing as generative AI reshapes industries. NVIDIA NIM provides production-ready microservice...'
---


![](https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/hpc-mlperf-
training-graphic-1-768x432.png)The demand for ready-to-deploy high-performance
inference is growing as generative AI reshapes industries. NVIDIA NIM provides
production-ready microservice...![](https://developer-blogs.nvidia.com/wp-
content/uploads/2023/11/hpc-mlperf-training-graphic-1-768x432.png)

The demand for ready-to-deploy high-performance inference is growing as
generative AI reshapes industries. NVIDIA NIM provides production-ready
microservice containers for AI model inference, constantly improving
enterprise-grade generative AI performance. With the upcoming NIM version 1.4
scheduled for release in early December, request performance is improved by up
to 2.4x out-of-the-box withâ€¦

[Source](https://developer.nvidia.com/blog/nvidia-nim-1-4-ready-to-deploy-
with-2-4x-faster-inference/)

