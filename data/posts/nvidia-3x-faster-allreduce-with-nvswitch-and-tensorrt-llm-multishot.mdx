---
title: '3x Faster AllReduce with NVSwitch and TensorRT-LLM MultiShot'
date: 2024-11-01
categories: ['AI Platforms / Deployment', 'Generative AI', 'Top Stories', 'AI Inference / Inference Microservices', 'featured', 'Inference Performance']
url: https://developer.nvidia.com/blog/3x-faster-allreduce-with-nvswitch-and-tensorrt-llm-multishot/
company: NVIDIA
authors: ['Anton Korzh']
summary: 'Deploying generative AI workloads in production environments where user numbers can fluctuate from hundreds to hundreds of thousands – and where input...'
---


![Image of an HGX H200](https://developer-blogs.nvidia.com/wp-
content/uploads/2024/08/HGX-H200-tech-blog-1920x1080-1-768x432.jpg)Deploying
generative AI workloads in production environments where user numbers can
fluctuate from hundreds to hundreds of thousands – and where input...![Image
of an HGX H200](https://developer-blogs.nvidia.com/wp-
content/uploads/2024/08/HGX-H200-tech-blog-1920x1080-1-768x432.jpg)

Deploying generative AI workloads in production environments where user
numbers can fluctuate from hundreds to hundreds of thousands – and where input
sequence lengths differ with each request – poses unique challenges. To
achieve low latency inference in these environments, multi-GPU setups are a
must – irrespective of the GPU generation or its memory capacity. To enhance
inference performance in…

[Source](https://developer.nvidia.com/blog/3x-faster-allreduce-with-nvswitch-
and-tensorrt-llm-multishot/)

