---
title: '5x Faster Time to First Token with NVIDIA TensorRT-LLM KV Cache Early Reuse'
date: 2024-11-08
categories: ['AI Platforms / Deployment', 'Data Science', 'Generative AI', 'AI Inference / Inference Microservices', 'featured', 'Inference Performance', 'Llama']
url: https://developer.nvidia.com/blog/5x-faster-time-to-first-token-with-nvidia-tensorrt-llm-kv-cache-early-reuse/
company: NVIDIA
authors: ['Amr Elmeleegy']
summary: 'In our previous blog post, we demonstrated how reusing the key-value (KV) cache by offloading it to CPU memory can accelerate time to first token (TTFT) by up...'
---


![NVIDIA H100.](https://developer-blogs.nvidia.com/wp-
content/uploads/2024/11/h100-768x432.jpg)In our previous blog post, we
demonstrated how reusing the key-value (KV) cache by offloading it to CPU
memory can accelerate time to first token (TTFT) by up...![NVIDIA
H100.](https://developer-blogs.nvidia.com/wp-
content/uploads/2024/11/h100-768x432.jpg)

In our previous blog post, we demonstrated how reusing the key-value (KV)
cache by offloading it to CPU memory can accelerate time to first token (TTFT)
by up to 14x on x86-based NVIDIA H100 Tensor Core GPUs and 28x on the NVIDIA
GH200 Superchip. In this post, we shed light on KV cache reuse techniques and
best practices that can drive even further TTFT speedups. LLM models are
rapidlyâ€¦

[Source](https://developer.nvidia.com/blog/5x-faster-time-to-first-token-with-
nvidia-tensorrt-llm-kv-cache-early-reuse/)

