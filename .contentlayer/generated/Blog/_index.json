[
  {
    "title": "ACC-Debate: An Actor-Critic Approach to Multi-Agent Debate",
    "date": "2024-10-30T00:00:00.000Z",
    "tags": [
      "multi-agent debate",
      "debate frameworks",
      "benchmark performance",
      "large language models",
      "model efficacy",
      "actor-critic approach",
      "collaborative behaviors"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2411.00053v2",
    "arx_url": "http://arxiv.org/abs/2411.00053v2",
    "authors": [
      "Andrew Estornell",
      "Jean-Francois Ton",
      "Yuanshun Yao",
      "Yang Liu",
      "Jean-FranÃ§ois Ton"
    ],
    "affiliations_aligned": [
      "ByteDance Research",
      "",
      "Meta GenAI",
      "University of California, Santa Cruz",
      "ByteDance Research"
    ],
    "affiliations": [
      "",
      "Meta GenAI",
      "University of California, Santa Cruz",
      "ByteDance Research"
    ],
    "problem": "limitations of current debate frameworks",
    "solution": "ACC-Debate learning framework",
    "score": 6,
    "body": {
      "raw": "\n\nLarge language models (LLMs) have demonstrated a remarkable ability to serve as general-purpose tools for various language-based tasks. Recent works have demonstrated that the efficacy of such models can be improved through iterative dialog between multiple models, frequently referred to as multi-agent debate (MAD). While debate shows promise as a means of improving model efficacy, most works in this area treat debate as an emergent behavior, rather than a learned behavior. In doing so, current debate frameworks rely on collaborative behaviors to have been sufficiently trained into off-the-shelf models. To address this limitation, we propose ACC-Debate, an Actor-Critic based learning framework to produce a two-agent team specialized in debate. We demonstrate that ACC-Debate outperforms SotA debate techniques on a wide array of benchmarks.",
      "code": "var Component=(()=>{var m=Object.create;var o=Object.defineProperty;var h=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var b=Object.getPrototypeOf,p=Object.prototype.hasOwnProperty;var g=(e,a)=>()=>(a||e((a={exports:{}}).exports,a),a.exports),v=(e,a)=>{for(var t in a)o(e,t,{get:a[t],enumerable:!0})},s=(e,a,t,i)=>{if(a&&typeof a==\"object\"||typeof a==\"function\")for(let r of u(a))!p.call(e,r)&&r!==t&&o(e,r,{get:()=>a[r],enumerable:!(i=h(a,r))||i.enumerable});return e};var C=(e,a,t)=>(t=e!=null?m(b(e)):{},s(a||!e||!e.__esModule?o(t,\"default\",{value:e,enumerable:!0}):t,e)),A=e=>s(o({},\"__esModule\",{value:!0}),e);var l=g((D,c)=>{c.exports=_jsx_runtime});var y={};v(y,{default:()=>f,frontmatter:()=>w});var n=C(l()),w={title:\"ACC-Debate: An Actor-Critic Approach to Multi-Agent Debate\",date:\"2024-10-30\",tags:[\"multi-agent debate\",\"debate frameworks\",\"benchmark performance\",\"large language models\",\"model efficacy\",\"actor-critic approach\",\"collaborative behaviors\"],categories:[\"cs.CL\",\"cs.AI\"],problem:\"limitations of current debate frameworks\",solution:\"ACC-Debate learning framework\",pdf_url:\"http://arxiv.org/pdf/2411.00053v2\",arx_url:\"http://arxiv.org/abs/2411.00053v2\",score:6,authors:[\"Andrew Estornell\",\"Jean-Francois Ton\",\"Yuanshun Yao\",\"Yang Liu\",\"Jean-Fran\\xE7ois Ton\"],affiliations_aligned:[\"ByteDance Research\",\"\",\"Meta GenAI\",\"University of California, Santa Cruz\",\"ByteDance Research\"],affiliations:[\"\",\"Meta GenAI\",\"University of California, Santa Cruz\",\"ByteDance Research\"]};function d(e){let a={p:\"p\",...e.components};return(0,n.jsx)(a.p,{children:\"Large language models (LLMs) have demonstrated a remarkable ability to serve as general-purpose tools for various language-based tasks. Recent works have demonstrated that the efficacy of such models can be improved through iterative dialog between multiple models, frequently referred to as multi-agent debate (MAD). While debate shows promise as a means of improving model efficacy, most works in this area treat debate as an emergent behavior, rather than a learned behavior. In doing so, current debate frameworks rely on collaborative behaviors to have been sufficiently trained into off-the-shelf models. To address this limitation, we propose ACC-Debate, an Actor-Critic based learning framework to produce a two-agent team specialized in debate. We demonstrate that ACC-Debate outperforms SotA debate techniques on a wide array of benchmarks.\"})}function f(e={}){let{wrapper:a}=e.components||{};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}return A(y);})();\n;return Component;"
    },
    "_id": "articles/2411.00053.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.00053.mdx",
      "sourceFileName": "2411.00053.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.00053"
    },
    "type": "Blog",
    "readingTime": {
      "text": "1 min read",
      "minutes": 0.615,
      "time": 36900,
      "words": 123
    },
    "slug": "2411.00053",
    "path": "articles/2411.00053",
    "filePath": "articles/2411.00053.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "ACC-Debate: An Actor-Critic Approach to Multi-Agent Debate",
      "datePublished": "2024-10-30T00:00:00.000Z",
      "dateModified": "2024-10-30T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.00053"
    }
  },
  {
    "title": "Interpretable Language Modeling via Induction-head Ngram Models",
    "date": "2024-10-31T00:00:00.000Z",
    "tags": [
      "large language models",
      "Induction-head ngram models",
      "interpretability",
      "efficiency",
      "natural-language neuroscience",
      "next-word prediction",
      "language selectivity",
      "speculative decoding",
      "neural similarity metric",
      "fMRI response prediction"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2411.00066v1",
    "arx_url": "http://arxiv.org/abs/2411.00066v1",
    "authors": [
      "Eunji Kim",
      "Sriya Mantena",
      "Weiwei Yang",
      "Chandan Singh",
      "Sungroh Yoon",
      "Jianfeng Gao"
    ],
    "affiliations_aligned": [
      "Department of Electrical and Computer Engineering, Seoul National University",
      "Stanford University",
      "Microsoft Research",
      "Microsoft Research",
      "Interdisciplinary Program in Artificial Intelligence, Seoul National University",
      "Microsoft Research"
    ],
    "affiliations": [
      "Microsoft Research",
      "Stanford University",
      "Department of Electrical and Computer Engineering, Seoul National University",
      "Interdisciplinary Program in Artificial Intelligence, Seoul National University"
    ],
    "problem": "demand for interpretability and efficiency in language models",
    "solution": "Induction-head ngram models",
    "score": 7,
    "body": {
      "raw": "\n\nRecent large language models (LLMs) have excelled across a wide range of tasks, but their use in high-stakes and compute-limited settings has intensified the demand for interpretability and efficiency. We address this need by proposing Induction-head ngram models (Induction-Gram), a method that builds an efficient, interpretable LM by bolstering modern ngram models with a hand-engineered \"induction head\". This induction head uses a custom neural similarity metric to efficiently search the model's input context for potential next-word completions. This process enables Induction-Gram to provide ngram-level grounding for each generated token. Moreover, experiments show that this simple method significantly improves next-word prediction over baseline interpretable models (up to 26%p) and can be used to speed up LLM inference for large models through speculative decoding. We further study Induction-Gram in a natural-language neuroscience setting, where the goal is to predict the next fMRI response in a sequence. It again provides a significant improvement over interpretable models (20% relative increase in the correlation of predicted fMRI responses), potentially enabling deeper scientific investigation of language selectivity in the brain. The code is available at https://github.com/ejkim47/induction-gram.",
      "code": "var Component=(()=>{var u=Object.create;var r=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var h=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var v=(e,i)=>()=>(i||e((i={exports:{}}).exports,i),i.exports),y=(e,i)=>{for(var n in i)r(e,n,{get:i[n],enumerable:!0})},s=(e,i,n,o)=>{if(i&&typeof i==\"object\"||typeof i==\"function\")for(let a of p(i))!f.call(e,a)&&a!==n&&r(e,a,{get:()=>i[a],enumerable:!(o=m(i,a))||o.enumerable});return e};var b=(e,i,n)=>(n=e!=null?u(h(e)):{},s(i||!e||!e.__esModule?r(n,\"default\",{value:e,enumerable:!0}):n,e)),x=e=>s(r({},\"__esModule\",{value:!0}),e);var c=v((L,l)=>{l.exports=_jsx_runtime});var M={};y(M,{default:()=>g,frontmatter:()=>I});var t=b(c()),I={title:\"Interpretable Language Modeling via Induction-head Ngram Models\",date:\"2024-10-31\",tags:[\"large language models\",\"Induction-head ngram models\",\"interpretability\",\"efficiency\",\"natural-language neuroscience\",\"next-word prediction\",\"language selectivity\",\"speculative decoding\",\"neural similarity metric\",\"fMRI response prediction\"],categories:[\"cs.CL\",\"cs.AI\",\"cs.LG\"],problem:\"demand for interpretability and efficiency in language models\",solution:\"Induction-head ngram models\",pdf_url:\"http://arxiv.org/pdf/2411.00066v1\",arx_url:\"http://arxiv.org/abs/2411.00066v1\",score:7,authors:[\"Eunji Kim\",\"Sriya Mantena\",\"Weiwei Yang\",\"Chandan Singh\",\"Sungroh Yoon\",\"Jianfeng Gao\"],affiliations_aligned:[\"Department of Electrical and Computer Engineering, Seoul National University\",\"Stanford University\",\"Microsoft Research\",\"Microsoft Research\",\"Interdisciplinary Program in Artificial Intelligence, Seoul National University\",\"Microsoft Research\"],affiliations:[\"Microsoft Research\",\"Stanford University\",\"Department of Electrical and Computer Engineering, Seoul National University\",\"Interdisciplinary Program in Artificial Intelligence, Seoul National University\"]};function d(e){let i={a:\"a\",p:\"p\",...e.components};return(0,t.jsxs)(i.p,{children:[`Recent large language models (LLMs) have excelled across a wide range of tasks, but their use in high-stakes and compute-limited settings has intensified the demand for interpretability and efficiency. We address this need by proposing Induction-head ngram models (Induction-Gram), a method that builds an efficient, interpretable LM by bolstering modern ngram models with a hand-engineered \"induction head\". This induction head uses a custom neural similarity metric to efficiently search the model's input context for potential next-word completions. This process enables Induction-Gram to provide ngram-level grounding for each generated token. Moreover, experiments show that this simple method significantly improves next-word prediction over baseline interpretable models (up to 26%p) and can be used to speed up LLM inference for large models through speculative decoding. We further study Induction-Gram in a natural-language neuroscience setting, where the goal is to predict the next fMRI response in a sequence. It again provides a significant improvement over interpretable models (20% relative increase in the correlation of predicted fMRI responses), potentially enabling deeper scientific investigation of language selectivity in the brain. The code is available at `,(0,t.jsx)(i.a,{href:\"https://github.com/ejkim47/induction-gram\",children:\"https://github.com/ejkim47/induction-gram\"}),\".\"]})}function g(e={}){let{wrapper:i}=e.components||{};return i?(0,t.jsx)(i,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}return x(M);})();\n;return Component;"
    },
    "_id": "articles/2411.00066.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.00066.mdx",
      "sourceFileName": "2411.00066.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.00066"
    },
    "type": "Blog",
    "readingTime": {
      "text": "1 min read",
      "minutes": 0.9,
      "time": 54000,
      "words": 180
    },
    "slug": "2411.00066",
    "path": "articles/2411.00066",
    "filePath": "articles/2411.00066.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "Interpretable Language Modeling via Induction-head Ngram Models",
      "datePublished": "2024-10-31T00:00:00.000Z",
      "dateModified": "2024-10-31T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.00066"
    }
  },
  {
    "title": "RSL-SQL: Robust Schema Linking in Text-to-SQL Generation",
    "date": "2024-10-31T00:00:00.000Z",
    "tags": [
      "DeepSeek",
      "large language models",
      "contextual information augmentation",
      "Text-to-SQL generation",
      "computational overhead",
      "schema linking",
      "execution accuracy",
      "ablation studies",
      "multi-turn self-correction"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB"
    ],
    "pdf_url": "http://arxiv.org/pdf/2411.00073v1",
    "arx_url": "http://arxiv.org/abs/2411.00073v1",
    "authors": [
      "Zhenbiao Cao",
      "Yuanlei Zheng",
      "Zhihao Fan",
      "Xiaojin Zhang",
      "Wei Chen"
    ],
    "affiliations_aligned": [
      "Huazhong University of Science and Technology",
      "Huazhong University of Science and Technology",
      "Alibaba Inc.",
      "Huazhong University of Science and Technology",
      "Huazhong University of Science and Technology"
    ],
    "affiliations": [
      "Huazhong University of Science and Technology",
      "Alibaba Inc."
    ],
    "problem": "risks in schema linking for Text-to-SQL generation",
    "solution": "RSL-SQL framework",
    "score": 6,
    "body": {
      "raw": "\n\nText-to-SQL generation aims to translate natural language questions into SQL statements. In large language models (LLMs) based Text-to-SQL, schema linking is a widely adopted strategy to streamline the input for LLMs by selecting only relevant schema elements, therefore reducing noise and computational overhead. However, schema linking faces risks that requires caution, including the potential omission of necessary elements and disruption of database structural integrity. To address these challenges, we propose a novel framework called RSL-SQL that combines bidirectional schema linking, contextual information augmentation, binary selection strategy, and multi-turn self-correction. Our approach improves the recall of schema linking through forward and backward pruning and hedges the risk by voting between full schema and contextual information augmented simplified schema. Experiments on the BIRD and Spider benchmarks demonstrate that our approach achieves state-of-the-art execution accuracy among open-source solutions, with 67.2% on BIRD and 87.9% on Spider using GPT-4o. Furthermore, our approach outperforms a series of GPT-4 based Text-to-SQL systems when adopting DeepSeek (much cheaper) with same intact prompts. Extensive analysis and ablation studies confirm the effectiveness of each component in our framework. The codes are available at https://github.com/Laqcce-cao/RSL-SQL.",
      "code": "var Component=(()=>{var m=Object.create;var i=Object.defineProperty;var g=Object.getOwnPropertyDescriptor;var d=Object.getOwnPropertyNames;var f=Object.getPrototypeOf,p=Object.prototype.hasOwnProperty;var S=(e,n)=>()=>(n||e((n={exports:{}}).exports,n),n.exports),L=(e,n)=>{for(var a in n)i(e,a,{get:n[a],enumerable:!0})},s=(e,n,a,r)=>{if(n&&typeof n==\"object\"||typeof n==\"function\")for(let o of d(n))!p.call(e,o)&&o!==a&&i(e,o,{get:()=>n[o],enumerable:!(r=g(n,o))||r.enumerable});return e};var x=(e,n,a)=>(a=e!=null?m(f(e)):{},s(n||!e||!e.__esModule?i(a,\"default\",{value:e,enumerable:!0}):a,e)),b=e=>s(i({},\"__esModule\",{value:!0}),e);var l=S((T,c)=>{c.exports=_jsx_runtime});var v={};L(v,{default:()=>u,frontmatter:()=>y});var t=x(l()),y={title:\"RSL-SQL: Robust Schema Linking in Text-to-SQL Generation\",date:\"2024-10-31\",tags:[\"DeepSeek\",\"large language models\",\"contextual information augmentation\",\"Text-to-SQL generation\",\"computational overhead\",\"schema linking\",\"execution accuracy\",\"ablation studies\",\"multi-turn self-correction\"],categories:[\"cs.CL\",\"cs.AI\",\"cs.DB\"],problem:\"risks in schema linking for Text-to-SQL generation\",solution:\"RSL-SQL framework\",pdf_url:\"http://arxiv.org/pdf/2411.00073v1\",arx_url:\"http://arxiv.org/abs/2411.00073v1\",score:6,authors:[\"Zhenbiao Cao\",\"Yuanlei Zheng\",\"Zhihao Fan\",\"Xiaojin Zhang\",\"Wei Chen\"],affiliations_aligned:[\"Huazhong University of Science and Technology\",\"Huazhong University of Science and Technology\",\"Alibaba Inc.\",\"Huazhong University of Science and Technology\",\"Huazhong University of Science and Technology\"],affiliations:[\"Huazhong University of Science and Technology\",\"Alibaba Inc.\"]};function h(e){let n={a:\"a\",p:\"p\",...e.components};return(0,t.jsxs)(n.p,{children:[\"Text-to-SQL generation aims to translate natural language questions into SQL statements. In large language models (LLMs) based Text-to-SQL, schema linking is a widely adopted strategy to streamline the input for LLMs by selecting only relevant schema elements, therefore reducing noise and computational overhead. However, schema linking faces risks that requires caution, including the potential omission of necessary elements and disruption of database structural integrity. To address these challenges, we propose a novel framework called RSL-SQL that combines bidirectional schema linking, contextual information augmentation, binary selection strategy, and multi-turn self-correction. Our approach improves the recall of schema linking through forward and backward pruning and hedges the risk by voting between full schema and contextual information augmented simplified schema. Experiments on the BIRD and Spider benchmarks demonstrate that our approach achieves state-of-the-art execution accuracy among open-source solutions, with 67.2% on BIRD and 87.9% on Spider using GPT-4o. Furthermore, our approach outperforms a series of GPT-4 based Text-to-SQL systems when adopting DeepSeek (much cheaper) with same intact prompts. Extensive analysis and ablation studies confirm the effectiveness of each component in our framework. The codes are available at \",(0,t.jsx)(n.a,{href:\"https://github.com/Laqcce-cao/RSL-SQL\",children:\"https://github.com/Laqcce-cao/RSL-SQL\"}),\".\"]})}function u(e={}){let{wrapper:n}=e.components||{};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}return b(v);})();\n;return Component;"
    },
    "_id": "articles/2411.00073.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.00073.mdx",
      "sourceFileName": "2411.00073.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.00073"
    },
    "type": "Blog",
    "readingTime": {
      "text": "1 min read",
      "minutes": 0.925,
      "time": 55500,
      "words": 185
    },
    "slug": "2411.00073",
    "path": "articles/2411.00073",
    "filePath": "articles/2411.00073.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "RSL-SQL: Robust Schema Linking in Text-to-SQL Generation",
      "datePublished": "2024-10-31T00:00:00.000Z",
      "dateModified": "2024-10-31T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.00073"
    }
  },
  {
    "title": "JudgeRank: Leveraging Large Language Models for Reasoning-Intensive Reranking",
    "date": "2024-10-31T00:00:00.000Z",
    "tags": [
      "document retrieval",
      "reasoning-intensive tasks",
      "large language models",
      "query analysis",
      "BRIGHT benchmark",
      "BEIR benchmark",
      "ablation studies",
      "zero-shot generalization",
      "reranking",
      "document analysis",
      "retrieval-augmented generation",
      "relevance judgment"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2411.00142v1",
    "arx_url": "http://arxiv.org/abs/2411.00142v1",
    "authors": [
      "Tong Niu",
      "Shafiq Joty",
      "Ye Liu",
      "Caiming Xiong",
      "Yingbo Zhou",
      "Semih Yavuz"
    ],
    "affiliations_aligned": [
      "Salesforce AI Research",
      "Salesforce AI Research",
      "Salesforce AI Research",
      "Salesforce AI Research",
      "Salesforce AI Research",
      "Salesforce AI Research"
    ],
    "affiliations": [
      "Salesforce AI Research"
    ],
    "problem": "lack of nuanced analysis in judging document relevance",
    "solution": "JudgeRank, a novel agentic reranker",
    "score": 8,
    "body": {
      "raw": "\n\nAccurate document retrieval is crucial for the success of retrieval-augmented generation (RAG) applications, including open-domain question answering and code completion. While large language models (LLMs) have been employed as dense encoders or listwise rerankers in RAG systems, they often struggle with reasoning-intensive tasks because they lack nuanced analysis when judging document relevance. To address this limitation, we introduce JudgeRank, a novel agentic reranker that emulates human cognitive processes when assessing document relevance. Our approach consists of three key steps: (1) query analysis to identify the core problem, (2) document analysis to extract a query-aware summary, and (3) relevance judgment to provide a concise assessment of document relevance. We evaluate JudgeRank on the reasoning-intensive BRIGHT benchmark, demonstrating substantial performance improvements over first-stage retrieval methods and outperforming other popular reranking approaches. In addition, JudgeRank performs on par with fine-tuned state-of-the-art rerankers on the popular BEIR benchmark, validating its zero-shot generalization capability. Through comprehensive ablation studies, we demonstrate that JudgeRank's performance generalizes well across LLMs of various sizes while ensembling them yields even more accurate reranking than individual models.",
      "code": "var Component=(()=>{var d=Object.create;var t=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var h=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var v=(e,a)=>()=>(a||e((a={exports:{}}).exports,a),a.exports),k=(e,a)=>{for(var n in a)t(e,n,{get:a[n],enumerable:!0})},i=(e,a,n,o)=>{if(a&&typeof a==\"object\"||typeof a==\"function\")for(let r of h(a))!f.call(e,r)&&r!==n&&t(e,r,{get:()=>a[r],enumerable:!(o=m(a,r))||o.enumerable});return e};var R=(e,a,n)=>(n=e!=null?d(p(e)):{},i(a||!e||!e.__esModule?t(n,\"default\",{value:e,enumerable:!0}):n,e)),y=e=>i(t({},\"__esModule\",{value:!0}),e);var l=v((x,c)=>{c.exports=_jsx_runtime});var I={};k(I,{default:()=>g,frontmatter:()=>b});var s=R(l()),b={title:\"JudgeRank: Leveraging Large Language Models for Reasoning-Intensive Reranking\",date:\"2024-10-31\",tags:[\"document retrieval\",\"reasoning-intensive tasks\",\"large language models\",\"query analysis\",\"BRIGHT benchmark\",\"BEIR benchmark\",\"ablation studies\",\"zero-shot generalization\",\"reranking\",\"document analysis\",\"retrieval-augmented generation\",\"relevance judgment\"],categories:[\"cs.CL\",\"cs.AI\"],problem:\"lack of nuanced analysis in judging document relevance\",solution:\"JudgeRank, a novel agentic reranker\",pdf_url:\"http://arxiv.org/pdf/2411.00142v1\",arx_url:\"http://arxiv.org/abs/2411.00142v1\",score:8,authors:[\"Tong Niu\",\"Shafiq Joty\",\"Ye Liu\",\"Caiming Xiong\",\"Yingbo Zhou\",\"Semih Yavuz\"],affiliations_aligned:[\"Salesforce AI Research\",\"Salesforce AI Research\",\"Salesforce AI Research\",\"Salesforce AI Research\",\"Salesforce AI Research\",\"Salesforce AI Research\"],affiliations:[\"Salesforce AI Research\"]};function u(e){let a={p:\"p\",...e.components};return(0,s.jsx)(a.p,{children:\"Accurate document retrieval is crucial for the success of retrieval-augmented generation (RAG) applications, including open-domain question answering and code completion. While large language models (LLMs) have been employed as dense encoders or listwise rerankers in RAG systems, they often struggle with reasoning-intensive tasks because they lack nuanced analysis when judging document relevance. To address this limitation, we introduce JudgeRank, a novel agentic reranker that emulates human cognitive processes when assessing document relevance. Our approach consists of three key steps: (1) query analysis to identify the core problem, (2) document analysis to extract a query-aware summary, and (3) relevance judgment to provide a concise assessment of document relevance. We evaluate JudgeRank on the reasoning-intensive BRIGHT benchmark, demonstrating substantial performance improvements over first-stage retrieval methods and outperforming other popular reranking approaches. In addition, JudgeRank performs on par with fine-tuned state-of-the-art rerankers on the popular BEIR benchmark, validating its zero-shot generalization capability. Through comprehensive ablation studies, we demonstrate that JudgeRank's performance generalizes well across LLMs of various sizes while ensembling them yields even more accurate reranking than individual models.\"})}function g(e={}){let{wrapper:a}=e.components||{};return a?(0,s.jsx)(a,{...e,children:(0,s.jsx)(u,{...e})}):u(e)}return y(I);})();\n;return Component;"
    },
    "_id": "articles/2411.00142.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.00142.mdx",
      "sourceFileName": "2411.00142.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.00142"
    },
    "type": "Blog",
    "readingTime": {
      "text": "1 min read",
      "minutes": 0.88,
      "time": 52800,
      "words": 176
    },
    "slug": "2411.00142",
    "path": "articles/2411.00142",
    "filePath": "articles/2411.00142.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "JudgeRank: Leveraging Large Language Models for Reasoning-Intensive Reranking",
      "datePublished": "2024-10-31T00:00:00.000Z",
      "dateModified": "2024-10-31T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.00142"
    }
  },
  {
    "title": "GRSQA -- Graph Reasoning-Structured Question Answering Dataset",
    "date": "2024-11-08T00:00:00.000Z",
    "tags": [
      "reasoning graphs",
      "Large Language Models",
      "multi-hop question-answering",
      "evaluation of LLM reasoning capabilities",
      "reasoning structures",
      "reasoning pathways",
      "QA datasets",
      "semantic contexts",
      "Graph Reasoning-Structured Question Answering Dataset",
      "reasoning abilities"
    ],
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2411.00369",
    "arx_url": "https://arxiv.org/abs/2411.00369",
    "authors": [
      "Anish Pahilajani",
      "Devasha Trivedi",
      "Jincen Shuai",
      "Khin S. Yone",
      "Samyak Rajesh Jain",
      "Namyong Park",
      "Ryan A. Rossi",
      "Nesreen K. Ahmed",
      "Franck Dernoncourt",
      "Yu Wang"
    ],
    "affiliations_aligned": [
      "University of California Santa Cruz",
      "University of California Santa Cruz",
      "University of California Santa Cruz",
      "University of California Santa Cruz",
      "University of California Santa Cruz",
      "",
      "Adobe Research",
      "Cisco Outshift",
      "Adobe Research",
      "University of Oregon"
    ],
    "affiliations": [
      "",
      "Adobe Research",
      "University of Oregon",
      "University of California Santa Cruz",
      "Cisco Outshift"
    ],
    "problem": "impact of reasoning structures on LLM M-QA performance",
    "solution": "Graph Reasoning-Structured Question Answering Dataset",
    "score": 6,
    "body": {
      "raw": "\n\nLarge Language Models (LLMs) have excelled in multi-hop question-answering (M-QA) due to their advanced reasoning abilities. However, the impact of the inherent reasoning structures on LLM M-QA performance remains unclear, largely due to the absence of QA datasets that provide fine-grained reasoning structures. To address this gap, we introduce the Graph Reasoning-Structured Question Answering Dataset (GRS-QA), which includes both semantic contexts and reasoning structures for QA pairs. Unlike existing M-QA datasets, where different reasoning structures are entangled together, GRS-QA explicitly captures intricate reasoning pathways by constructing reasoning graphs, where nodes represent textual contexts and edges denote logical flows. These reasoning graphs of different structures enable a fine-grained evaluation of LLM reasoning capabilities across various reasoning structures. Our empirical analysis reveals that LLMs perform differently when handling questions with varying reasoning structures. This finding facilitates the exploration of textual structures as compared with semantics.",
      "code": "var Component=(()=>{var l=Object.create;var i=Object.defineProperty;var f=Object.getOwnPropertyDescriptor;var d=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,m=Object.prototype.hasOwnProperty;var v=(e,n)=>()=>(n||e((n={exports:{}}).exports,n),n.exports),y=(e,n)=>{for(var a in n)i(e,a,{get:n[a],enumerable:!0})},o=(e,n,a,s)=>{if(n&&typeof n==\"object\"||typeof n==\"function\")for(let t of d(n))!m.call(e,t)&&t!==a&&i(e,t,{get:()=>n[t],enumerable:!(s=f(n,t))||s.enumerable});return e};var A=(e,n,a)=>(a=e!=null?l(p(e)):{},o(n||!e||!e.__esModule?i(a,\"default\",{value:e,enumerable:!0}):a,e)),x=e=>o(i({},\"__esModule\",{value:!0}),e);var c=v((S,u)=>{u.exports=_jsx_runtime});var L={};y(L,{default:()=>h,frontmatter:()=>w});var r=A(c()),w={title:\"GRSQA -- Graph Reasoning-Structured Question Answering Dataset\",date:\"2024-11-08\",tags:[\"reasoning graphs\",\"Large Language Models\",\"multi-hop question-answering\",\"evaluation of LLM reasoning capabilities\",\"reasoning structures\",\"reasoning pathways\",\"QA datasets\",\"semantic contexts\",\"Graph Reasoning-Structured Question Answering Dataset\",\"reasoning abilities\"],categories:[\"cs.CL\"],problem:\"impact of reasoning structures on LLM M-QA performance\",solution:\"Graph Reasoning-Structured Question Answering Dataset\",pdf_url:\"https://arxiv.org/pdf/2411.00369\",arx_url:\"https://arxiv.org/abs/2411.00369\",score:6,authors:[\"Anish Pahilajani\",\"Devasha Trivedi\",\"Jincen Shuai\",\"Khin S. Yone\",\"Samyak Rajesh Jain\",\"Namyong Park\",\"Ryan A. Rossi\",\"Nesreen K. Ahmed\",\"Franck Dernoncourt\",\"Yu Wang\"],affiliations_aligned:[\"University of California Santa Cruz\",\"University of California Santa Cruz\",\"University of California Santa Cruz\",\"University of California Santa Cruz\",\"University of California Santa Cruz\",\"\",\"Adobe Research\",\"Cisco Outshift\",\"Adobe Research\",\"University of Oregon\"],affiliations:[\"\",\"Adobe Research\",\"University of Oregon\",\"University of California Santa Cruz\",\"Cisco Outshift\"]};function g(e){let n={p:\"p\",...e.components};return(0,r.jsx)(n.p,{children:\"Large Language Models (LLMs) have excelled in multi-hop question-answering (M-QA) due to their advanced reasoning abilities. However, the impact of the inherent reasoning structures on LLM M-QA performance remains unclear, largely due to the absence of QA datasets that provide fine-grained reasoning structures. To address this gap, we introduce the Graph Reasoning-Structured Question Answering Dataset (GRS-QA), which includes both semantic contexts and reasoning structures for QA pairs. Unlike existing M-QA datasets, where different reasoning structures are entangled together, GRS-QA explicitly captures intricate reasoning pathways by constructing reasoning graphs, where nodes represent textual contexts and edges denote logical flows. These reasoning graphs of different structures enable a fine-grained evaluation of LLM reasoning capabilities across various reasoning structures. Our empirical analysis reveals that LLMs perform differently when handling questions with varying reasoning structures. This finding facilitates the exploration of textual structures as compared with semantics.\"})}function h(e={}){let{wrapper:n}=e.components||{};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(g,{...e})}):g(e)}return x(L);})();\n;return Component;"
    },
    "_id": "articles/2411.00369.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.00369.mdx",
      "sourceFileName": "2411.00369.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.00369"
    },
    "type": "Blog",
    "readingTime": {
      "text": "1 min read",
      "minutes": 0.715,
      "time": 42900,
      "words": 143
    },
    "slug": "2411.00369",
    "path": "articles/2411.00369",
    "filePath": "articles/2411.00369.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "GRSQA -- Graph Reasoning-Structured Question Answering Dataset",
      "datePublished": "2024-11-08T00:00:00.000Z",
      "dateModified": "2024-11-08T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.00369"
    }
  },
  {
    "title": "Self-Evolved Reward Learning for LLMs",
    "date": "2024-11-01T00:00:00.000Z",
    "tags": [
      "biases",
      "human preferences",
      "training data",
      "Reinforcement Learning from Human Feedback",
      "reward model",
      "self-feedback",
      "large language models",
      "experiments",
      "language models",
      "datasets"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2411.00418v1",
    "arx_url": "http://arxiv.org/abs/2411.00418v1",
    "authors": [
      "Chenghua Huang",
      "Zhizhen Fan",
      "Lu Wang",
      "Fangkai Yang",
      "Pu Zhao",
      "Zeqi Lin",
      "Qingwei Lin",
      "Dongmei Zhang",
      "Saravan Rajmohan",
      "Qi Zhang"
    ],
    "affiliations_aligned": [
      "School of Computer Science, Fudan University",
      "School of Computer Science, Peking University",
      "Microsoft",
      "Microsoft",
      "Microsoft",
      "Microsoft",
      "Microsoft",
      "Microsoft",
      "Microsoft",
      "Microsoft"
    ],
    "affiliations": [
      "Microsoft",
      "School of Computer Science, Fudan University",
      "School of Computer Science, Peking University"
    ],
    "problem": "training a reliable reward model",
    "solution": "Self-Evolved Reward Learning",
    "score": 6,
    "body": {
      "raw": "\n\nReinforcement Learning from Human Feedback (RLHF) is a crucial technique for aligning language models with human preferences, playing a pivotal role in the success of conversational models like GPT-4, ChatGPT, and Llama 2. A core challenge in employing RLHF lies in training a reliable reward model (RM), which relies on high-quality labels typically provided by human experts or advanced AI system. These methods can be costly and may introduce biases that affect the language model's responses. As language models improve, human input may become less effective in further enhancing their performance. In this paper, we propose Self-Evolved Reward Learning (SER), a novel approach where the RM generates additional training data to iteratively improve itself. We conducted extensive experiments on multiple datasets such as HH-RLHF and UltraFeedback, using models like Mistral and Llama 3, and compare SER against various baselines. Our results demonstrate that even with limited human-annotated data, learning from self-feedback can robustly enhance RM performance, thereby boosting the capabilities of large language models (LLMs).",
      "code": "var Component=(()=>{var g=Object.create;var o=Object.defineProperty;var f=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var h=Object.getPrototypeOf,p=Object.prototype.hasOwnProperty;var v=(e,a)=>()=>(a||e((a={exports:{}}).exports,a),a.exports),b=(e,a)=>{for(var n in a)o(e,n,{get:a[n],enumerable:!0})},s=(e,a,n,r)=>{if(a&&typeof a==\"object\"||typeof a==\"function\")for(let i of u(a))!p.call(e,i)&&i!==n&&o(e,i,{get:()=>a[i],enumerable:!(r=f(a,i))||r.enumerable});return e};var L=(e,a,n)=>(n=e!=null?g(h(e)):{},s(a||!e||!e.__esModule?o(n,\"default\",{value:e,enumerable:!0}):n,e)),y=e=>s(o({},\"__esModule\",{value:!0}),e);var c=v((S,l)=>{l.exports=_jsx_runtime});var x={};b(x,{default:()=>m,frontmatter:()=>M});var t=L(c()),M={title:\"Self-Evolved Reward Learning for LLMs\",date:\"2024-11-01\",tags:[\"biases\",\"human preferences\",\"training data\",\"Reinforcement Learning from Human Feedback\",\"reward model\",\"self-feedback\",\"large language models\",\"experiments\",\"language models\",\"datasets\"],categories:[\"cs.CL\",\"cs.AI\"],problem:\"training a reliable reward model\",solution:\"Self-Evolved Reward Learning\",pdf_url:\"http://arxiv.org/pdf/2411.00418v1\",arx_url:\"http://arxiv.org/abs/2411.00418v1\",score:6,authors:[\"Chenghua Huang\",\"Zhizhen Fan\",\"Lu Wang\",\"Fangkai Yang\",\"Pu Zhao\",\"Zeqi Lin\",\"Qingwei Lin\",\"Dongmei Zhang\",\"Saravan Rajmohan\",\"Qi Zhang\"],affiliations_aligned:[\"School of Computer Science, Fudan University\",\"School of Computer Science, Peking University\",\"Microsoft\",\"Microsoft\",\"Microsoft\",\"Microsoft\",\"Microsoft\",\"Microsoft\",\"Microsoft\",\"Microsoft\"],affiliations:[\"Microsoft\",\"School of Computer Science, Fudan University\",\"School of Computer Science, Peking University\"]};function d(e){let a={p:\"p\",...e.components};return(0,t.jsx)(a.p,{children:\"Reinforcement Learning from Human Feedback (RLHF) is a crucial technique for aligning language models with human preferences, playing a pivotal role in the success of conversational models like GPT-4, ChatGPT, and Llama 2. A core challenge in employing RLHF lies in training a reliable reward model (RM), which relies on high-quality labels typically provided by human experts or advanced AI system. These methods can be costly and may introduce biases that affect the language model's responses. As language models improve, human input may become less effective in further enhancing their performance. In this paper, we propose Self-Evolved Reward Learning (SER), a novel approach where the RM generates additional training data to iteratively improve itself. We conducted extensive experiments on multiple datasets such as HH-RLHF and UltraFeedback, using models like Mistral and Llama 3, and compare SER against various baselines. Our results demonstrate that even with limited human-annotated data, learning from self-feedback can robustly enhance RM performance, thereby boosting the capabilities of large language models (LLMs).\"})}function m(e={}){let{wrapper:a}=e.components||{};return a?(0,t.jsx)(a,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}return y(x);})();\n;return Component;"
    },
    "_id": "articles/2411.00418.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.00418.mdx",
      "sourceFileName": "2411.00418.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.00418"
    },
    "type": "Blog",
    "readingTime": {
      "text": "1 min read",
      "minutes": 0.825,
      "time": 49500,
      "words": 165
    },
    "slug": "2411.00418",
    "path": "articles/2411.00418",
    "filePath": "articles/2411.00418.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "Self-Evolved Reward Learning for LLMs",
      "datePublished": "2024-11-01T00:00:00.000Z",
      "dateModified": "2024-11-01T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.00418"
    }
  },
  {
    "title": "DARD: A Multi-Agent Approach for Task-Oriented Dialog Systems",
    "date": "2024-11-01T00:00:00.000Z",
    "tags": [
      "entity types",
      "domain-specific knowledge",
      "agent modeling approaches",
      "MultiWOZ benchmark",
      "multi-agent conversational system",
      "task-oriented dialogue systems",
      "dialogue inform rate",
      "Large Language Models",
      "fine-tuned models",
      "MultiWOZ dataset",
      "multi-domain systems",
      "dialog manager agent",
      "annotator discrepancies",
      "success rate",
      "user intents"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2411.00427v1",
    "arx_url": "http://arxiv.org/abs/2411.00427v1",
    "authors": [
      "Aman Gupta",
      "Anirudh Ravichandran",
      "Ziji Zhang",
      "Swair Shah",
      "Anurag Beniwal",
      "Narayanan Sadagopan"
    ],
    "affiliations_aligned": [
      "Carnegie Mellon University",
      "Amazon",
      "Amazon",
      "Amazon",
      "Amazon",
      "Amazon"
    ],
    "affiliations": [
      "Carnegie Mellon University",
      "Amazon"
    ],
    "problem": "developing effective multi-domain systems",
    "solution": "DARD (Domain Assigned Response Delegation)",
    "score": 7,
    "body": {
      "raw": "\n\nTask-oriented dialogue systems are essential for applications ranging from customer service to personal assistants and are widely used across various industries. However, developing effective multi-domain systems remains a significant challenge due to the complexity of handling diverse user intents, entity types, and domain-specific knowledge across several domains. In this work, we propose DARD (Domain Assigned Response Delegation), a multi-agent conversational system capable of successfully handling multi-domain dialogs. DARD leverages domain-specific agents, orchestrated by a central dialog manager agent. Our extensive experiments compare and utilize various agent modeling approaches, combining the strengths of smaller fine-tuned models (Flan-T5-large & Mistral-7B) with their larger counterparts, Large Language Models (LLMs) (Claude Sonnet 3.0). We provide insights into the strengths and limitations of each approach, highlighting the benefits of our multi-agent framework in terms of flexibility and composability. We evaluate DARD using the well-established MultiWOZ benchmark, achieving state-of-the-art performance by improving the dialogue inform rate by 6.6% and the success rate by 4.1% over the best-performing existing approaches. Additionally, we discuss various annotator discrepancies and issues within the MultiWOZ dataset and its evaluation system.",
      "code": "var Component=(()=>{var c=Object.create;var i=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var h=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var v=(e,a)=>()=>(a||e((a={exports:{}}).exports,a),a.exports),y=(e,a)=>{for(var t in a)i(e,t,{get:a[t],enumerable:!0})},r=(e,a,t,o)=>{if(a&&typeof a==\"object\"||typeof a==\"function\")for(let n of p(a))!f.call(e,n)&&n!==t&&i(e,n,{get:()=>a[n],enumerable:!(o=u(a,n))||o.enumerable});return e};var A=(e,a,t)=>(t=e!=null?c(h(e)):{},r(a||!e||!e.__esModule?i(t,\"default\",{value:e,enumerable:!0}):t,e)),D=e=>r(i({},\"__esModule\",{value:!0}),e);var d=v((M,l)=>{l.exports=_jsx_runtime});var x={};y(x,{default:()=>m,frontmatter:()=>b});var s=A(d()),b={title:\"DARD: A Multi-Agent Approach for Task-Oriented Dialog Systems\",date:\"2024-11-01\",tags:[\"entity types\",\"domain-specific knowledge\",\"agent modeling approaches\",\"MultiWOZ benchmark\",\"multi-agent conversational system\",\"task-oriented dialogue systems\",\"dialogue inform rate\",\"Large Language Models\",\"fine-tuned models\",\"MultiWOZ dataset\",\"multi-domain systems\",\"dialog manager agent\",\"annotator discrepancies\",\"success rate\",\"user intents\"],categories:[\"cs.CL\",\"cs.AI\"],problem:\"developing effective multi-domain systems\",solution:\"DARD (Domain Assigned Response Delegation)\",pdf_url:\"http://arxiv.org/pdf/2411.00427v1\",arx_url:\"http://arxiv.org/abs/2411.00427v1\",score:7,authors:[\"Aman Gupta\",\"Anirudh Ravichandran\",\"Ziji Zhang\",\"Swair Shah\",\"Anurag Beniwal\",\"Narayanan Sadagopan\"],affiliations_aligned:[\"Carnegie Mellon University\",\"Amazon\",\"Amazon\",\"Amazon\",\"Amazon\",\"Amazon\"],affiliations:[\"Carnegie Mellon University\",\"Amazon\"]};function g(e){let a={p:\"p\",...e.components};return(0,s.jsx)(a.p,{children:\"Task-oriented dialogue systems are essential for applications ranging from customer service to personal assistants and are widely used across various industries. However, developing effective multi-domain systems remains a significant challenge due to the complexity of handling diverse user intents, entity types, and domain-specific knowledge across several domains. In this work, we propose DARD (Domain Assigned Response Delegation), a multi-agent conversational system capable of successfully handling multi-domain dialogs. DARD leverages domain-specific agents, orchestrated by a central dialog manager agent. Our extensive experiments compare and utilize various agent modeling approaches, combining the strengths of smaller fine-tuned models (Flan-T5-large & Mistral-7B) with their larger counterparts, Large Language Models (LLMs) (Claude Sonnet 3.0). We provide insights into the strengths and limitations of each approach, highlighting the benefits of our multi-agent framework in terms of flexibility and composability. We evaluate DARD using the well-established MultiWOZ benchmark, achieving state-of-the-art performance by improving the dialogue inform rate by 6.6% and the success rate by 4.1% over the best-performing existing approaches. Additionally, we discuss various annotator discrepancies and issues within the MultiWOZ dataset and its evaluation system.\"})}function m(e={}){let{wrapper:a}=e.components||{};return a?(0,s.jsx)(a,{...e,children:(0,s.jsx)(g,{...e})}):g(e)}return D(x);})();\n;return Component;"
    },
    "_id": "articles/2411.00427.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.00427.mdx",
      "sourceFileName": "2411.00427.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.00427"
    },
    "type": "Blog",
    "readingTime": {
      "text": "1 min read",
      "minutes": 0.895,
      "time": 53700,
      "words": 179
    },
    "slug": "2411.00427",
    "path": "articles/2411.00427",
    "filePath": "articles/2411.00427.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "DARD: A Multi-Agent Approach for Task-Oriented Dialog Systems",
      "datePublished": "2024-11-01T00:00:00.000Z",
      "dateModified": "2024-11-01T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.00427"
    }
  },
  {
    "title": "Birdie: Advancing State Space Models with Reward-Driven Objectives and Curricula",
    "date": "2024-11-08T00:00:00.000Z",
    "tags": [
      "causal generation",
      "retrieval-intensive tasks",
      "linear attention variants",
      "question answering",
      "text copying",
      "specialized pre-training objectives",
      "bidirectional input processing",
      "linear recurrent neural networks",
      "architectural modifications",
      "computational efficiency",
      "training procedure",
      "bidirectional SSM architecture",
      "Transformers",
      "in-context retrieval",
      "associative recall",
      "reinforcement learning",
      "long paragraph question-answering",
      "infilling",
      "multi-number phone book lookup",
      "state space models"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2411.01030",
    "arx_url": "https://arxiv.org/abs/2411.01030",
    "authors": [
      "Sam Blouir",
      "Jimmy T. H. Smith",
      "Antonios Anastasopoulos",
      "Amarda Shehu",
      "Jimmy T.H. Smith"
    ],
    "affiliations_aligned": [
      "Department of Computer Science, George Mason University, Fairfax, VA",
      "",
      "Department of Computer Science, George Mason University, Fairfax, VA; Archimedes AI, Athena RC, Athens, Greece",
      "Department of Computer Science, George Mason University, Fairfax, VA",
      "Stanford University, Stanford, CA"
    ],
    "affiliations": [
      "",
      "Department of Computer Science, George Mason University, Fairfax, VA; Archimedes AI, Athena RC, Athens, Greece",
      "Stanford University, Stanford, CA",
      "Department of Computer Science, George Mason University, Fairfax, VA"
    ],
    "problem": "long-range in-context retrieval challenges in state space models",
    "solution": "novel training procedure Birdie for enhancing in-context retrieval capabilities",
    "score": 7,
    "body": {
      "raw": "\n\nEfficient state space models (SSMs), such as linear recurrent neural networks and linear attention variants, offer computational advantages over Transformers but struggle with tasks requiring long-range in-context retrieval-like text copying, associative recall, and question answering over long contexts. Previous efforts to address these challenges have focused on architectural modifications, often reintroducing computational inefficiencies. In this paper, we propose a novel training procedure, Birdie, that significantly enhances the in-context retrieval capabilities of SSMs without altering their architecture. Our approach combines bidirectional input processing with dynamic mixtures of specialized pre-training objectives, optimized via reinforcement learning. We introduce a new bidirectional SSM architecture that seamlessly transitions from bidirectional context processing to causal generation. Experimental evaluations demonstrate that Birdie markedly improves performance on retrieval-intensive tasks such as multi-number phone book lookup, long paragraph question-answering, and infilling. This narrows the performance gap with Transformers, while retaining computational efficiency. Our findings highlight the importance of training procedures in leveraging the fixed-state capacity of SSMs, offering a new direction to advance their capabilities. All code and pre-trained models are available at https://www.github.com/samblouir/birdie, with support for JAX and PyTorch.",
      "code": "var Component=(()=>{var d=Object.create;var a=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var f=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,h=Object.prototype.hasOwnProperty;var v=(e,i)=>()=>(i||e((i={exports:{}}).exports,i),i.exports),x=(e,i)=>{for(var t in i)a(e,t,{get:i[t],enumerable:!0})},s=(e,i,t,o)=>{if(i&&typeof i==\"object\"||typeof i==\"function\")for(let r of f(i))!h.call(e,r)&&r!==t&&a(e,r,{get:()=>i[r],enumerable:!(o=m(i,r))||o.enumerable});return e};var b=(e,i,t)=>(t=e!=null?d(g(e)):{},s(i||!e||!e.__esModule?a(t,\"default\",{value:e,enumerable:!0}):t,e)),w=e=>s(a({},\"__esModule\",{value:!0}),e);var l=v((M,c)=>{c.exports=_jsx_runtime});var A={};x(A,{default:()=>u,frontmatter:()=>S});var n=b(l()),S={title:\"Birdie: Advancing State Space Models with Reward-Driven Objectives and Curricula\",date:\"2024-11-08\",tags:[\"causal generation\",\"retrieval-intensive tasks\",\"linear attention variants\",\"question answering\",\"text copying\",\"specialized pre-training objectives\",\"bidirectional input processing\",\"linear recurrent neural networks\",\"architectural modifications\",\"computational efficiency\",\"training procedure\",\"bidirectional SSM architecture\",\"Transformers\",\"in-context retrieval\",\"associative recall\",\"reinforcement learning\",\"long paragraph question-answering\",\"infilling\",\"multi-number phone book lookup\",\"state space models\"],categories:[\"cs.AI\",\"cs.CL\",\"cs.LG\"],problem:\"long-range in-context retrieval challenges in state space models\",solution:\"novel training procedure Birdie for enhancing in-context retrieval capabilities\",pdf_url:\"https://arxiv.org/pdf/2411.01030\",arx_url:\"https://arxiv.org/abs/2411.01030\",score:7,authors:[\"Sam Blouir\",\"Jimmy T. H. Smith\",\"Antonios Anastasopoulos\",\"Amarda Shehu\",\"Jimmy T.H. Smith\"],affiliations_aligned:[\"Department of Computer Science, George Mason University, Fairfax, VA\",\"\",\"Department of Computer Science, George Mason University, Fairfax, VA; Archimedes AI, Athena RC, Athens, Greece\",\"Department of Computer Science, George Mason University, Fairfax, VA\",\"Stanford University, Stanford, CA\"],affiliations:[\"\",\"Department of Computer Science, George Mason University, Fairfax, VA; Archimedes AI, Athena RC, Athens, Greece\",\"Stanford University, Stanford, CA\",\"Department of Computer Science, George Mason University, Fairfax, VA\"]};function p(e){let i={a:\"a\",p:\"p\",...e.components};return(0,n.jsxs)(i.p,{children:[\"Efficient state space models (SSMs), such as linear recurrent neural networks and linear attention variants, offer computational advantages over Transformers but struggle with tasks requiring long-range in-context retrieval-like text copying, associative recall, and question answering over long contexts. Previous efforts to address these challenges have focused on architectural modifications, often reintroducing computational inefficiencies. In this paper, we propose a novel training procedure, Birdie, that significantly enhances the in-context retrieval capabilities of SSMs without altering their architecture. Our approach combines bidirectional input processing with dynamic mixtures of specialized pre-training objectives, optimized via reinforcement learning. We introduce a new bidirectional SSM architecture that seamlessly transitions from bidirectional context processing to causal generation. Experimental evaluations demonstrate that Birdie markedly improves performance on retrieval-intensive tasks such as multi-number phone book lookup, long paragraph question-answering, and infilling. This narrows the performance gap with Transformers, while retaining computational efficiency. Our findings highlight the importance of training procedures in leveraging the fixed-state capacity of SSMs, offering a new direction to advance their capabilities. All code and pre-trained models are available at \",(0,n.jsx)(i.a,{href:\"https://www.github.com/samblouir/birdie\",children:\"https://www.github.com/samblouir/birdie\"}),\", with support for JAX and PyTorch.\"]})}function u(e={}){let{wrapper:i}=e.components||{};return i?(0,n.jsx)(i,{...e,children:(0,n.jsx)(p,{...e})}):p(e)}return w(A);})();\n;return Component;"
    },
    "_id": "articles/2411.01030.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.01030.mdx",
      "sourceFileName": "2411.01030.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.01030"
    },
    "type": "Blog",
    "readingTime": {
      "text": "1 min read",
      "minutes": 0.905,
      "time": 54300,
      "words": 181
    },
    "slug": "2411.01030",
    "path": "articles/2411.01030",
    "filePath": "articles/2411.01030.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "Birdie: Advancing State Space Models with Reward-Driven Objectives and Curricula",
      "datePublished": "2024-11-08T00:00:00.000Z",
      "dateModified": "2024-11-08T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.01030"
    }
  },
  {
    "title": "Visual Fourier Prompt Tuning",
    "date": "2024-11-02T00:00:00.000Z",
    "tags": [
      "finetuning",
      "empirical results",
      "Visual prompt tuning",
      "parameter-efficient finetuning",
      "Fast Fourier Transform",
      "spatial domain",
      "performance degradation",
      "frequency domain",
      "state-of-the-art baselines",
      "prompt embeddings",
      "vision Transformer",
      "datasets"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2411.01327v1",
    "arx_url": "http://arxiv.org/abs/2411.01327v1",
    "authors": [
      "Runjia Zeng",
      "Cheng Han",
      "Qifan Wang",
      "Chunshu Wu",
      "Tong Geng",
      "Lifu Huang",
      "Ying Nian Wu",
      "Dongfang Liu"
    ],
    "affiliations_aligned": [
      "Rochester Institute of Technology",
      "University of Missouri - Kansas City",
      "Meta AI",
      "University of Rochester",
      "University of Rochester",
      "Virginia Tech",
      "University of California, Los Angeles",
      "Rochester Institute of Technology"
    ],
    "affiliations": [
      "Rochester Institute of Technology",
      "Meta AI",
      "University of Rochester",
      "University of Missouri - Kansas City",
      "Virginia Tech",
      "University of California, Los Angeles"
    ],
    "problem": "performance degradation due to dataset disparity",
    "solution": "Visual Fourier Prompt Tuning method",
    "score": 6,
    "body": {
      "raw": "\n\nWith the scale of vision Transformer-based models continuing to grow, finetuning these large-scale pretrained models for new tasks has become increasingly parameter-intensive. Visual prompt tuning is introduced as a parameter-efficient finetuning (PEFT) method to this trend. Despite its successes, a notable research challenge persists within almost all PEFT approaches: significant performance degradation is observed when there is a substantial disparity between the datasets applied in pretraining and finetuning phases. To address this challenge, we draw inspiration from human visual cognition, and propose the Visual Fourier Prompt Tuning (VFPT) method as a general and effective solution for adapting large-scale transformer-based models. Our approach innovatively incorporates the Fast Fourier Transform into prompt embeddings and harmoniously considers both spatial and frequency domain information. Apart from its inherent simplicity and intuitiveness, VFPT exhibits superior performance across all datasets, offering a general solution to dataset challenges, irrespective of data disparities. Empirical results demonstrate that our approach outperforms current state-of-the-art baselines on two benchmarks, with low parameter usage (e.g., 0.57% of model parameters on VTAB-1k) and notable performance enhancements (e.g., 73.20% of mean accuracy on VTAB-1k). Our code is avaliable at https://github.com/runtsang/VFPT.",
      "code": "var Component=(()=>{var p=Object.create;var r=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var h=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var v=(e,t)=>()=>(t||e((t={exports:{}}).exports,t),t.exports),y=(e,t)=>{for(var n in t)r(e,n,{get:t[n],enumerable:!0})},o=(e,t,n,s)=>{if(t&&typeof t==\"object\"||typeof t==\"function\")for(let i of h(t))!g.call(e,i)&&i!==n&&r(e,i,{get:()=>t[i],enumerable:!(s=d(t,i))||s.enumerable});return e};var T=(e,t,n)=>(n=e!=null?p(m(e)):{},o(t||!e||!e.__esModule?r(n,\"default\",{value:e,enumerable:!0}):n,e)),b=e=>o(r({},\"__esModule\",{value:!0}),e);var u=v((w,c)=>{c.exports=_jsx_runtime});var x={};y(x,{default:()=>f,frontmatter:()=>V});var a=T(u()),V={title:\"Visual Fourier Prompt Tuning\",date:\"2024-11-02\",tags:[\"finetuning\",\"empirical results\",\"Visual prompt tuning\",\"parameter-efficient finetuning\",\"Fast Fourier Transform\",\"spatial domain\",\"performance degradation\",\"frequency domain\",\"state-of-the-art baselines\",\"prompt embeddings\",\"vision Transformer\",\"datasets\"],categories:[\"cs.CV\",\"cs.AI\"],problem:\"performance degradation due to dataset disparity\",solution:\"Visual Fourier Prompt Tuning method\",pdf_url:\"http://arxiv.org/pdf/2411.01327v1\",arx_url:\"http://arxiv.org/abs/2411.01327v1\",score:6,authors:[\"Runjia Zeng\",\"Cheng Han\",\"Qifan Wang\",\"Chunshu Wu\",\"Tong Geng\",\"Lifu Huang\",\"Ying Nian Wu\",\"Dongfang Liu\"],affiliations_aligned:[\"Rochester Institute of Technology\",\"University of Missouri - Kansas City\",\"Meta AI\",\"University of Rochester\",\"University of Rochester\",\"Virginia Tech\",\"University of California, Los Angeles\",\"Rochester Institute of Technology\"],affiliations:[\"Rochester Institute of Technology\",\"Meta AI\",\"University of Rochester\",\"University of Missouri - Kansas City\",\"Virginia Tech\",\"University of California, Los Angeles\"]};function l(e){let t={a:\"a\",p:\"p\",...e.components};return(0,a.jsxs)(t.p,{children:[\"With the scale of vision Transformer-based models continuing to grow, finetuning these large-scale pretrained models for new tasks has become increasingly parameter-intensive. Visual prompt tuning is introduced as a parameter-efficient finetuning (PEFT) method to this trend. Despite its successes, a notable research challenge persists within almost all PEFT approaches: significant performance degradation is observed when there is a substantial disparity between the datasets applied in pretraining and finetuning phases. To address this challenge, we draw inspiration from human visual cognition, and propose the Visual Fourier Prompt Tuning (VFPT) method as a general and effective solution for adapting large-scale transformer-based models. Our approach innovatively incorporates the Fast Fourier Transform into prompt embeddings and harmoniously considers both spatial and frequency domain information. Apart from its inherent simplicity and intuitiveness, VFPT exhibits superior performance across all datasets, offering a general solution to dataset challenges, irrespective of data disparities. Empirical results demonstrate that our approach outperforms current state-of-the-art baselines on two benchmarks, with low parameter usage (e.g., 0.57% of model parameters on VTAB-1k) and notable performance enhancements (e.g., 73.20% of mean accuracy on VTAB-1k). Our code is avaliable at \",(0,a.jsx)(t.a,{href:\"https://github.com/runtsang/VFPT\",children:\"https://github.com/runtsang/VFPT\"}),\".\"]})}function f(e={}){let{wrapper:t}=e.components||{};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(l,{...e})}):l(e)}return b(x);})();\n;return Component;"
    },
    "_id": "articles/2411.01327.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.01327.mdx",
      "sourceFileName": "2411.01327.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.01327"
    },
    "type": "Blog",
    "readingTime": {
      "text": "1 min read",
      "minutes": 0.93,
      "time": 55800,
      "words": 186
    },
    "slug": "2411.01327",
    "path": "articles/2411.01327",
    "filePath": "articles/2411.01327.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "Visual Fourier Prompt Tuning",
      "datePublished": "2024-11-02T00:00:00.000Z",
      "dateModified": "2024-11-02T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.01327"
    }
  },
  {
    "title": "Online and Offline Evaluations of Collaborative Filtering and Content Based Recommender Systems",
    "date": "2024-11-02T00:00:00.000Z",
    "tags": [
      "e-commerce",
      "hybrid approaches",
      "hit-rate@k",
      "A/B testing",
      "user-based recommendations",
      "news broadcasting",
      "offline evaluations",
      "popularity bias",
      "manual evaluation",
      "item-based recommendations",
      "collaborative filtering",
      "nDCG",
      "digital publishing",
      "user satisfaction",
      "content-based methods",
      "algorithms",
      "information retrieval",
      "trend-based methods",
      "media streaming",
      "Persian language",
      "click-through rate",
      "online evaluations",
      "datasets",
      "accuracy metrics",
      "recommender systems",
      "cold-start"
    ],
    "categories": [
      "cs.CL",
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2411.01354v1",
    "arx_url": "http://arxiv.org/abs/2411.01354v1",
    "authors": [
      "Ali Elahi",
      "Armin Zirak"
    ],
    "affiliations_aligned": [
      "University of Illinois at Chicago, Chicago, Illinois, USA",
      "University of Illinois at Chicago, Chicago, Illinois, USA"
    ],
    "affiliations": [
      "University of Illinois at Chicago, Chicago, Illinois, USA"
    ],
    "problem": "user satisfaction complexity in recommender systems",
    "solution": "comparative analysis of evaluation methods for recommender systems",
    "score": 6,
    "body": {
      "raw": "\n\nRecommender systems are widely used AI applications designed to help users efficiently discover relevant items. The effectiveness of such systems is tied to the satisfaction of both users and providers. However, user satisfaction is complex and cannot be easily framed mathematically using information retrieval and accuracy metrics. While many studies evaluate accuracy through offline tests, a growing number of researchers argue that online evaluation methods such as A/B testing are better suited for this purpose. We have employed a variety of algorithms on different types of datasets divergent in size and subject, producing recommendations in various platforms, including media streaming services, digital publishing websites, e-commerce systems, and news broadcasting networks. Notably, our target websites and datasets are in Persian (Farsi) language.   This study provides a comparative analysis of a large-scale recommender system that has been operating for the past year across about 70 websites in Iran, processing roughly 300 requests per second collectively. The system employs user-based and item-based recommendations using content-based, collaborative filtering, trend-based methods, and hybrid approaches. Through both offline and online evaluations, we aim to identify where these algorithms perform most efficiently and determine the best method for our specific needs, considering the dataset and system scale. Our methods of evaluation include manual evaluation, offline tests including accuracy and ranking metrics like hit-rate@k and nDCG, and online tests consisting of click-through rate (CTR). Additionally we analyzed and proposed methods to address cold-start and popularity bias.",
      "code": "var Component=(()=>{var u=Object.create;var i=Object.defineProperty;var h=Object.getOwnPropertyDescriptor;var f=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,p=Object.prototype.hasOwnProperty;var y=(e,t)=>()=>(t||e((t={exports:{}}).exports,t),t.exports),v=(e,t)=>{for(var a in t)i(e,a,{get:t[a],enumerable:!0})},r=(e,t,a,o)=>{if(t&&typeof t==\"object\"||typeof t==\"function\")for(let s of f(t))!p.call(e,s)&&s!==a&&i(e,s,{get:()=>t[s],enumerable:!(o=h(t,s))||o.enumerable});return e};var b=(e,t,a)=>(a=e!=null?u(g(e)):{},r(t||!e||!e.__esModule?i(a,\"default\",{value:e,enumerable:!0}):a,e)),C=e=>r(i({},\"__esModule\",{value:!0}),e);var c=y((I,l)=>{l.exports=_jsx_runtime});var x={};v(x,{default:()=>m,frontmatter:()=>w});var n=b(c()),w={title:\"Online and Offline Evaluations of Collaborative Filtering and Content Based Recommender Systems\",date:\"2024-11-02\",tags:[\"e-commerce\",\"hybrid approaches\",\"hit-rate@k\",\"A/B testing\",\"user-based recommendations\",\"news broadcasting\",\"offline evaluations\",\"popularity bias\",\"manual evaluation\",\"item-based recommendations\",\"collaborative filtering\",\"nDCG\",\"digital publishing\",\"user satisfaction\",\"content-based methods\",\"algorithms\",\"information retrieval\",\"trend-based methods\",\"media streaming\",\"Persian language\",\"click-through rate\",\"online evaluations\",\"datasets\",\"accuracy metrics\",\"recommender systems\",\"cold-start\"],categories:[\"cs.CL\",\"cs.IR\",\"cs.AI\",\"cs.LG\"],problem:\"user satisfaction complexity in recommender systems\",solution:\"comparative analysis of evaluation methods for recommender systems\",pdf_url:\"http://arxiv.org/pdf/2411.01354v1\",arx_url:\"http://arxiv.org/abs/2411.01354v1\",score:6,authors:[\"Ali Elahi\",\"Armin Zirak\"],affiliations_aligned:[\"University of Illinois at Chicago, Chicago, Illinois, USA\",\"University of Illinois at Chicago, Chicago, Illinois, USA\"],affiliations:[\"University of Illinois at Chicago, Chicago, Illinois, USA\"]};function d(e){let t={p:\"p\",...e.components};return(0,n.jsx)(t.p,{children:\"Recommender systems are widely used AI applications designed to help users efficiently discover relevant items. The effectiveness of such systems is tied to the satisfaction of both users and providers. However, user satisfaction is complex and cannot be easily framed mathematically using information retrieval and accuracy metrics. While many studies evaluate accuracy through offline tests, a growing number of researchers argue that online evaluation methods such as A/B testing are better suited for this purpose. We have employed a variety of algorithms on different types of datasets divergent in size and subject, producing recommendations in various platforms, including media streaming services, digital publishing websites, e-commerce systems, and news broadcasting networks. Notably, our target websites and datasets are in Persian (Farsi) language. This study provides a comparative analysis of a large-scale recommender system that has been operating for the past year across about 70 websites in Iran, processing roughly 300 requests per second collectively. The system employs user-based and item-based recommendations using content-based, collaborative filtering, trend-based methods, and hybrid approaches. Through both offline and online evaluations, we aim to identify where these algorithms perform most efficiently and determine the best method for our specific needs, considering the dataset and system scale. Our methods of evaluation include manual evaluation, offline tests including accuracy and ranking metrics like hit-rate@k and nDCG, and online tests consisting of click-through rate (CTR). Additionally we analyzed and proposed methods to address cold-start and popularity bias.\"})}function m(e={}){let{wrapper:t}=e.components||{};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}return C(x);})();\n;return Component;"
    },
    "_id": "articles/2411.01354.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.01354.mdx",
      "sourceFileName": "2411.01354.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.01354"
    },
    "type": "Blog",
    "readingTime": {
      "text": "2 min read",
      "minutes": 1.19,
      "time": 71400,
      "words": 238
    },
    "slug": "2411.01354",
    "path": "articles/2411.01354",
    "filePath": "articles/2411.01354.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "Online and Offline Evaluations of Collaborative Filtering and Content Based Recommender Systems",
      "datePublished": "2024-11-02T00:00:00.000Z",
      "dateModified": "2024-11-02T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.01354"
    }
  },
  {
    "title": "EcoAct: Economic Agent Determines When to Register What Action",
    "date": "2024-11-03T00:00:00.000Z",
    "tags": [
      "reasoning procedures",
      "Large Language Models",
      "context optimization",
      "agents",
      "computational efficiency",
      "tool registration"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2411.01643v1",
    "arx_url": "http://arxiv.org/abs/2411.01643v1",
    "authors": [
      "Shaokun Zhang",
      "Jieyu Zhang",
      "Dujian Ding",
      "Mirian Hipolito Garcia",
      "Ankur Mallick",
      "Daniel Madrigal",
      "Menglin Xia",
      "Victor RÃ¼hle",
      "Qingyun Wu",
      "Chi Wang"
    ],
    "affiliations_aligned": [
      "Pennsylvania State University",
      "Microsoft",
      "The University of Washington",
      "Microsoft",
      "Microsoft",
      "Microsoft",
      "Microsoft",
      "Microsoft",
      "Pennsylvania State University",
      "Google DeepMind"
    ],
    "affiliations": [
      "Google DeepMind",
      "Microsoft",
      "The University of Washington",
      "Pennsylvania State University"
    ],
    "problem": "inefficient tool registration in LLM agents",
    "solution": "EcoAct tool for selective tool registration",
    "score": 6,
    "body": {
      "raw": "\n\nRecent advancements have enabled Large Language Models (LLMs) to function as agents that can perform actions using external tools. This requires registering, i.e., integrating tool information into the LLM context prior to taking actions. Current methods indiscriminately incorporate all candidate tools into the agent's context and retain them across multiple reasoning steps. This process remains opaque to LLM agents and is not integrated into their reasoning procedures, leading to inefficiencies due to increased context length from irrelevant tools. To address this, we introduce EcoAct, a tool using algorithm that allows LLMs to selectively register tools as needed, optimizing context use. By integrating the tool registration process into the reasoning procedure, EcoAct reduces computational costs by over 50% in multiple steps reasoning tasks while maintaining performance, as demonstrated through extensive experiments. Moreover, it can be plugged into any reasoning pipeline with only minor modifications to the prompt, making it applicable to LLM agents now and future.",
      "code": "var Component=(()=>{var u=Object.create;var o=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var h=Object.getOwnPropertyNames;var f=Object.getPrototypeOf,m=Object.prototype.hasOwnProperty;var M=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),v=(t,e)=>{for(var n in e)o(t,n,{get:e[n],enumerable:!0})},s=(t,e,n,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of h(e))!m.call(t,i)&&i!==n&&o(t,i,{get:()=>e[i],enumerable:!(r=d(e,i))||r.enumerable});return t};var x=(t,e,n)=>(n=t!=null?u(f(t)):{},s(e||!t||!t.__esModule?o(n,\"default\",{value:t,enumerable:!0}):n,t)),y=t=>s(o({},\"__esModule\",{value:!0}),t);var l=M((_,c)=>{c.exports=_jsx_runtime});var A={};v(A,{default:()=>p,frontmatter:()=>L});var a=x(l()),L={title:\"EcoAct: Economic Agent Determines When to Register What Action\",date:\"2024-11-03\",tags:[\"reasoning procedures\",\"Large Language Models\",\"context optimization\",\"agents\",\"computational efficiency\",\"tool registration\"],categories:[\"cs.CL\",\"cs.AI\"],problem:\"inefficient tool registration in LLM agents\",solution:\"EcoAct tool for selective tool registration\",pdf_url:\"http://arxiv.org/pdf/2411.01643v1\",arx_url:\"http://arxiv.org/abs/2411.01643v1\",score:6,authors:[\"Shaokun Zhang\",\"Jieyu Zhang\",\"Dujian Ding\",\"Mirian Hipolito Garcia\",\"Ankur Mallick\",\"Daniel Madrigal\",\"Menglin Xia\",\"Victor R\\xFChle\",\"Qingyun Wu\",\"Chi Wang\"],affiliations_aligned:[\"Pennsylvania State University\",\"Microsoft\",\"The University of Washington\",\"Microsoft\",\"Microsoft\",\"Microsoft\",\"Microsoft\",\"Microsoft\",\"Pennsylvania State University\",\"Google DeepMind\"],affiliations:[\"Google DeepMind\",\"Microsoft\",\"The University of Washington\",\"Pennsylvania State University\"]};function g(t){let e={p:\"p\",...t.components};return(0,a.jsx)(e.p,{children:\"Recent advancements have enabled Large Language Models (LLMs) to function as agents that can perform actions using external tools. This requires registering, i.e., integrating tool information into the LLM context prior to taking actions. Current methods indiscriminately incorporate all candidate tools into the agent's context and retain them across multiple reasoning steps. This process remains opaque to LLM agents and is not integrated into their reasoning procedures, leading to inefficiencies due to increased context length from irrelevant tools. To address this, we introduce EcoAct, a tool using algorithm that allows LLMs to selectively register tools as needed, optimizing context use. By integrating the tool registration process into the reasoning procedure, EcoAct reduces computational costs by over 50% in multiple steps reasoning tasks while maintaining performance, as demonstrated through extensive experiments. Moreover, it can be plugged into any reasoning pipeline with only minor modifications to the prompt, making it applicable to LLM agents now and future.\"})}function p(t={}){let{wrapper:e}=t.components||{};return e?(0,a.jsx)(e,{...t,children:(0,a.jsx)(g,{...t})}):g(t)}return y(A);})();\n;return Component;"
    },
    "_id": "articles/2411.01643.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.01643.mdx",
      "sourceFileName": "2411.01643.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.01643"
    },
    "type": "Blog",
    "readingTime": {
      "text": "1 min read",
      "minutes": 0.775,
      "time": 46500,
      "words": 155
    },
    "slug": "2411.01643",
    "path": "articles/2411.01643",
    "filePath": "articles/2411.01643.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "EcoAct: Economic Agent Determines When to Register What Action",
      "datePublished": "2024-11-03T00:00:00.000Z",
      "dateModified": "2024-11-03T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.01643"
    }
  },
  {
    "title": "SALSA: Soup-based Alignment Learning for Stronger Adaptation in RLHF",
    "date": "2024-11-04T00:00:00.000Z",
    "tags": [
      "model robustness",
      "out-of-distribution generalization",
      "Kullback-Leibler divergence",
      "Reinforcement Learning from Human Feedback",
      "Large Language Models",
      "policy optimization algorithms",
      "benchmark validation",
      "Proximal Policy Optimization",
      "weight-space averaging",
      "supervised fine-tuned models"
    ],
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2411.01798v1",
    "arx_url": "http://arxiv.org/abs/2411.01798v1",
    "authors": [
      "Atoosa Chegini",
      "Hamid Kazemi",
      "Iman Mirzadeh",
      "Dong Yin",
      "Maxwell Horton",
      "Moin Nabi",
      "Mehrdad Farajtabar",
      "Keivan Alizadeh"
    ],
    "affiliations_aligned": [
      "University of Maryland",
      "Apple",
      "Apple",
      "Apple",
      "Apple",
      "Apple",
      "Apple",
      "Apple"
    ],
    "affiliations": [
      "University of Maryland",
      "Apple"
    ],
    "problem": "suboptimal alignment and performance in RLHF",
    "solution": "Soup-based Alignment Learning for Stronger Adaptation",
    "score": 6,
    "body": {
      "raw": "\n\nIn Large Language Model (LLM) development, Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning models with human values and preferences. RLHF traditionally relies on the Kullback-Leibler (KL) divergence between the current policy and a frozen initial policy as a reference, which is added as a penalty in policy optimization algorithms like Proximal Policy Optimization (PPO). While this constraint prevents models from deviating too far from the initial checkpoint, it limits exploration of the reward landscape, reducing the model's ability to discover higher-quality solutions. As a result, policy optimization is often trapped in a narrow region of the parameter space, leading to suboptimal alignment and performance. This paper presents SALSA (Soup-based Alignment Learning for Stronger Adaptation), a novel approach designed to overcome these limitations by creating a more flexible and better located reference model through weight-space averaging of two independent supervised fine-tuned (SFT) models. This model soup allows for larger deviation in KL divergence and exploring a promising region of the solution space without sacrificing stability. By leveraging this more robust reference model, SALSA fosters better exploration, achieving higher rewards and improving model robustness, out-of-distribution generalization, and performance. We validate the effectiveness of SALSA through extensive experiments on popular open models (Llama2-7B, Mistral-7B, and Gemma-2B) across various benchmarks (MT-Bench, Arena-Hard, UltraFeedback), where it consistently surpasses PPO by fostering deeper exploration and achieving superior alignment in LLMs.",
      "code": "var Component=(()=>{var m=Object.create;var o=Object.defineProperty;var g=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var f=Object.getPrototypeOf,h=Object.prototype.hasOwnProperty;var b=(e,i)=>()=>(i||e((i={exports:{}}).exports,i),i.exports),v=(e,i)=>{for(var n in i)o(e,n,{get:i[n],enumerable:!0})},l=(e,i,n,r)=>{if(i&&typeof i==\"object\"||typeof i==\"function\")for(let a of u(i))!h.call(e,a)&&a!==n&&o(e,a,{get:()=>i[a],enumerable:!(r=g(i,a))||r.enumerable});return e};var L=(e,i,n)=>(n=e!=null?m(f(e)):{},l(i||!e||!e.__esModule?o(n,\"default\",{value:e,enumerable:!0}):n,e)),A=e=>l(o({},\"__esModule\",{value:!0}),e);var p=b((S,s)=>{s.exports=_jsx_runtime});var x={};v(x,{default:()=>c,frontmatter:()=>y});var t=L(p()),y={title:\"SALSA: Soup-based Alignment Learning for Stronger Adaptation in RLHF\",date:\"2024-11-04\",tags:[\"model robustness\",\"out-of-distribution generalization\",\"Kullback-Leibler divergence\",\"Reinforcement Learning from Human Feedback\",\"Large Language Models\",\"policy optimization algorithms\",\"benchmark validation\",\"Proximal Policy Optimization\",\"weight-space averaging\",\"supervised fine-tuned models\"],categories:[\"cs.LG\"],problem:\"suboptimal alignment and performance in RLHF\",solution:\"Soup-based Alignment Learning for Stronger Adaptation\",pdf_url:\"http://arxiv.org/pdf/2411.01798v1\",arx_url:\"http://arxiv.org/abs/2411.01798v1\",score:6,authors:[\"Atoosa Chegini\",\"Hamid Kazemi\",\"Iman Mirzadeh\",\"Dong Yin\",\"Maxwell Horton\",\"Moin Nabi\",\"Mehrdad Farajtabar\",\"Keivan Alizadeh\"],affiliations_aligned:[\"University of Maryland\",\"Apple\",\"Apple\",\"Apple\",\"Apple\",\"Apple\",\"Apple\",\"Apple\"],affiliations:[\"University of Maryland\",\"Apple\"]};function d(e){let i={p:\"p\",...e.components};return(0,t.jsx)(i.p,{children:\"In Large Language Model (LLM) development, Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning models with human values and preferences. RLHF traditionally relies on the Kullback-Leibler (KL) divergence between the current policy and a frozen initial policy as a reference, which is added as a penalty in policy optimization algorithms like Proximal Policy Optimization (PPO). While this constraint prevents models from deviating too far from the initial checkpoint, it limits exploration of the reward landscape, reducing the model's ability to discover higher-quality solutions. As a result, policy optimization is often trapped in a narrow region of the parameter space, leading to suboptimal alignment and performance. This paper presents SALSA (Soup-based Alignment Learning for Stronger Adaptation), a novel approach designed to overcome these limitations by creating a more flexible and better located reference model through weight-space averaging of two independent supervised fine-tuned (SFT) models. This model soup allows for larger deviation in KL divergence and exploring a promising region of the solution space without sacrificing stability. By leveraging this more robust reference model, SALSA fosters better exploration, achieving higher rewards and improving model robustness, out-of-distribution generalization, and performance. We validate the effectiveness of SALSA through extensive experiments on popular open models (Llama2-7B, Mistral-7B, and Gemma-2B) across various benchmarks (MT-Bench, Arena-Hard, UltraFeedback), where it consistently surpasses PPO by fostering deeper exploration and achieving superior alignment in LLMs.\"})}function c(e={}){let{wrapper:i}=e.components||{};return i?(0,t.jsx)(i,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}return A(x);})();\n;return Component;"
    },
    "_id": "articles/2411.01798.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.01798.mdx",
      "sourceFileName": "2411.01798.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.01798"
    },
    "type": "Blog",
    "readingTime": {
      "text": "2 min read",
      "minutes": 1.135,
      "time": 68100,
      "words": 227
    },
    "slug": "2411.01798",
    "path": "articles/2411.01798",
    "filePath": "articles/2411.01798.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "SALSA: Soup-based Alignment Learning for Stronger Adaptation in RLHF",
      "datePublished": "2024-11-04T00:00:00.000Z",
      "dateModified": "2024-11-04T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.01798"
    }
  },
  {
    "title": "Leveraging Label Semantics and Meta-Label Refinement for Multi-Label Question Classification",
    "date": "2024-11-04T00:00:00.000Z",
    "tags": [
      "distribution imbalance",
      "Math LLM",
      "Retrieval Reranking",
      "personalized learning",
      "resource recommendation",
      "semantic overlap",
      "class center learning",
      "F1 scores",
      "meta-label refinement",
      "educational resources",
      "Precision@k",
      "multi-label question classification",
      "label semantics"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2411.01841v1",
    "arx_url": "http://arxiv.org/abs/2411.01841v1",
    "authors": [
      "Shi Dong",
      "Xiaobei Niu",
      "Rui Zhong",
      "Zhifeng Wang",
      "Mingzhang Zuo"
    ],
    "affiliations_aligned": [
      "Central China Normal University",
      "Central China Normal University",
      "Central China Normal University",
      "Central China Normal University",
      "Central China Normal University"
    ],
    "affiliations": [
      "Central China Normal University"
    ],
    "problem": "challenges with semantic overlap and distribution imbalance of labels in the multi-label context",
    "solution": "RR2QC, a novel Retrieval Reranking method for multi-label Question Classification",
    "score": 6,
    "body": {
      "raw": "\n\nAccurate annotation of educational resources is critical in the rapidly advancing field of online education due to the complexity and volume of content. Existing classification methods face challenges with semantic overlap and distribution imbalance of labels in the multi-label context, which impedes effective personalized learning and resource recommendation. This paper introduces RR2QC, a novel Retrieval Reranking method To multi-label Question Classification by leveraging label semantics and meta-label refinement. Firstly, RR2QC leverages semantic relationships within and across label groups to enhance pre-training strategie in multi-label context. Next, a class center learning task is introduced, integrating label texts into downstream training to ensure questions consistently align with label semantics, retrieving the most relevant label sequences. Finally, this method decomposes labels into meta-labels and trains a meta-label classifier to rerank the retrieved label sequences. In doing so, RR2QC enhances the understanding and prediction capability of long-tail labels by learning from meta-labels frequently appearing in other labels. Addtionally, a Math LLM is used to generate solutions for questions, extracting latent information to further refine the model's insights. Experimental results demonstrate that RR2QC outperforms existing classification methods in Precision@k and F1 scores across multiple educational datasets, establishing it as a potent enhancement for online educational content utilization.",
      "code": "var Component=(()=>{var d=Object.create;var a=Object.defineProperty;var h=Object.getOwnPropertyDescriptor;var g=Object.getOwnPropertyNames;var f=Object.getPrototypeOf,b=Object.prototype.hasOwnProperty;var p=(e,n)=>()=>(n||e((n={exports:{}}).exports,n),n.exports),v=(e,n)=>{for(var t in n)a(e,t,{get:n[t],enumerable:!0})},o=(e,n,t,s)=>{if(n&&typeof n==\"object\"||typeof n==\"function\")for(let i of g(n))!b.call(e,i)&&i!==t&&a(e,i,{get:()=>n[i],enumerable:!(s=h(n,i))||s.enumerable});return e};var C=(e,n,t)=>(t=e!=null?d(f(e)):{},o(n||!e||!e.__esModule?a(t,\"default\",{value:e,enumerable:!0}):t,e)),x=e=>o(a({},\"__esModule\",{value:!0}),e);var c=p((M,r)=>{r.exports=_jsx_runtime});var y={};v(y,{default:()=>u,frontmatter:()=>R});var l=C(c()),R={title:\"Leveraging Label Semantics and Meta-Label Refinement for Multi-Label Question Classification\",date:\"2024-11-04\",tags:[\"distribution imbalance\",\"Math LLM\",\"Retrieval Reranking\",\"personalized learning\",\"resource recommendation\",\"semantic overlap\",\"class center learning\",\"F1 scores\",\"meta-label refinement\",\"educational resources\",\"Precision@k\",\"multi-label question classification\",\"label semantics\"],categories:[\"cs.CL\",\"cs.LG\"],problem:\"challenges with semantic overlap and distribution imbalance of labels in the multi-label context\",solution:\"RR2QC, a novel Retrieval Reranking method for multi-label Question Classification\",pdf_url:\"http://arxiv.org/pdf/2411.01841v1\",arx_url:\"http://arxiv.org/abs/2411.01841v1\",score:6,authors:[\"Shi Dong\",\"Xiaobei Niu\",\"Rui Zhong\",\"Zhifeng Wang\",\"Mingzhang Zuo\"],affiliations_aligned:[\"Central China Normal University\",\"Central China Normal University\",\"Central China Normal University\",\"Central China Normal University\",\"Central China Normal University\"],affiliations:[\"Central China Normal University\"]};function m(e){let n={p:\"p\",...e.components};return(0,l.jsx)(n.p,{children:\"Accurate annotation of educational resources is critical in the rapidly advancing field of online education due to the complexity and volume of content. Existing classification methods face challenges with semantic overlap and distribution imbalance of labels in the multi-label context, which impedes effective personalized learning and resource recommendation. This paper introduces RR2QC, a novel Retrieval Reranking method To multi-label Question Classification by leveraging label semantics and meta-label refinement. Firstly, RR2QC leverages semantic relationships within and across label groups to enhance pre-training strategie in multi-label context. Next, a class center learning task is introduced, integrating label texts into downstream training to ensure questions consistently align with label semantics, retrieving the most relevant label sequences. Finally, this method decomposes labels into meta-labels and trains a meta-label classifier to rerank the retrieved label sequences. In doing so, RR2QC enhances the understanding and prediction capability of long-tail labels by learning from meta-labels frequently appearing in other labels. Addtionally, a Math LLM is used to generate solutions for questions, extracting latent information to further refine the model's insights. Experimental results demonstrate that RR2QC outperforms existing classification methods in Precision@k and F1 scores across multiple educational datasets, establishing it as a potent enhancement for online educational content utilization.\"})}function u(e={}){let{wrapper:n}=e.components||{};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(m,{...e})}):m(e)}return x(y);})();\n;return Component;"
    },
    "_id": "articles/2411.01841.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.01841.mdx",
      "sourceFileName": "2411.01841.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.01841"
    },
    "type": "Blog",
    "readingTime": {
      "text": "2 min read",
      "minutes": 1.01,
      "time": 60600,
      "words": 202
    },
    "slug": "2411.01841",
    "path": "articles/2411.01841",
    "filePath": "articles/2411.01841.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "Leveraging Label Semantics and Meta-Label Refinement for Multi-Label Question Classification",
      "datePublished": "2024-11-04T00:00:00.000Z",
      "dateModified": "2024-11-04T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.01841"
    }
  },
  {
    "title": "Typicalness-Aware Learning for Failure Detection",
    "date": "2024-11-08T00:00:00.000Z",
    "tags": [
      "cross-entropy loss",
      "atypical samples",
      "logit direction",
      "Area Under the Risk-Coverage Curve",
      "failure detection",
      "metric for typicalness",
      "logit magnitude",
      "benchmark datasets",
      "Typicalness-Aware Learning",
      "Deep neural networks",
      "overconfidence issue"
    ],
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "https://arxiv.org/pdf/2411.01981",
    "arx_url": "https://arxiv.org/abs/2411.01981",
    "authors": [
      "Yijun Liu",
      "Jiequan Cui",
      "Zhuotao Tian",
      "Senqiao Yang",
      "Qingdong He",
      "Xiaoling Wang",
      "Jingyong Su"
    ],
    "affiliations_aligned": [
      "Harbin Institute of Technology (Shenzhen)",
      "Nanyang Technological University",
      "Harbin Institute of Technology (Shenzhen)",
      "The Chinese University of Hong Kong",
      "Tencent Youtu Lab",
      "Harbin Institute of Technology (Shenzhen)",
      "Harbin Institute of Technology (Shenzhen)"
    ],
    "affiliations": [
      "Harbin Institute of Technology (Shenzhen)",
      "Tencent Youtu Lab",
      "Nanyang Technological University",
      "The Chinese University of Hong Kong"
    ],
    "problem": "overconfidence issue in deep neural networks",
    "solution": "Typicalness-Aware Learning approach",
    "score": 6,
    "body": {
      "raw": "\n\nDeep neural networks (DNNs) often suffer from the overconfidence issue, where incorrect predictions are made with high confidence scores, hindering the applications in critical systems. In this paper, we propose a novel approach called Typicalness-Aware Learning (TAL) to address this issue and improve failure detection performance. We observe that, with the cross-entropy loss, model predictions are optimized to align with the corresponding labels via increasing logit magnitude or refining logit direction. However, regarding atypical samples, the image content and their labels may exhibit disparities. This discrepancy can lead to overfitting on atypical samples, ultimately resulting in the overconfidence issue that we aim to address. To tackle the problem, we have devised a metric that quantifies the typicalness of each sample, enabling the dynamic adjustment of the logit magnitude during the training process. By allowing atypical samples to be adequately fitted while preserving reliable logit direction, the problem of overconfidence can be mitigated. TAL has been extensively evaluated on benchmark datasets, and the results demonstrate its superiority over existing failure detection methods. Specifically, TAL achieves a more than 5% improvement on CIFAR100 in terms of the Area Under the Risk-Coverage Curve (AURC) compared to the state-of-the-art. Code is available at https://github.com/liuyijungoon/TAL.",
      "code": "var Component=(()=>{var g=Object.create;var o=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var f=Object.getPrototypeOf,m=Object.prototype.hasOwnProperty;var y=(e,n)=>()=>(n||e((n={exports:{}}).exports,n),n.exports),v=(e,n)=>{for(var t in n)o(e,t,{get:n[t],enumerable:!0})},s=(e,n,t,r)=>{if(n&&typeof n==\"object\"||typeof n==\"function\")for(let a of u(n))!m.call(e,a)&&a!==t&&o(e,a,{get:()=>n[a],enumerable:!(r=p(n,a))||r.enumerable});return e};var b=(e,n,t)=>(t=e!=null?g(f(e)):{},s(n||!e||!e.__esModule?o(t,\"default\",{value:e,enumerable:!0}):t,e)),T=e=>s(o({},\"__esModule\",{value:!0}),e);var l=y((C,c)=>{c.exports=_jsx_runtime});var x={};v(x,{default:()=>d,frontmatter:()=>w});var i=b(l()),w={title:\"Typicalness-Aware Learning for Failure Detection\",date:\"2024-11-08\",tags:[\"cross-entropy loss\",\"atypical samples\",\"logit direction\",\"Area Under the Risk-Coverage Curve\",\"failure detection\",\"metric for typicalness\",\"logit magnitude\",\"benchmark datasets\",\"Typicalness-Aware Learning\",\"Deep neural networks\",\"overconfidence issue\"],categories:[\"cs.CV\"],problem:\"overconfidence issue in deep neural networks\",solution:\"Typicalness-Aware Learning approach\",pdf_url:\"https://arxiv.org/pdf/2411.01981\",arx_url:\"https://arxiv.org/abs/2411.01981\",score:6,authors:[\"Yijun Liu\",\"Jiequan Cui\",\"Zhuotao Tian\",\"Senqiao Yang\",\"Qingdong He\",\"Xiaoling Wang\",\"Jingyong Su\"],affiliations_aligned:[\"Harbin Institute of Technology (Shenzhen)\",\"Nanyang Technological University\",\"Harbin Institute of Technology (Shenzhen)\",\"The Chinese University of Hong Kong\",\"Tencent Youtu Lab\",\"Harbin Institute of Technology (Shenzhen)\",\"Harbin Institute of Technology (Shenzhen)\"],affiliations:[\"Harbin Institute of Technology (Shenzhen)\",\"Tencent Youtu Lab\",\"Nanyang Technological University\",\"The Chinese University of Hong Kong\"]};function h(e){let n={a:\"a\",p:\"p\",...e.components};return(0,i.jsxs)(n.p,{children:[\"Deep neural networks (DNNs) often suffer from the overconfidence issue, where incorrect predictions are made with high confidence scores, hindering the applications in critical systems. In this paper, we propose a novel approach called Typicalness-Aware Learning (TAL) to address this issue and improve failure detection performance. We observe that, with the cross-entropy loss, model predictions are optimized to align with the corresponding labels via increasing logit magnitude or refining logit direction. However, regarding atypical samples, the image content and their labels may exhibit disparities. This discrepancy can lead to overfitting on atypical samples, ultimately resulting in the overconfidence issue that we aim to address. To tackle the problem, we have devised a metric that quantifies the typicalness of each sample, enabling the dynamic adjustment of the logit magnitude during the training process. By allowing atypical samples to be adequately fitted while preserving reliable logit direction, the problem of overconfidence can be mitigated. TAL has been extensively evaluated on benchmark datasets, and the results demonstrate its superiority over existing failure detection methods. Specifically, TAL achieves a more than 5% improvement on CIFAR100 in terms of the Area Under the Risk-Coverage Curve (AURC) compared to the state-of-the-art. Code is available at \",(0,i.jsx)(n.a,{href:\"https://github.com/liuyijungoon/TAL\",children:\"https://github.com/liuyijungoon/TAL\"}),\".\"]})}function d(e={}){let{wrapper:n}=e.components||{};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}return T(x);})();\n;return Component;"
    },
    "_id": "articles/2411.01981.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.01981.mdx",
      "sourceFileName": "2411.01981.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.01981"
    },
    "type": "Blog",
    "readingTime": {
      "text": "1 min read",
      "minutes": 1,
      "time": 60000,
      "words": 200
    },
    "slug": "2411.01981",
    "path": "articles/2411.01981",
    "filePath": "articles/2411.01981.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "Typicalness-Aware Learning for Failure Detection",
      "datePublished": "2024-11-08T00:00:00.000Z",
      "dateModified": "2024-11-08T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.01981"
    }
  },
  {
    "title": "CRMArena: Understanding the Capacity of LLM Agents to Perform Professional CRM Tasks in Realistic Environments",
    "date": "2024-11-04T00:00:00.000Z",
    "tags": [
      "latent variables",
      "Customer Relationship Management",
      "customer service tasks",
      "AI agents",
      "rule-following",
      "data distributions",
      "agent capabilities",
      "benchmarking",
      "realistic tasks",
      "function-calling",
      "professional work environments"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2411.02305v1",
    "arx_url": "http://arxiv.org/abs/2411.02305v1",
    "authors": [
      "Kung-Hsiang Huang",
      "Akshara Prabhakar",
      "Sidharth Dhawan",
      "Yixin Mao",
      "Huan Wang",
      "Silvio Savarese",
      "Caiming Xiong",
      "Philippe Laban",
      "Chien-Sheng Wu"
    ],
    "affiliations_aligned": [
      "Salesforce AI Research",
      "Salesforce AI Research",
      "Salesforce AI Research",
      "Salesforce AI Research",
      "Salesforce AI Research",
      "Salesforce AI Research",
      "Salesforce AI Research",
      "Salesforce AI Research",
      "Salesforce AI Research"
    ],
    "affiliations": [
      "Salesforce AI Research"
    ],
    "problem": "lack of realistic benchmarks for evaluating AI agents in CRM systems",
    "solution": "CRMArena benchmark for evaluating AI agents on realistic CRM tasks",
    "score": 6,
    "body": {
      "raw": "\n\nCustomer Relationship Management (CRM) systems are vital for modern enterprises, providing a foundation for managing customer interactions and data. Integrating AI agents into CRM systems can automate routine processes and enhance personalized service. However, deploying and evaluating these agents is challenging due to the lack of realistic benchmarks that reflect the complexity of real-world CRM tasks. To address this issue, we introduce CRMArena, a novel benchmark designed to evaluate AI agents on realistic tasks grounded in professional work environments. Following guidance from CRM experts and industry best practices, we designed CRMArena with nine customer service tasks distributed across three personas: service agent, analyst, and manager. The benchmark includes 16 commonly used industrial objects (e.g., account, order, knowledge article, case) with high interconnectivity, along with latent variables (e.g., complaint habits, policy violations) to simulate realistic data distributions. Experimental results reveal that state-of-the-art LLM agents succeed in less than 40% of the tasks with ReAct prompting, and less than 55% even with function-calling abilities. Our findings highlight the need for enhanced agent capabilities in function-calling and rule-following to be deployed in real-world work environments. CRMArena is an open challenge to the community: systems that can reliably complete tasks showcase direct business value in a popular work environment.",
      "code": "var Component=(()=>{var d=Object.create;var s=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var f=Object.getPrototypeOf,p=Object.prototype.hasOwnProperty;var v=(e,a)=>()=>(a||e((a={exports:{}}).exports,a),a.exports),R=(e,a)=>{for(var n in a)s(e,n,{get:a[n],enumerable:!0})},o=(e,a,n,r)=>{if(a&&typeof a==\"object\"||typeof a==\"function\")for(let t of m(a))!p.call(e,t)&&t!==n&&s(e,t,{get:()=>a[t],enumerable:!(r=u(a,t))||r.enumerable});return e};var b=(e,a,n)=>(n=e!=null?d(f(e)):{},o(a||!e||!e.__esModule?s(n,\"default\",{value:e,enumerable:!0}):n,e)),k=e=>o(s({},\"__esModule\",{value:!0}),e);var l=v((M,c)=>{c.exports=_jsx_runtime});var w={};R(w,{default:()=>g,frontmatter:()=>A});var i=b(l()),A={title:\"CRMArena: Understanding the Capacity of LLM Agents to Perform Professional CRM Tasks in Realistic Environments\",date:\"2024-11-04\",tags:[\"latent variables\",\"Customer Relationship Management\",\"customer service tasks\",\"AI agents\",\"rule-following\",\"data distributions\",\"agent capabilities\",\"benchmarking\",\"realistic tasks\",\"function-calling\",\"professional work environments\"],categories:[\"cs.CL\",\"cs.AI\"],problem:\"lack of realistic benchmarks for evaluating AI agents in CRM systems\",solution:\"CRMArena benchmark for evaluating AI agents on realistic CRM tasks\",pdf_url:\"http://arxiv.org/pdf/2411.02305v1\",arx_url:\"http://arxiv.org/abs/2411.02305v1\",score:6,authors:[\"Kung-Hsiang Huang\",\"Akshara Prabhakar\",\"Sidharth Dhawan\",\"Yixin Mao\",\"Huan Wang\",\"Silvio Savarese\",\"Caiming Xiong\",\"Philippe Laban\",\"Chien-Sheng Wu\"],affiliations_aligned:[\"Salesforce AI Research\",\"Salesforce AI Research\",\"Salesforce AI Research\",\"Salesforce AI Research\",\"Salesforce AI Research\",\"Salesforce AI Research\",\"Salesforce AI Research\",\"Salesforce AI Research\",\"Salesforce AI Research\"],affiliations:[\"Salesforce AI Research\"]};function h(e){let a={p:\"p\",...e.components};return(0,i.jsx)(a.p,{children:\"Customer Relationship Management (CRM) systems are vital for modern enterprises, providing a foundation for managing customer interactions and data. Integrating AI agents into CRM systems can automate routine processes and enhance personalized service. However, deploying and evaluating these agents is challenging due to the lack of realistic benchmarks that reflect the complexity of real-world CRM tasks. To address this issue, we introduce CRMArena, a novel benchmark designed to evaluate AI agents on realistic tasks grounded in professional work environments. Following guidance from CRM experts and industry best practices, we designed CRMArena with nine customer service tasks distributed across three personas: service agent, analyst, and manager. The benchmark includes 16 commonly used industrial objects (e.g., account, order, knowledge article, case) with high interconnectivity, along with latent variables (e.g., complaint habits, policy violations) to simulate realistic data distributions. Experimental results reveal that state-of-the-art LLM agents succeed in less than 40% of the tasks with ReAct prompting, and less than 55% even with function-calling abilities. Our findings highlight the need for enhanced agent capabilities in function-calling and rule-following to be deployed in real-world work environments. CRMArena is an open challenge to the community: systems that can reliably complete tasks showcase direct business value in a popular work environment.\"})}function g(e={}){let{wrapper:a}=e.components||{};return a?(0,i.jsx)(a,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}return k(w);})();\n;return Component;"
    },
    "_id": "articles/2411.02305.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.02305.mdx",
      "sourceFileName": "2411.02305.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.02305"
    },
    "type": "Blog",
    "readingTime": {
      "text": "2 min read",
      "minutes": 1.025,
      "time": 61500,
      "words": 205
    },
    "slug": "2411.02305",
    "path": "articles/2411.02305",
    "filePath": "articles/2411.02305.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "CRMArena: Understanding the Capacity of LLM Agents to Perform Professional CRM Tasks in Realistic Environments",
      "datePublished": "2024-11-04T00:00:00.000Z",
      "dateModified": "2024-11-04T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.02305"
    }
  },
  {
    "title": "Seq-VCR: Preventing Collapse in Intermediate Transformer Representations for Enhanced Reasoning",
    "date": "2024-11-04T00:00:00.000Z",
    "tags": [
      "exact match accuracy",
      "arithmetic expression datasets",
      "chain-of-thought tokens",
      "longest increasing subsequence datasets",
      "performance improvement",
      "integer multiplication task",
      "entropy of intermediate representations",
      "arithmetic reasoning",
      "representation collapse",
      "Decoder-only Transformers",
      "dummy pause tokens",
      "complex reasoning tasks",
      "Sequential Variance-Covariance Regularization"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2411.02344v1",
    "arx_url": "http://arxiv.org/abs/2411.02344v1",
    "authors": [
      "Md Rifat Arefin",
      "Gopeshh Subbaraj",
      "Nicolas Gontier",
      "Yann LeCun",
      "Irina Rish",
      "Ravid Shwartz-Ziv",
      "Christopher Pal"
    ],
    "affiliations_aligned": [
      "UniversitÃ© de MontrÃ©al, Mila",
      "UniversitÃ© de MontrÃ©al, Mila",
      "ServiceNow",
      "Meta FAIR",
      "UniversitÃ© de MontrÃ©al, Mila",
      "New York University",
      "ServiceNow, Polytechnique Montreal"
    ],
    "affiliations": [
      "ServiceNow, Polytechnique Montreal",
      "UniversitÃ© de MontrÃ©al, Mila",
      "Meta FAIR",
      "New York University",
      "ServiceNow"
    ],
    "problem": "representation collapse in intermediate layers limiting reasoning capabilities",
    "solution": "Sequential Variance-Covariance Regularization (Seq-VCR)",
    "score": 6,
    "body": {
      "raw": "\n\nDecoder-only Transformers often struggle with complex reasoning tasks, particularly arithmetic reasoning requiring multiple sequential operations. In this work, we identify representation collapse in the model's intermediate layers as a key factor limiting their reasoning capabilities. To address this, we propose Sequential Variance-Covariance Regularization (Seq-VCR), which enhances the entropy of intermediate representations and prevents collapse. Combined with dummy pause tokens as substitutes for chain-of-thought (CoT) tokens, our method significantly improves performance in arithmetic reasoning problems. In the challenging $5 \\times 5$ integer multiplication task, our approach achieves $99.5\\%$ exact match accuracy, outperforming models of the same size (which yield $0\\%$ accuracy) and GPT-4 with five-shot CoT prompting ($44\\%$). We also demonstrate superior results on arithmetic expression and longest increasing subsequence (LIS) datasets. Our findings highlight the importance of preventing intermediate layer representation collapse to enhance the reasoning capabilities of Transformers and show that Seq-VCR offers an effective solution without requiring explicit CoT supervision.",
      "code": "var Component=(()=>{var d=Object.create;var s=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var g=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,x=Object.prototype.hasOwnProperty;var w=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),f=(n,e)=>{for(var t in e)s(n,t,{get:e[t],enumerable:!0})},l=(n,e,t,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of g(e))!x.call(n,i)&&i!==t&&s(n,i,{get:()=>e[i],enumerable:!(r=p(e,i))||r.enumerable});return n};var N=(n,e,t)=>(t=n!=null?d(u(n)):{},l(e||!n||!n.__esModule?s(t,\"default\",{value:n,enumerable:!0}):t,n)),v=n=>l(s({},\"__esModule\",{value:!0}),n);var c=w((b,o)=>{o.exports=_jsx_runtime});var M={};f(M,{default:()=>h,frontmatter:()=>y});var a=N(c()),y={title:\"Seq-VCR: Preventing Collapse in Intermediate Transformer Representations for Enhanced Reasoning\",date:\"2024-11-04\",tags:[\"exact match accuracy\",\"arithmetic expression datasets\",\"chain-of-thought tokens\",\"longest increasing subsequence datasets\",\"performance improvement\",\"integer multiplication task\",\"entropy of intermediate representations\",\"arithmetic reasoning\",\"representation collapse\",\"Decoder-only Transformers\",\"dummy pause tokens\",\"complex reasoning tasks\",\"Sequential Variance-Covariance Regularization\"],categories:[\"cs.CL\",\"cs.LG\"],problem:\"representation collapse in intermediate layers limiting reasoning capabilities\",solution:\"Sequential Variance-Covariance Regularization (Seq-VCR)\",pdf_url:\"http://arxiv.org/pdf/2411.02344v1\",arx_url:\"http://arxiv.org/abs/2411.02344v1\",score:6,authors:[\"Md Rifat Arefin\",\"Gopeshh Subbaraj\",\"Nicolas Gontier\",\"Yann LeCun\",\"Irina Rish\",\"Ravid Shwartz-Ziv\",\"Christopher Pal\"],affiliations_aligned:[\"Universit\\xE9 de Montr\\xE9al, Mila\",\"Universit\\xE9 de Montr\\xE9al, Mila\",\"ServiceNow\",\"Meta FAIR\",\"Universit\\xE9 de Montr\\xE9al, Mila\",\"New York University\",\"ServiceNow, Polytechnique Montreal\"],affiliations:[\"ServiceNow, Polytechnique Montreal\",\"Universit\\xE9 de Montr\\xE9al, Mila\",\"Meta FAIR\",\"New York University\",\"ServiceNow\"]};function m(n){let e={annotation:\"annotation\",math:\"math\",mi:\"mi\",mn:\"mn\",mo:\"mo\",mrow:\"mrow\",p:\"p\",semantics:\"semantics\",span:\"span\",...n.components};return(0,a.jsxs)(e.p,{children:[\"Decoder-only Transformers often struggle with complex reasoning tasks, particularly arithmetic reasoning requiring multiple sequential operations. In this work, we identify representation collapse in the model's intermediate layers as a key factor limiting their reasoning capabilities. To address this, we propose Sequential Variance-Covariance Regularization (Seq-VCR), which enhances the entropy of intermediate representations and prevents collapse. Combined with dummy pause tokens as substitutes for chain-of-thought (CoT) tokens, our method significantly improves performance in arithmetic reasoning problems. In the challenging \",(0,a.jsxs)(e.span,{className:\"katex\",translate:\"no\",children:[(0,a.jsx)(e.span,{className:\"katex-mathml\",children:(0,a.jsx)(e.math,{xmlns:\"http://www.w3.org/1998/Math/MathML\",children:(0,a.jsxs)(e.semantics,{children:[(0,a.jsxs)(e.mrow,{children:[(0,a.jsx)(e.mn,{children:\"5\"}),(0,a.jsx)(e.mo,{children:\"\\xD7\"}),(0,a.jsx)(e.mn,{children:\"5\"})]}),(0,a.jsx)(e.annotation,{encoding:\"application/x-tex\",children:\"5 \\\\times 5\"})]})})}),(0,a.jsxs)(e.span,{className:\"katex-html\",\"aria-hidden\":\"true\",children:[(0,a.jsxs)(e.span,{className:\"base\",children:[(0,a.jsx)(e.span,{className:\"strut\",style:{height:\".7278em\",verticalAlign:\"-.0833em\"}}),(0,a.jsx)(e.span,{className:\"mord\",children:\"5\"}),(0,a.jsx)(e.span,{className:\"mspace\",style:{marginRight:\".2222em\"}}),(0,a.jsx)(e.span,{className:\"mbin\",children:\"\\xD7\"}),(0,a.jsx)(e.span,{className:\"mspace\",style:{marginRight:\".2222em\"}})]}),(0,a.jsxs)(e.span,{className:\"base\",children:[(0,a.jsx)(e.span,{className:\"strut\",style:{height:\".6444em\"}}),(0,a.jsx)(e.span,{className:\"mord\",children:\"5\"})]})]})]}),\" integer multiplication task, our approach achieves \",(0,a.jsxs)(e.span,{className:\"katex\",translate:\"no\",children:[(0,a.jsx)(e.span,{className:\"katex-mathml\",children:(0,a.jsx)(e.math,{xmlns:\"http://www.w3.org/1998/Math/MathML\",children:(0,a.jsxs)(e.semantics,{children:[(0,a.jsxs)(e.mrow,{children:[(0,a.jsx)(e.mn,{children:\"99.5\"}),(0,a.jsx)(e.mi,{mathvariant:\"normal\",children:\"%\"})]}),(0,a.jsx)(e.annotation,{encoding:\"application/x-tex\",children:\"99.5\\\\%\"})]})})}),(0,a.jsx)(e.span,{className:\"katex-html\",\"aria-hidden\":\"true\",children:(0,a.jsxs)(e.span,{className:\"base\",children:[(0,a.jsx)(e.span,{className:\"strut\",style:{height:\".8056em\",verticalAlign:\"-.0556em\"}}),(0,a.jsx)(e.span,{className:\"mord\",children:\"99.5%\"})]})})]}),\" exact match accuracy, outperforming models of the same size (which yield \",(0,a.jsxs)(e.span,{className:\"katex\",translate:\"no\",children:[(0,a.jsx)(e.span,{className:\"katex-mathml\",children:(0,a.jsx)(e.math,{xmlns:\"http://www.w3.org/1998/Math/MathML\",children:(0,a.jsxs)(e.semantics,{children:[(0,a.jsxs)(e.mrow,{children:[(0,a.jsx)(e.mn,{children:\"0\"}),(0,a.jsx)(e.mi,{mathvariant:\"normal\",children:\"%\"})]}),(0,a.jsx)(e.annotation,{encoding:\"application/x-tex\",children:\"0\\\\%\"})]})})}),(0,a.jsx)(e.span,{className:\"katex-html\",\"aria-hidden\":\"true\",children:(0,a.jsxs)(e.span,{className:\"base\",children:[(0,a.jsx)(e.span,{className:\"strut\",style:{height:\".8056em\",verticalAlign:\"-.0556em\"}}),(0,a.jsx)(e.span,{className:\"mord\",children:\"0%\"})]})})]}),\" accuracy) and GPT-4 with five-shot CoT prompting (\",(0,a.jsxs)(e.span,{className:\"katex\",translate:\"no\",children:[(0,a.jsx)(e.span,{className:\"katex-mathml\",children:(0,a.jsx)(e.math,{xmlns:\"http://www.w3.org/1998/Math/MathML\",children:(0,a.jsxs)(e.semantics,{children:[(0,a.jsxs)(e.mrow,{children:[(0,a.jsx)(e.mn,{children:\"44\"}),(0,a.jsx)(e.mi,{mathvariant:\"normal\",children:\"%\"})]}),(0,a.jsx)(e.annotation,{encoding:\"application/x-tex\",children:\"44\\\\%\"})]})})}),(0,a.jsx)(e.span,{className:\"katex-html\",\"aria-hidden\":\"true\",children:(0,a.jsxs)(e.span,{className:\"base\",children:[(0,a.jsx)(e.span,{className:\"strut\",style:{height:\".8056em\",verticalAlign:\"-.0556em\"}}),(0,a.jsx)(e.span,{className:\"mord\",children:\"44%\"})]})})]}),\"). We also demonstrate superior results on arithmetic expression and longest increasing subsequence (LIS) datasets. Our findings highlight the importance of preventing intermediate layer representation collapse to enhance the reasoning capabilities of Transformers and show that Seq-VCR offers an effective solution without requiring explicit CoT supervision.\"]})}function h(n={}){let{wrapper:e}=n.components||{};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(m,{...n})}):m(n)}return v(M);})();\n;return Component;"
    },
    "_id": "articles/2411.02344.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.02344.mdx",
      "sourceFileName": "2411.02344.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.02344"
    },
    "type": "Blog",
    "readingTime": {
      "text": "1 min read",
      "minutes": 0.76,
      "time": 45600,
      "words": 152
    },
    "slug": "2411.02344",
    "path": "articles/2411.02344",
    "filePath": "articles/2411.02344.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "Seq-VCR: Preventing Collapse in Intermediate Transformer Representations for Enhanced Reasoning",
      "datePublished": "2024-11-04T00:00:00.000Z",
      "dateModified": "2024-11-04T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.02344"
    }
  },
  {
    "title": "Enhancing Retrieval Performance: An Ensemble Approach For Hard Negative Mining",
    "date": "2024-10-18T00:00:00.000Z",
    "tags": [
      "Retrieval-Augmented Generation",
      "Reasoning and Action Agents",
      "cross-encoder models",
      "retrieval performance",
      "enterprise dataset",
      "negative pairs",
      "ranking models",
      "hard negative mining",
      "information retrieval"
    ],
    "categories": [
      "cs.CL",
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2411.02404v1",
    "arx_url": "http://arxiv.org/abs/2411.02404v1",
    "authors": [
      "Hansa Meghwani"
    ],
    "affiliations_aligned": [
      ""
    ],
    "affiliations": [
      ""
    ],
    "problem": "identifying suitable negative pairs from a vast pool of documents",
    "solution": "robust hard negative mining technique for efficient training of cross-encoder re-rank models",
    "score": 6,
    "body": {
      "raw": "\n\nRanking consistently emerges as a primary focus in information retrieval research. Retrieval and ranking models serve as the foundation for numerous applications, including web search, open domain QA, enterprise domain QA, and text-based recommender systems. Typically, these models undergo training on triplets consisting of binary relevance assignments, comprising one positive and one negative passage. However, their utilization involves a context where a significantly more nuanced understanding of relevance is necessary, especially when re-ranking a large pool of potentially relevant passages. Although collecting positive examples through user feedback like impressions or clicks is straightforward, identifying suitable negative pairs from a vast pool of possibly millions or even billions of documents possess a greater challenge. Generating a substantial number of negative pairs is often necessary to maintain the high quality of the model. Several approaches have been suggested in literature to tackle the issue of selecting suitable negative pairs from an extensive corpus. This study focuses on explaining the crucial role of hard negatives in the training process of cross-encoder models, specifically aiming to explain the performance gains observed with hard negative sampling compared to random sampling. We have developed a robust hard negative mining technique for efficient training of cross-encoder re-rank models on an enterprise dataset which has domain specific context. We provide a novel perspective to enhance retrieval models, ultimately influencing the performance of advanced LLM systems like Retrieval-Augmented Generation (RAG) and Reasoning and Action Agents (ReAct). The proposed approach demonstrates that learning both similarity and dissimilarity simultaneously with cross-encoders improves performance of retrieval systems.",
      "code": "var Component=(()=>{var m=Object.create;var t=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var f=Object.getOwnPropertyNames;var h=Object.getPrototypeOf,u=Object.prototype.hasOwnProperty;var v=(e,n)=>()=>(n||e((n={exports:{}}).exports,n),n.exports),y=(e,n)=>{for(var i in n)t(e,i,{get:n[i],enumerable:!0})},o=(e,n,i,r)=>{if(n&&typeof n==\"object\"||typeof n==\"function\")for(let a of f(n))!u.call(e,a)&&a!==i&&t(e,a,{get:()=>n[a],enumerable:!(r=p(n,a))||r.enumerable});return e};var b=(e,n,i)=>(i=e!=null?m(h(e)):{},o(n||!e||!e.__esModule?t(i,\"default\",{value:e,enumerable:!0}):i,e)),x=e=>o(t({},\"__esModule\",{value:!0}),e);var c=v((R,l)=>{l.exports=_jsx_runtime});var k={};y(k,{default:()=>g,frontmatter:()=>A});var s=b(c()),A={title:\"Enhancing Retrieval Performance: An Ensemble Approach For Hard Negative Mining\",date:\"2024-10-18\",tags:[\"Retrieval-Augmented Generation\",\"Reasoning and Action Agents\",\"cross-encoder models\",\"retrieval performance\",\"enterprise dataset\",\"negative pairs\",\"ranking models\",\"hard negative mining\",\"information retrieval\"],categories:[\"cs.CL\",\"cs.IR\",\"cs.AI\",\"cs.LG\"],problem:\"identifying suitable negative pairs from a vast pool of documents\",solution:\"robust hard negative mining technique for efficient training of cross-encoder re-rank models\",pdf_url:\"http://arxiv.org/pdf/2411.02404v1\",arx_url:\"http://arxiv.org/abs/2411.02404v1\",score:6,authors:[\"Hansa Meghwani\"],affiliations_aligned:[\"\"],affiliations:[\"\"]};function d(e){let n={p:\"p\",...e.components};return(0,s.jsx)(n.p,{children:\"Ranking consistently emerges as a primary focus in information retrieval research. Retrieval and ranking models serve as the foundation for numerous applications, including web search, open domain QA, enterprise domain QA, and text-based recommender systems. Typically, these models undergo training on triplets consisting of binary relevance assignments, comprising one positive and one negative passage. However, their utilization involves a context where a significantly more nuanced understanding of relevance is necessary, especially when re-ranking a large pool of potentially relevant passages. Although collecting positive examples through user feedback like impressions or clicks is straightforward, identifying suitable negative pairs from a vast pool of possibly millions or even billions of documents possess a greater challenge. Generating a substantial number of negative pairs is often necessary to maintain the high quality of the model. Several approaches have been suggested in literature to tackle the issue of selecting suitable negative pairs from an extensive corpus. This study focuses on explaining the crucial role of hard negatives in the training process of cross-encoder models, specifically aiming to explain the performance gains observed with hard negative sampling compared to random sampling. We have developed a robust hard negative mining technique for efficient training of cross-encoder re-rank models on an enterprise dataset which has domain specific context. We provide a novel perspective to enhance retrieval models, ultimately influencing the performance of advanced LLM systems like Retrieval-Augmented Generation (RAG) and Reasoning and Action Agents (ReAct). The proposed approach demonstrates that learning both similarity and dissimilarity simultaneously with cross-encoders improves performance of retrieval systems.\"})}function g(e={}){let{wrapper:n}=e.components||{};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}return x(k);})();\n;return Component;"
    },
    "_id": "articles/2411.02404.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.02404.mdx",
      "sourceFileName": "2411.02404.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.02404"
    },
    "type": "Blog",
    "readingTime": {
      "text": "2 min read",
      "minutes": 1.275,
      "time": 76500,
      "words": 255
    },
    "slug": "2411.02404",
    "path": "articles/2411.02404",
    "filePath": "articles/2411.02404.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "Enhancing Retrieval Performance: An Ensemble Approach For Hard Negative Mining",
      "datePublished": "2024-10-18T00:00:00.000Z",
      "dateModified": "2024-10-18T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.02404"
    }
  },
  {
    "title": "Rate, Explain and Cite (REC): Enhanced Explanation and Attribution in Automatic Evaluation by Large Language Models",
    "date": "2024-11-03T00:00:00.000Z",
    "tags": [
      "instruction following",
      "evaluation metrics",
      "faithfulness",
      "text generation",
      "factual inaccuracies",
      "hallucinations",
      "citations",
      "Large Language Models",
      "coherence",
      "trust in content",
      "benchmark evaluations",
      "completeness",
      "auto-evaluators"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2411.02448v1",
    "arx_url": "http://arxiv.org/abs/2411.02448v1",
    "authors": [
      "Aliyah R. Hsu",
      "James Zhu",
      "Zhichao Wang",
      "Bin Bi",
      "Shubham Mehrotra",
      "Shiva K. Pentyala",
      "Katherine Tan",
      "Xiang-Bo Mao",
      "Roshanak Omrani",
      "Sougata Chaudhuri",
      "Regunathan Radhakrishnan",
      "Sitaram Asur",
      "Claire Na Cheng",
      "Bin Yu"
    ],
    "affiliations_aligned": [
      "UC Berkeley",
      "Salesforce AI Platform",
      "Salesforce AI Platform",
      "Salesforce AI Platform",
      "Salesforce AI Platform",
      "Salesforce AI Platform",
      "Salesforce AI Platform",
      "Salesforce AI Platform",
      "Salesforce AI Platform",
      "Salesforce AI Platform",
      "Salesforce AI Platform",
      "Salesforce AI Platform",
      "Salesforce AI Platform",
      "UC Berkeley"
    ],
    "affiliations": [
      "UC Berkeley",
      "Salesforce AI Platform"
    ],
    "problem": "rigorous evaluation of generated content quality",
    "solution": "fine-tuned general-purpose LLM autoevaluators",
    "score": 6,
    "body": {
      "raw": "\nLLMs have demonstrated impressive proficiency in generating coherent and high-quality text, making them valuable across a range of text-generation tasks. However, rigorous evaluation of this generated content is crucial, as ensuring its quality remains a significant challenge due to persistent issues such as factual inaccuracies and hallucinations. This paper introduces two fine-tuned general-purpose LLM autoevaluators, REC-12B and REC-70B, specifically designed to evaluate generated text across several dimensions: faithfulness, instruction following, coherence, and completeness. These models not only provide ratings for these metrics but also offer detailed explanations and verifiable citations, thereby enhancing trust in the content. Moreover, the models support various citation modes, accommodating different requirements for latency and granularity. Extensive evaluations on diverse benchmarks demonstrate that our general-purpose LLM auto-evaluator, REC-70B, outperforms state-of-the-art LLMs, excelling in content evaluation by delivering better quality explanations and citations with minimal bias. It achieves Rank \\#1 as a generative model on the RewardBench leaderboard https://huggingface.co/spaces/allenai/reward-bench under the model name TextEval-Llama3.1-70B. Our REC dataset and models are released at https://github.com/adelaidehsu/REC.\n",
      "code": "var Component=(()=>{var f=Object.create;var r=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,p=Object.prototype.hasOwnProperty;var v=(e,a)=>()=>(a||e((a={exports:{}}).exports,a),a.exports),x=(e,a)=>{for(var t in a)r(e,t,{get:a[t],enumerable:!0})},s=(e,a,t,i)=>{if(a&&typeof a==\"object\"||typeof a==\"function\")for(let o of m(a))!p.call(e,o)&&o!==t&&r(e,o,{get:()=>a[o],enumerable:!(i=d(a,o))||i.enumerable});return e};var b=(e,a,t)=>(t=e!=null?f(g(e)):{},s(a||!e||!e.__esModule?r(t,\"default\",{value:e,enumerable:!0}):t,e)),y=e=>s(r({},\"__esModule\",{value:!0}),e);var c=v((S,l)=>{l.exports=_jsx_runtime});var C={};x(C,{default:()=>h,frontmatter:()=>A});var n=b(c()),A={title:\"Rate, Explain and Cite (REC): Enhanced Explanation and Attribution in Automatic Evaluation by Large Language Models\",date:\"2024-11-03\",tags:[\"instruction following\",\"evaluation metrics\",\"faithfulness\",\"text generation\",\"factual inaccuracies\",\"hallucinations\",\"citations\",\"Large Language Models\",\"coherence\",\"trust in content\",\"benchmark evaluations\",\"completeness\",\"auto-evaluators\"],categories:[\"cs.CL\",\"cs.AI\"],problem:\"rigorous evaluation of generated content quality\",solution:\"fine-tuned general-purpose LLM autoevaluators\",pdf_url:\"http://arxiv.org/pdf/2411.02448v1\",arx_url:\"http://arxiv.org/abs/2411.02448v1\",score:6,authors:[\"Aliyah R. Hsu\",\"James Zhu\",\"Zhichao Wang\",\"Bin Bi\",\"Shubham Mehrotra\",\"Shiva K. Pentyala\",\"Katherine Tan\",\"Xiang-Bo Mao\",\"Roshanak Omrani\",\"Sougata Chaudhuri\",\"Regunathan Radhakrishnan\",\"Sitaram Asur\",\"Claire Na Cheng\",\"Bin Yu\"],affiliations_aligned:[\"UC Berkeley\",\"Salesforce AI Platform\",\"Salesforce AI Platform\",\"Salesforce AI Platform\",\"Salesforce AI Platform\",\"Salesforce AI Platform\",\"Salesforce AI Platform\",\"Salesforce AI Platform\",\"Salesforce AI Platform\",\"Salesforce AI Platform\",\"Salesforce AI Platform\",\"Salesforce AI Platform\",\"Salesforce AI Platform\",\"UC Berkeley\"],affiliations:[\"UC Berkeley\",\"Salesforce AI Platform\"]};function u(e){let a={a:\"a\",p:\"p\",...e.components};return(0,n.jsxs)(a.p,{children:[\"LLMs have demonstrated impressive proficiency in generating coherent and high-quality text, making them valuable across a range of text-generation tasks. However, rigorous evaluation of this generated content is crucial, as ensuring its quality remains a significant challenge due to persistent issues such as factual inaccuracies and hallucinations. This paper introduces two fine-tuned general-purpose LLM autoevaluators, REC-12B and REC-70B, specifically designed to evaluate generated text across several dimensions: faithfulness, instruction following, coherence, and completeness. These models not only provide ratings for these metrics but also offer detailed explanations and verifiable citations, thereby enhancing trust in the content. Moreover, the models support various citation modes, accommodating different requirements for latency and granularity. Extensive evaluations on diverse benchmarks demonstrate that our general-purpose LLM auto-evaluator, REC-70B, outperforms state-of-the-art LLMs, excelling in content evaluation by delivering better quality explanations and citations with minimal bias. It achieves Rank #1 as a generative model on the RewardBench leaderboard \",(0,n.jsx)(a.a,{href:\"https://huggingface.co/spaces/allenai/reward-bench\",children:\"https://huggingface.co/spaces/allenai/reward-bench\"}),\" under the model name TextEval-Llama3.1-70B. Our REC dataset and models are released at \",(0,n.jsx)(a.a,{href:\"https://github.com/adelaidehsu/REC\",children:\"https://github.com/adelaidehsu/REC\"}),\".\"]})}function h(e={}){let{wrapper:a}=e.components||{};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(u,{...e})}):u(e)}return y(C);})();\n;return Component;"
    },
    "_id": "articles/2411.02448.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.02448.mdx",
      "sourceFileName": "2411.02448.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.02448"
    },
    "type": "Blog",
    "readingTime": {
      "text": "1 min read",
      "minutes": 0.83,
      "time": 49800,
      "words": 166
    },
    "slug": "2411.02448",
    "path": "articles/2411.02448",
    "filePath": "articles/2411.02448.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "Rate, Explain and Cite (REC): Enhanced Explanation and Attribution in Automatic Evaluation by Large Language Models",
      "datePublished": "2024-11-03T00:00:00.000Z",
      "dateModified": "2024-11-03T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.02448"
    }
  },
  {
    "title": "MM-Embed: Universal Multimodal Retrieval with Multimodal LLMs",
    "date": "2024-11-04T00:00:00.000Z",
    "tags": [
      "zero-shot reranking",
      "hard negative mining",
      "text-image queries",
      "bi-encoder retriever",
      "modality bias",
      "information retrieval",
      "large language models",
      "multimodal retrieval"
    ],
    "categories": [
      "cs.CL",
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2411.02571v1",
    "arx_url": "http://arxiv.org/abs/2411.02571v1",
    "authors": [
      "Sheng-Chieh Lin",
      "Chankyu Lee",
      "Mohammad Shoeybi",
      "Jimmy Lin",
      "Bryan Catanzaro",
      "Wei Ping"
    ],
    "affiliations_aligned": [
      "NVIDIA",
      "NVIDIA",
      "NVIDIA",
      "University of Waterloo",
      "NVIDIA",
      "NVIDIA"
    ],
    "affiliations": [
      "University of Waterloo",
      "NVIDIA"
    ],
    "problem": "modality bias in multimodal retrieval",
    "solution": "modality-aware hard negative mining",
    "score": 9,
    "body": {
      "raw": "\n\nState-of-the-art retrieval models typically address a straightforward search scenario, where retrieval tasks are fixed (e.g., finding a passage to answer a specific question) and only a single modality is supported for both queries and retrieved results. This paper introduces techniques for advancing information retrieval with multimodal large language models (MLLMs), enabling a broader search scenario, termed universal multimodal retrieval, where multiple modalities and diverse retrieval tasks are accommodated. To this end, we first study fine-tuning an MLLM as a bi-encoder retriever on 10 datasets with 16 retrieval tasks. Our empirical results show that the fine-tuned MLLM retriever is capable of understanding challenging queries, composed of both text and image, but underperforms a smaller CLIP retriever in cross-modal retrieval tasks due to modality bias from MLLMs. To address the issue, we propose modality-aware hard negative mining to mitigate the modality bias exhibited by MLLM retrievers. Second, we propose to continually fine-tune the universal multimodal retriever to enhance its text retrieval capability while maintaining multimodal retrieval capability. As a result, our model, MM-Embed, achieves state-of-the-art performance on the multimodal retrieval benchmark M-BEIR, which spans multiple domains and tasks, while also surpassing the state-of-the-art text retrieval model, NV-Embed-v1, on MTEB retrieval benchmark. Finally, we explore to prompt the off-the-shelf MLLMs as the zero-shot rerankers to refine the ranking of the candidates from the multimodal retriever. We find that through prompt-and-reranking, MLLMs can further improve multimodal retrieval when the user queries (e.g., text-image composed queries) are more complex and challenging to understand. These findings also pave the way to advance universal multimodal retrieval in the future.",
      "code": "var Component=(()=>{var u=Object.create;var i=Object.defineProperty;var c=Object.getOwnPropertyDescriptor;var v=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var p=(e,t)=>()=>(t||e((t={exports:{}}).exports,t),t.exports),M=(e,t)=>{for(var a in t)i(e,a,{get:t[a],enumerable:!0})},s=(e,t,a,n)=>{if(t&&typeof t==\"object\"||typeof t==\"function\")for(let r of v(t))!f.call(e,r)&&r!==a&&i(e,r,{get:()=>t[r],enumerable:!(n=c(t,r))||n.enumerable});return e};var b=(e,t,a)=>(a=e!=null?u(g(e)):{},s(t||!e||!e.__esModule?i(a,\"default\",{value:e,enumerable:!0}):a,e)),y=e=>s(i({},\"__esModule\",{value:!0}),e);var d=p((I,l)=>{l.exports=_jsx_runtime});var w={};M(w,{default:()=>h,frontmatter:()=>L});var o=b(d()),L={title:\"MM-Embed: Universal Multimodal Retrieval with Multimodal LLMs\",date:\"2024-11-04\",tags:[\"zero-shot reranking\",\"hard negative mining\",\"text-image queries\",\"bi-encoder retriever\",\"modality bias\",\"information retrieval\",\"large language models\",\"multimodal retrieval\"],categories:[\"cs.CL\",\"cs.CV\",\"cs.AI\",\"cs.LG\",\"cs.IR\"],problem:\"modality bias in multimodal retrieval\",solution:\"modality-aware hard negative mining\",pdf_url:\"http://arxiv.org/pdf/2411.02571v1\",arx_url:\"http://arxiv.org/abs/2411.02571v1\",score:9,authors:[\"Sheng-Chieh Lin\",\"Chankyu Lee\",\"Mohammad Shoeybi\",\"Jimmy Lin\",\"Bryan Catanzaro\",\"Wei Ping\"],affiliations_aligned:[\"NVIDIA\",\"NVIDIA\",\"NVIDIA\",\"University of Waterloo\",\"NVIDIA\",\"NVIDIA\"],affiliations:[\"University of Waterloo\",\"NVIDIA\"]};function m(e){let t={p:\"p\",...e.components};return(0,o.jsx)(t.p,{children:\"State-of-the-art retrieval models typically address a straightforward search scenario, where retrieval tasks are fixed (e.g., finding a passage to answer a specific question) and only a single modality is supported for both queries and retrieved results. This paper introduces techniques for advancing information retrieval with multimodal large language models (MLLMs), enabling a broader search scenario, termed universal multimodal retrieval, where multiple modalities and diverse retrieval tasks are accommodated. To this end, we first study fine-tuning an MLLM as a bi-encoder retriever on 10 datasets with 16 retrieval tasks. Our empirical results show that the fine-tuned MLLM retriever is capable of understanding challenging queries, composed of both text and image, but underperforms a smaller CLIP retriever in cross-modal retrieval tasks due to modality bias from MLLMs. To address the issue, we propose modality-aware hard negative mining to mitigate the modality bias exhibited by MLLM retrievers. Second, we propose to continually fine-tune the universal multimodal retriever to enhance its text retrieval capability while maintaining multimodal retrieval capability. As a result, our model, MM-Embed, achieves state-of-the-art performance on the multimodal retrieval benchmark M-BEIR, which spans multiple domains and tasks, while also surpassing the state-of-the-art text retrieval model, NV-Embed-v1, on MTEB retrieval benchmark. Finally, we explore to prompt the off-the-shelf MLLMs as the zero-shot rerankers to refine the ranking of the candidates from the multimodal retriever. We find that through prompt-and-reranking, MLLMs can further improve multimodal retrieval when the user queries (e.g., text-image composed queries) are more complex and challenging to understand. These findings also pave the way to advance universal multimodal retrieval in the future.\"})}function h(e={}){let{wrapper:t}=e.components||{};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(m,{...e})}):m(e)}return y(w);})();\n;return Component;"
    },
    "_id": "articles/2411.02571.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.02571.mdx",
      "sourceFileName": "2411.02571.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.02571"
    },
    "type": "Blog",
    "readingTime": {
      "text": "2 min read",
      "minutes": 1.31,
      "time": 78600,
      "words": 262
    },
    "slug": "2411.02571",
    "path": "articles/2411.02571",
    "filePath": "articles/2411.02571.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "MM-Embed: Universal Multimodal Retrieval with Multimodal LLMs",
      "datePublished": "2024-11-04T00:00:00.000Z",
      "dateModified": "2024-11-04T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.02571"
    }
  },
  {
    "title": "RT-Affordance: Affordances are Versatile Intermediate Representations for Robot Manipulation",
    "date": "2024-11-05T00:00:00.000Z",
    "tags": [
      "task language",
      "hierarchical model",
      "manipulation tasks",
      "intermediate policy representations",
      "affordance images",
      "affordances",
      "robot manipulation",
      "supervision sources",
      "knowledge transfer"
    ],
    "categories": [
      "cs.CL",
      "cs.RO",
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2411.02704v1",
    "arx_url": "http://arxiv.org/abs/2411.02704v1",
    "authors": [
      "Soroush Nasiriany",
      "Sean Kirmani",
      "Tianli Ding",
      "Laura Smith",
      "Yuke Zhu",
      "Danny Driess",
      "Dorsa Sadigh",
      "Ted Xiao"
    ],
    "affiliations_aligned": [
      "Google DeepMind, The University of Austin at Texas",
      "Google DeepMind, The University of Austin at Texas",
      "Google DeepMind, The University of Austin at Texas",
      "Google DeepMind, The University of Austin at Texas",
      "The University of Austin at Texas",
      "Google DeepMind, The University of Austin at Texas",
      "Google DeepMind, The University of Austin at Texas",
      "Google DeepMind, The University of Austin at Texas"
    ],
    "affiliations": [
      "Google DeepMind, The University of Austin at Texas",
      "The University of Austin at Texas"
    ],
    "problem": "insufficient context in existing representations for manipulation tasks",
    "solution": "RT-Affordance model conditioning policies on affordances",
    "score": 6,
    "body": {
      "raw": "\n\nWe explore how intermediate policy representations can facilitate generalization by providing guidance on how to perform manipulation tasks. Existing representations such as language, goal images, and trajectory sketches have been shown to be helpful, but these representations either do not provide enough context or provide over-specified context that yields less robust policies. We propose conditioning policies on affordances, which capture the pose of the robot at key stages of the task. Affordances offer expressive yet lightweight abstractions, are easy for users to specify, and facilitate efficient learning by transferring knowledge from large internet datasets. Our method, RT-Affordance, is a hierarchical model that first proposes an affordance plan given the task language, and then conditions the policy on this affordance plan to perform manipulation. Our model can flexibly bridge heterogeneous sources of supervision including large web datasets and robot trajectories. We additionally train our model on cheap-to-collect in-domain affordance images, allowing us to learn new tasks without collecting any additional costly robot trajectories. We show on a diverse set of novel tasks how RT-Affordance exceeds the performance of existing methods by over 50%, and we empirically demonstrate that affordances are robust to novel settings. Videos available at https://snasiriany.me/rt-affordance",
      "code": "var Component=(()=>{var p=Object.create;var i=Object.defineProperty;var h=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,m=Object.prototype.hasOwnProperty;var x=(e,t)=>()=>(t||e((t={exports:{}}).exports,t),t.exports),y=(e,t)=>{for(var o in t)i(e,o,{get:t[o],enumerable:!0})},r=(e,t,o,s)=>{if(t&&typeof t==\"object\"||typeof t==\"function\")for(let n of u(t))!m.call(e,n)&&n!==o&&i(e,n,{get:()=>t[n],enumerable:!(s=h(t,n))||s.enumerable});return e};var v=(e,t,o)=>(o=e!=null?p(g(e)):{},r(t||!e||!e.__esModule?i(o,\"default\",{value:e,enumerable:!0}):o,e)),T=e=>r(i({},\"__esModule\",{value:!0}),e);var l=x((D,f)=>{f.exports=_jsx_runtime});var A={};y(A,{default:()=>d,frontmatter:()=>b});var a=v(l()),b={title:\"RT-Affordance: Affordances are Versatile Intermediate Representations for Robot Manipulation\",date:\"2024-11-05\",tags:[\"task language\",\"hierarchical model\",\"manipulation tasks\",\"intermediate policy representations\",\"affordance images\",\"affordances\",\"robot manipulation\",\"supervision sources\",\"knowledge transfer\"],categories:[\"cs.CL\",\"cs.RO\",\"cs.CV\",\"cs.AI\",\"cs.LG\"],problem:\"insufficient context in existing representations for manipulation tasks\",solution:\"RT-Affordance model conditioning policies on affordances\",pdf_url:\"http://arxiv.org/pdf/2411.02704v1\",arx_url:\"http://arxiv.org/abs/2411.02704v1\",score:6,authors:[\"Soroush Nasiriany\",\"Sean Kirmani\",\"Tianli Ding\",\"Laura Smith\",\"Yuke Zhu\",\"Danny Driess\",\"Dorsa Sadigh\",\"Ted Xiao\"],affiliations_aligned:[\"Google DeepMind, The University of Austin at Texas\",\"Google DeepMind, The University of Austin at Texas\",\"Google DeepMind, The University of Austin at Texas\",\"Google DeepMind, The University of Austin at Texas\",\"The University of Austin at Texas\",\"Google DeepMind, The University of Austin at Texas\",\"Google DeepMind, The University of Austin at Texas\",\"Google DeepMind, The University of Austin at Texas\"],affiliations:[\"Google DeepMind, The University of Austin at Texas\",\"The University of Austin at Texas\"]};function c(e){let t={a:\"a\",p:\"p\",...e.components};return(0,a.jsxs)(t.p,{children:[\"We explore how intermediate policy representations can facilitate generalization by providing guidance on how to perform manipulation tasks. Existing representations such as language, goal images, and trajectory sketches have been shown to be helpful, but these representations either do not provide enough context or provide over-specified context that yields less robust policies. We propose conditioning policies on affordances, which capture the pose of the robot at key stages of the task. Affordances offer expressive yet lightweight abstractions, are easy for users to specify, and facilitate efficient learning by transferring knowledge from large internet datasets. Our method, RT-Affordance, is a hierarchical model that first proposes an affordance plan given the task language, and then conditions the policy on this affordance plan to perform manipulation. Our model can flexibly bridge heterogeneous sources of supervision including large web datasets and robot trajectories. We additionally train our model on cheap-to-collect in-domain affordance images, allowing us to learn new tasks without collecting any additional costly robot trajectories. We show on a diverse set of novel tasks how RT-Affordance exceeds the performance of existing methods by over 50%, and we empirically demonstrate that affordances are robust to novel settings. Videos available at \",(0,a.jsx)(t.a,{href:\"https://snasiriany.me/rt-affordance\",children:\"https://snasiriany.me/rt-affordance\"})]})}function d(e={}){let{wrapper:t}=e.components||{};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}return T(A);})();\n;return Component;"
    },
    "_id": "articles/2411.02704.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.02704.mdx",
      "sourceFileName": "2411.02704.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.02704"
    },
    "type": "Blog",
    "readingTime": {
      "text": "1 min read",
      "minutes": 0.985,
      "time": 59100,
      "words": 197
    },
    "slug": "2411.02704",
    "path": "articles/2411.02704",
    "filePath": "articles/2411.02704.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "RT-Affordance: Affordances are Versatile Intermediate Representations for Robot Manipulation",
      "datePublished": "2024-11-05T00:00:00.000Z",
      "dateModified": "2024-11-05T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.02704"
    }
  },
  {
    "title": "Multimodal Commonsense Knowledge Distillation for Visual Question Answering",
    "date": "2024-11-05T00:00:00.000Z",
    "tags": [
      "Visual Question Answering",
      "commonsense knowledge",
      "ScienceQA dataset",
      "Multimodal Large Language Models",
      "teacher-student environment",
      "Graph Convolutional Network",
      "Visual Language Pretrained Models",
      "knowledge distillation"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2411.02722v1",
    "arx_url": "http://arxiv.org/abs/2411.02722v1",
    "authors": [
      "Shuo Yang",
      "Siwen Luo",
      "Soyeon Caren Han"
    ],
    "affiliations_aligned": [
      "",
      "",
      ""
    ],
    "affiliations": [
      ""
    ],
    "problem": "VQA questions requiring external commonsense knowledge",
    "solution": "graph-based multimodal commonsense knowledge distillation framework",
    "score": 6,
    "body": {
      "raw": "\n\nExisting Multimodal Large Language Models (MLLMs) and Visual Language Pretrained Models (VLPMs) have shown remarkable performances in the general Visual Question Answering (VQA). However, these models struggle with VQA questions that require external commonsense knowledge due to the challenges in generating high-quality prompts and the high computational costs of fine-tuning. In this work, we propose a novel graph-based multimodal commonsense knowledge distillation framework that constructs a unified relational graph over commonsense knowledge, visual objects and questions through a Graph Convolutional Network (GCN) following a teacher-student environment. This proposed framework is flexible with any type of teacher and student models without further fine-tuning, and has achieved competitive performances on the ScienceQA dataset.",
      "code": "var Component=(()=>{var m=Object.create;var a=Object.defineProperty;var c=Object.getOwnPropertyDescriptor;var h=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,w=Object.prototype.hasOwnProperty;var f=(e,n)=>()=>(n||e((n={exports:{}}).exports,n),n.exports),v=(e,n)=>{for(var o in n)a(e,o,{get:n[o],enumerable:!0})},r=(e,n,o,s)=>{if(n&&typeof n==\"object\"||typeof n==\"function\")for(let t of h(n))!w.call(e,t)&&t!==o&&a(e,t,{get:()=>n[t],enumerable:!(s=c(n,t))||s.enumerable});return e};var k=(e,n,o)=>(o=e!=null?m(p(e)):{},r(n||!e||!e.__esModule?a(o,\"default\",{value:e,enumerable:!0}):o,e)),x=e=>r(a({},\"__esModule\",{value:!0}),e);var u=f((V,l)=>{l.exports=_jsx_runtime});var L={};v(L,{default:()=>g,frontmatter:()=>M});var i=k(u()),M={title:\"Multimodal Commonsense Knowledge Distillation for Visual Question Answering\",date:\"2024-11-05\",tags:[\"Visual Question Answering\",\"commonsense knowledge\",\"ScienceQA dataset\",\"Multimodal Large Language Models\",\"teacher-student environment\",\"Graph Convolutional Network\",\"Visual Language Pretrained Models\",\"knowledge distillation\"],categories:[\"cs.CL\",\"cs.AI\"],problem:\"VQA questions requiring external commonsense knowledge\",solution:\"graph-based multimodal commonsense knowledge distillation framework\",pdf_url:\"http://arxiv.org/pdf/2411.02722v1\",arx_url:\"http://arxiv.org/abs/2411.02722v1\",score:6,authors:[\"Shuo Yang\",\"Siwen Luo\",\"Soyeon Caren Han\"],affiliations_aligned:[\"\",\"\",\"\"],affiliations:[\"\"]};function d(e){let n={p:\"p\",...e.components};return(0,i.jsx)(n.p,{children:\"Existing Multimodal Large Language Models (MLLMs) and Visual Language Pretrained Models (VLPMs) have shown remarkable performances in the general Visual Question Answering (VQA). However, these models struggle with VQA questions that require external commonsense knowledge due to the challenges in generating high-quality prompts and the high computational costs of fine-tuning. In this work, we propose a novel graph-based multimodal commonsense knowledge distillation framework that constructs a unified relational graph over commonsense knowledge, visual objects and questions through a Graph Convolutional Network (GCN) following a teacher-student environment. This proposed framework is flexible with any type of teacher and student models without further fine-tuning, and has achieved competitive performances on the ScienceQA dataset.\"})}function g(e={}){let{wrapper:n}=e.components||{};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}return x(L);})();\n;return Component;"
    },
    "_id": "articles/2411.02722.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.02722.mdx",
      "sourceFileName": "2411.02722.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.02722"
    },
    "type": "Blog",
    "readingTime": {
      "text": "1 min read",
      "minutes": 0.555,
      "time": 33300,
      "words": 111
    },
    "slug": "2411.02722",
    "path": "articles/2411.02722",
    "filePath": "articles/2411.02722.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "Multimodal Commonsense Knowledge Distillation for Visual Question Answering",
      "datePublished": "2024-11-05T00:00:00.000Z",
      "dateModified": "2024-11-05T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.02722"
    }
  },
  {
    "title": "Leveraging Large Language Models in Code Question Answering: Baselines and Issues",
    "date": "2024-11-05T00:00:00.000Z",
    "tags": [
      "error analysis",
      "Python",
      "BERTScore F1",
      "large language models",
      "question answering",
      "BLEU-4",
      "dataset preprocessing",
      "Exact Match",
      "source code",
      "grammar correction",
      "BLEURT"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2411.03012v1",
    "arx_url": "http://arxiv.org/abs/2411.03012v1",
    "authors": [
      "Georgy Andryushchenko",
      "Vladimir Ivanov",
      "Vladimir Makharev",
      "Elizaveta Tukhtina",
      "Aidar Valeev"
    ],
    "affiliations_aligned": [
      "Institute of Applied Physics, Russian Academy of Sciences",
      "Moscow Institute of Physics and Technology",
      "Kazan Federal University",
      "National Research University Higher School of Economics",
      "Kazan Federal University"
    ],
    "affiliations": [
      "National Research University Higher School of Economics",
      "Institute of Applied Physics, Russian Academy of Sciences",
      "Moscow Institute of Physics and Technology",
      "Kazan Federal University"
    ],
    "problem": "poor quality of public genuine question-answering datasets",
    "solution": "fine-tuning a large language model on a unified dataset of questions and answers for Python code",
    "score": 7,
    "body": {
      "raw": "\n\nQuestion answering over source code provides software engineers and project managers with helpful information about the implemented features of a software product. This paper presents a work devoted to using large language models for question answering over source code in Python. The proposed method for implementing a source code question answering system involves fine-tuning a large language model on a unified dataset of questions and answers for Python code. To achieve the highest quality answers, we tested various models trained on datasets preprocessed in different ways: a dataset without grammar correction, a dataset with grammar correction, and a dataset augmented with the generated summaries. The model answers were also analyzed for errors manually. We report BLEU-4, BERTScore F1, BLEURT, and Exact Match metric values, along with the conclusions from the manual error analysis. The obtained experimental results highlight the current problems of the research area, such as poor quality of the public genuine question-answering datasets. In addition, the findings include the positive effect of the grammar correction of the training data on the testing metric values. The addressed findings and issues could be important for other researchers who attempt to improve the quality of source code question answering solutions. The training and evaluation code is publicly available at https://github.com/IU-AES-AI4Code/CodeQuestionAnswering.",
      "code": "var Component=(()=>{var h=Object.create;var s=Object.defineProperty;var g=Object.getOwnPropertyDescriptor;var f=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,p=Object.prototype.hasOwnProperty;var y=(e,a)=>()=>(a||e((a={exports:{}}).exports,a),a.exports),v=(e,a)=>{for(var t in a)s(e,t,{get:a[t],enumerable:!0})},r=(e,a,t,i)=>{if(a&&typeof a==\"object\"||typeof a==\"function\")for(let o of f(a))!p.call(e,o)&&o!==t&&s(e,o,{get:()=>a[o],enumerable:!(i=g(a,o))||i.enumerable});return e};var w=(e,a,t)=>(t=e!=null?h(m(e)):{},r(a||!e||!e.__esModule?s(t,\"default\",{value:e,enumerable:!0}):t,e)),x=e=>r(s({},\"__esModule\",{value:!0}),e);var d=y((b,c)=>{c.exports=_jsx_runtime});var T={};v(T,{default:()=>l,frontmatter:()=>A});var n=w(d()),A={title:\"Leveraging Large Language Models in Code Question Answering: Baselines and Issues\",date:\"2024-11-05\",tags:[\"error analysis\",\"Python\",\"BERTScore F1\",\"large language models\",\"question answering\",\"BLEU-4\",\"dataset preprocessing\",\"Exact Match\",\"source code\",\"grammar correction\",\"BLEURT\"],categories:[\"cs.CL\",\"cs.AI\"],problem:\"poor quality of public genuine question-answering datasets\",solution:\"fine-tuning a large language model on a unified dataset of questions and answers for Python code\",pdf_url:\"http://arxiv.org/pdf/2411.03012v1\",arx_url:\"http://arxiv.org/abs/2411.03012v1\",score:7,authors:[\"Georgy Andryushchenko\",\"Vladimir Ivanov\",\"Vladimir Makharev\",\"Elizaveta Tukhtina\",\"Aidar Valeev\"],affiliations_aligned:[\"Institute of Applied Physics, Russian Academy of Sciences\",\"Moscow Institute of Physics and Technology\",\"Kazan Federal University\",\"National Research University Higher School of Economics\",\"Kazan Federal University\"],affiliations:[\"National Research University Higher School of Economics\",\"Institute of Applied Physics, Russian Academy of Sciences\",\"Moscow Institute of Physics and Technology\",\"Kazan Federal University\"]};function u(e){let a={a:\"a\",p:\"p\",...e.components};return(0,n.jsxs)(a.p,{children:[\"Question answering over source code provides software engineers and project managers with helpful information about the implemented features of a software product. This paper presents a work devoted to using large language models for question answering over source code in Python. The proposed method for implementing a source code question answering system involves fine-tuning a large language model on a unified dataset of questions and answers for Python code. To achieve the highest quality answers, we tested various models trained on datasets preprocessed in different ways: a dataset without grammar correction, a dataset with grammar correction, and a dataset augmented with the generated summaries. The model answers were also analyzed for errors manually. We report BLEU-4, BERTScore F1, BLEURT, and Exact Match metric values, along with the conclusions from the manual error analysis. The obtained experimental results highlight the current problems of the research area, such as poor quality of the public genuine question-answering datasets. In addition, the findings include the positive effect of the grammar correction of the training data on the testing metric values. The addressed findings and issues could be important for other researchers who attempt to improve the quality of source code question answering solutions. The training and evaluation code is publicly available at \",(0,n.jsx)(a.a,{href:\"https://github.com/IU-AES-AI4Code/CodeQuestionAnswering\",children:\"https://github.com/IU-AES-AI4Code/CodeQuestionAnswering\"}),\".\"]})}function l(e={}){let{wrapper:a}=e.components||{};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(u,{...e})}):u(e)}return x(T);})();\n;return Component;"
    },
    "_id": "articles/2411.03012.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.03012.mdx",
      "sourceFileName": "2411.03012.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.03012"
    },
    "type": "Blog",
    "readingTime": {
      "text": "2 min read",
      "minutes": 1.045,
      "time": 62700,
      "words": 209
    },
    "slug": "2411.03012",
    "path": "articles/2411.03012",
    "filePath": "articles/2411.03012.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "Leveraging Large Language Models in Code Question Answering: Baselines and Issues",
      "datePublished": "2024-11-05T00:00:00.000Z",
      "dateModified": "2024-11-05T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.03012"
    }
  },
  {
    "title": "MME-Finance: A Multimodal Finance Benchmark for Expert-level Understanding and Reasoning",
    "date": "2024-11-05T00:00:00.000Z",
    "tags": [
      "financial evaluation system",
      "candlestick charts",
      "technical indicator charts",
      "machine learning language models",
      "financial models",
      "Visual Question Answering",
      "multimodal benchmarks"
    ],
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2411.03314v1",
    "arx_url": "http://arxiv.org/abs/2411.03314v1",
    "authors": [
      "Ziliang Gan",
      "Yu Lu",
      "Dong Zhang",
      "Haohan Li",
      "Che Liu",
      "Jian Liu",
      "Ji Liu",
      "Haipang Wu",
      "Chaoyou Fu",
      "Zenglin Xu",
      "Rongjunchen Zhang",
      "Yong Dai"
    ],
    "affiliations_aligned": [
      "HiThink Research",
      "HiThink Research",
      "HiThink Research",
      "HiThink Research",
      "Imperial College London",
      "Beihang",
      "HiThink Research",
      "HiThink Research",
      "Nanjing",
      "Fudan University",
      "HiThink Research",
      "HiThink Research"
    ],
    "affiliations": [
      "Imperial College London",
      "HiThink Research",
      "Beihang",
      "Nanjing",
      "Fudan University"
    ],
    "problem": "inadequate performance measurement of multimodal models in the financial domain",
    "solution": "MME-Finance benchmark for multimodal finance evaluation",
    "score": 6,
    "body": {
      "raw": "\n\nIn recent years, multimodal benchmarks for general domains have guided the rapid development of multimodal models on general tasks. However, the financial field has its peculiarities. It features unique graphical images (e.g., candlestick charts, technical indicator charts) and possesses a wealth of specialized financial knowledge (e.g., futures, turnover rate). Therefore, benchmarks from general fields often fail to measure the performance of multimodal models in the financial domain, and thus cannot effectively guide the rapid development of large financial models. To promote the development of large financial multimodal models, we propose MME-Finance, an bilingual open-ended and practical usage-oriented Visual Question Answering (VQA) benchmark. The characteristics of our benchmark are finance and expertise, which include constructing charts that reflect the actual usage needs of users (e.g., computer screenshots and mobile photography), creating questions according to the preferences in financial domain inquiries, and annotating questions by experts with 10+ years of experience in the financial industry. Additionally, we have developed a custom-designed financial evaluation system in which visual information is first introduced in the multi-modal evaluation process. Extensive experimental evaluations of 19 mainstream MLLMs are conducted to test their perception, reasoning, and cognition capabilities. The results indicate that models performing well on general benchmarks cannot do well on MME-Finance; for instance, the top-performing open-source and closed-source models obtain 65.69 (Qwen2VL-72B) and 63.18 (GPT-4o), respectively. Their performance is particularly poor in categories most relevant to finance, such as candlestick charts and technical indicator charts. In addition, we propose a Chinese version, which helps compare performance of MLLMs under a Chinese context.",
      "code": "var Component=(()=>{var u=Object.create;var t=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var f=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var v=(e,n)=>()=>(n||e((n={exports:{}}).exports,n),n.exports),k=(e,n)=>{for(var a in n)t(e,a,{get:n[a],enumerable:!0})},s=(e,n,a,r)=>{if(n&&typeof n==\"object\"||typeof n==\"function\")for(let i of f(n))!g.call(e,i)&&i!==a&&t(e,i,{get:()=>n[i],enumerable:!(r=m(n,i))||r.enumerable});return e};var x=(e,n,a)=>(a=e!=null?u(p(e)):{},s(n||!e||!e.__esModule?t(a,\"default\",{value:e,enumerable:!0}):a,e)),w=e=>s(t({},\"__esModule\",{value:!0}),e);var l=v((b,c)=>{c.exports=_jsx_runtime});var y={};k(y,{default:()=>d,frontmatter:()=>M});var o=x(l()),M={title:\"MME-Finance: A Multimodal Finance Benchmark for Expert-level Understanding and Reasoning\",date:\"2024-11-05\",tags:[\"financial evaluation system\",\"candlestick charts\",\"technical indicator charts\",\"machine learning language models\",\"financial models\",\"Visual Question Answering\",\"multimodal benchmarks\"],categories:[\"cs.CL\",\"cs.CV\"],problem:\"inadequate performance measurement of multimodal models in the financial domain\",solution:\"MME-Finance benchmark for multimodal finance evaluation\",pdf_url:\"http://arxiv.org/pdf/2411.03314v1\",arx_url:\"http://arxiv.org/abs/2411.03314v1\",score:6,authors:[\"Ziliang Gan\",\"Yu Lu\",\"Dong Zhang\",\"Haohan Li\",\"Che Liu\",\"Jian Liu\",\"Ji Liu\",\"Haipang Wu\",\"Chaoyou Fu\",\"Zenglin Xu\",\"Rongjunchen Zhang\",\"Yong Dai\"],affiliations_aligned:[\"HiThink Research\",\"HiThink Research\",\"HiThink Research\",\"HiThink Research\",\"Imperial College London\",\"Beihang\",\"HiThink Research\",\"HiThink Research\",\"Nanjing\",\"Fudan University\",\"HiThink Research\",\"HiThink Research\"],affiliations:[\"Imperial College London\",\"HiThink Research\",\"Beihang\",\"Nanjing\",\"Fudan University\"]};function h(e){let n={p:\"p\",...e.components};return(0,o.jsx)(n.p,{children:\"In recent years, multimodal benchmarks for general domains have guided the rapid development of multimodal models on general tasks. However, the financial field has its peculiarities. It features unique graphical images (e.g., candlestick charts, technical indicator charts) and possesses a wealth of specialized financial knowledge (e.g., futures, turnover rate). Therefore, benchmarks from general fields often fail to measure the performance of multimodal models in the financial domain, and thus cannot effectively guide the rapid development of large financial models. To promote the development of large financial multimodal models, we propose MME-Finance, an bilingual open-ended and practical usage-oriented Visual Question Answering (VQA) benchmark. The characteristics of our benchmark are finance and expertise, which include constructing charts that reflect the actual usage needs of users (e.g., computer screenshots and mobile photography), creating questions according to the preferences in financial domain inquiries, and annotating questions by experts with 10+ years of experience in the financial industry. Additionally, we have developed a custom-designed financial evaluation system in which visual information is first introduced in the multi-modal evaluation process. Extensive experimental evaluations of 19 mainstream MLLMs are conducted to test their perception, reasoning, and cognition capabilities. The results indicate that models performing well on general benchmarks cannot do well on MME-Finance; for instance, the top-performing open-source and closed-source models obtain 65.69 (Qwen2VL-72B) and 63.18 (GPT-4o), respectively. Their performance is particularly poor in categories most relevant to finance, such as candlestick charts and technical indicator charts. In addition, we propose a Chinese version, which helps compare performance of MLLMs under a Chinese context.\"})}function d(e={}){let{wrapper:n}=e.components||{};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}return w(y);})();\n;return Component;"
    },
    "_id": "articles/2411.03314.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.03314.mdx",
      "sourceFileName": "2411.03314.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.03314"
    },
    "type": "Blog",
    "readingTime": {
      "text": "2 min read",
      "minutes": 1.285,
      "time": 77100,
      "words": 257
    },
    "slug": "2411.03314",
    "path": "articles/2411.03314",
    "filePath": "articles/2411.03314.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "MME-Finance: A Multimodal Finance Benchmark for Expert-level Understanding and Reasoning",
      "datePublished": "2024-11-05T00:00:00.000Z",
      "dateModified": "2024-11-05T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.03314"
    }
  },
  {
    "title": "QUILL: Quotation Generation Enhancement of Large Language Models",
    "date": "2024-11-06T00:00:00.000Z",
    "tags": [
      "bilingual knowledge base",
      "human preferences",
      "Large language models",
      "reranking metric",
      "automatic metrics",
      "evaluation system",
      "quotation generation"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2411.03675v1",
    "arx_url": "http://arxiv.org/abs/2411.03675v1",
    "authors": [
      "Jin Xiao",
      "Bowei Zhang",
      "Qianyu He",
      "Jiaqing Liang",
      "Feng Wei",
      "Jinglei Chen",
      "Zujie Liang",
      "Deqing Yang",
      "Yanghua Xiao"
    ],
    "affiliations_aligned": [
      "School of Data Science, Fudan University",
      "School of Data Science, Fudan University",
      "Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University",
      "School of Data Science, Fudan University",
      "MYbank, Ant Group",
      "MYbank, Ant Group",
      "MYbank, Ant Group",
      "School of Data Science, Fudan University",
      "Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University"
    ],
    "affiliations": [
      "School of Data Science, Fudan University",
      "MYbank, Ant Group",
      "Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University"
    ],
    "problem": "struggles with quotation generation",
    "solution": "quotation knowledge base and reranking metric",
    "score": 6,
    "body": {
      "raw": "\n\nWhile Large language models (LLMs) have become excellent writing assistants, they still struggle with quotation generation. This is because they either hallucinate when providing factual quotations or fail to provide quotes that exceed human expectations. To bridge the gap, we systematically study how to evaluate and improve LLMs' performance in quotation generation tasks. We first establish a holistic and automatic evaluation system for quotation generation task, which consists of five criteria each with corresponding automatic metric. To improve the LLMs' quotation generation abilities, we construct a bilingual knowledge base that is broad in scope and rich in dimensions, containing up to 32,022 quotes. Moreover, guided by our critiria, we further design a quotation-specific metric to rerank the retrieved quotations from the knowledge base. Extensive experiments show that our metrics strongly correlate with human preferences. Existing LLMs struggle to generate desired quotes, but our quotation knowledge base and reranking metric help narrow this gap. Our dataset and code are publicly available at https://github.com/GraceXiaoo/QUILL.",
      "code": "var Component=(()=>{var l=Object.create;var o=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var b=(e,t)=>()=>(t||e((t={exports:{}}).exports,t),t.exports),y=(e,t)=>{for(var a in t)o(e,a,{get:t[a],enumerable:!0})},s=(e,t,a,r)=>{if(t&&typeof t==\"object\"||typeof t==\"function\")for(let n of m(t))!f.call(e,n)&&n!==a&&o(e,n,{get:()=>t[n],enumerable:!(r=d(t,n))||r.enumerable});return e};var v=(e,t,a)=>(a=e!=null?l(p(e)):{},s(t||!e||!e.__esModule?o(a,\"default\",{value:e,enumerable:!0}):a,e)),L=e=>s(o({},\"__esModule\",{value:!0}),e);var u=b((q,c)=>{c.exports=_jsx_runtime});var w={};y(w,{default:()=>h,frontmatter:()=>S});var i=v(u()),S={title:\"QUILL: Quotation Generation Enhancement of Large Language Models\",date:\"2024-11-06\",tags:[\"bilingual knowledge base\",\"human preferences\",\"Large language models\",\"reranking metric\",\"automatic metrics\",\"evaluation system\",\"quotation generation\"],categories:[\"cs.CL\",\"cs.AI\"],problem:\"struggles with quotation generation\",solution:\"quotation knowledge base and reranking metric\",pdf_url:\"http://arxiv.org/pdf/2411.03675v1\",arx_url:\"http://arxiv.org/abs/2411.03675v1\",score:6,authors:[\"Jin Xiao\",\"Bowei Zhang\",\"Qianyu He\",\"Jiaqing Liang\",\"Feng Wei\",\"Jinglei Chen\",\"Zujie Liang\",\"Deqing Yang\",\"Yanghua Xiao\"],affiliations_aligned:[\"School of Data Science, Fudan University\",\"School of Data Science, Fudan University\",\"Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\",\"School of Data Science, Fudan University\",\"MYbank, Ant Group\",\"MYbank, Ant Group\",\"MYbank, Ant Group\",\"School of Data Science, Fudan University\",\"Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\"],affiliations:[\"School of Data Science, Fudan University\",\"MYbank, Ant Group\",\"Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\"]};function g(e){let t={a:\"a\",p:\"p\",...e.components};return(0,i.jsxs)(t.p,{children:[\"While Large language models (LLMs) have become excellent writing assistants, they still struggle with quotation generation. This is because they either hallucinate when providing factual quotations or fail to provide quotes that exceed human expectations. To bridge the gap, we systematically study how to evaluate and improve LLMs' performance in quotation generation tasks. We first establish a holistic and automatic evaluation system for quotation generation task, which consists of five criteria each with corresponding automatic metric. To improve the LLMs' quotation generation abilities, we construct a bilingual knowledge base that is broad in scope and rich in dimensions, containing up to 32,022 quotes. Moreover, guided by our critiria, we further design a quotation-specific metric to rerank the retrieved quotations from the knowledge base. Extensive experiments show that our metrics strongly correlate with human preferences. Existing LLMs struggle to generate desired quotes, but our quotation knowledge base and reranking metric help narrow this gap. Our dataset and code are publicly available at \",(0,i.jsx)(t.a,{href:\"https://github.com/GraceXiaoo/QUILL\",children:\"https://github.com/GraceXiaoo/QUILL\"}),\".\"]})}function h(e={}){let{wrapper:t}=e.components||{};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(g,{...e})}):g(e)}return L(w);})();\n;return Component;"
    },
    "_id": "articles/2411.03675.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.03675.mdx",
      "sourceFileName": "2411.03675.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.03675"
    },
    "type": "Blog",
    "readingTime": {
      "text": "1 min read",
      "minutes": 0.81,
      "time": 48600,
      "words": 162
    },
    "slug": "2411.03675",
    "path": "articles/2411.03675",
    "filePath": "articles/2411.03675.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "QUILL: Quotation Generation Enhancement of Large Language Models",
      "datePublished": "2024-11-06T00:00:00.000Z",
      "dateModified": "2024-11-06T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.03675"
    }
  },
  {
    "title": "3DGS-CD: 3D Gaussian Splatting-based Change Detection for Physical Object Rearrangement",
    "date": "2024-11-06T00:00:00.000Z",
    "tags": [
      "change detection",
      "object reconstruction",
      "3DGS model update",
      "robot workspace reset",
      "3D scenes",
      "physical object rearrangement",
      "3D Gaussian Splatting",
      "zero-shot segmentation",
      "real-world datasets",
      "3D changes",
      "cluttered environments",
      "2D object-level changes"
    ],
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2411.03706v1",
    "arx_url": "http://arxiv.org/abs/2411.03706v1",
    "authors": [
      "Ziqi Lu",
      "Jianbo Ye",
      "John Leonard"
    ],
    "affiliations_aligned": [
      "Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA 02139, USA",
      "Amazon, New York, NY 10018, USA",
      "Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA 02139, USA"
    ],
    "affiliations": [
      "Amazon, New York, NY 10018, USA",
      "Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA 02139, USA"
    ],
    "problem": "detecting physical object rearrangements in 3D scenes",
    "solution": "3DGS-based method for detecting changes using unaligned images",
    "score": 6,
    "body": {
      "raw": "\n\nWe present 3DGS-CD, the first 3D Gaussian Splatting (3DGS)-based method for detecting physical object rearrangements in 3D scenes. Our approach estimates 3D object-level changes by comparing two sets of unaligned images taken at different times. Leveraging 3DGS's novel view rendering and EfficientSAM's zero-shot segmentation capabilities, we detect 2D object-level changes, which are then associated and fused across views to estimate 3D changes. Our method can detect changes in cluttered environments using sparse post-change images within as little as 18s, using as few as a single new image. It does not rely on depth input, user instructions, object classes, or object models -- An object is recognized simply if it has been re-arranged. Our approach is evaluated on both public and self-collected real-world datasets, achieving up to 14% higher accuracy and three orders of magnitude faster performance compared to the state-of-the-art radiance-field-based change detection method. This significant performance boost enables a broad range of downstream applications, where we highlight three key use cases: object reconstruction, robot workspace reset, and 3DGS model update. Our code and data will be made available at https://github.com/520xyxyzq/3DGS-CD.",
      "code": "var Component=(()=>{var h=Object.create;var o=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,b=Object.prototype.hasOwnProperty;var f=(e,t)=>()=>(t||e((t={exports:{}}).exports,t),t.exports),D=(e,t)=>{for(var a in t)o(e,a,{get:t[a],enumerable:!0})},r=(e,t,a,i)=>{if(t&&typeof t==\"object\"||typeof t==\"function\")for(let s of m(t))!b.call(e,s)&&s!==a&&o(e,s,{get:()=>t[s],enumerable:!(i=u(t,s))||i.enumerable});return e};var S=(e,t,a)=>(a=e!=null?h(p(e)):{},r(t||!e||!e.__esModule?o(a,\"default\",{value:e,enumerable:!0}):a,e)),y=e=>r(o({},\"__esModule\",{value:!0}),e);var d=f((x,c)=>{c.exports=_jsx_runtime});var j={};D(j,{default:()=>g,frontmatter:()=>w});var n=S(d()),w={title:\"3DGS-CD: 3D Gaussian Splatting-based Change Detection for Physical Object Rearrangement\",date:\"2024-11-06\",tags:[\"change detection\",\"object reconstruction\",\"3DGS model update\",\"robot workspace reset\",\"3D scenes\",\"physical object rearrangement\",\"3D Gaussian Splatting\",\"zero-shot segmentation\",\"real-world datasets\",\"3D changes\",\"cluttered environments\",\"2D object-level changes\"],categories:[\"cs.CV\",\"cs.RO\"],problem:\"detecting physical object rearrangements in 3D scenes\",solution:\"3DGS-based method for detecting changes using unaligned images\",pdf_url:\"http://arxiv.org/pdf/2411.03706v1\",arx_url:\"http://arxiv.org/abs/2411.03706v1\",score:6,authors:[\"Ziqi Lu\",\"Jianbo Ye\",\"John Leonard\"],affiliations_aligned:[\"Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA 02139, USA\",\"Amazon, New York, NY 10018, USA\",\"Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA 02139, USA\"],affiliations:[\"Amazon, New York, NY 10018, USA\",\"Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA 02139, USA\"]};function l(e){let t={a:\"a\",p:\"p\",...e.components};return(0,n.jsxs)(t.p,{children:[\"We present 3DGS-CD, the first 3D Gaussian Splatting (3DGS)-based method for detecting physical object rearrangements in 3D scenes. Our approach estimates 3D object-level changes by comparing two sets of unaligned images taken at different times. Leveraging 3DGS's novel view rendering and EfficientSAM's zero-shot segmentation capabilities, we detect 2D object-level changes, which are then associated and fused across views to estimate 3D changes. Our method can detect changes in cluttered environments using sparse post-change images within as little as 18s, using as few as a single new image. It does not rely on depth input, user instructions, object classes, or object models -- An object is recognized simply if it has been re-arranged. Our approach is evaluated on both public and self-collected real-world datasets, achieving up to 14% higher accuracy and three orders of magnitude faster performance compared to the state-of-the-art radiance-field-based change detection method. This significant performance boost enables a broad range of downstream applications, where we highlight three key use cases: object reconstruction, robot workspace reset, and 3DGS model update. Our code and data will be made available at \",(0,n.jsx)(t.a,{href:\"https://github.com/520xyxyzq/3DGS-CD\",children:\"https://github.com/520xyxyzq/3DGS-CD\"}),\".\"]})}function g(e={}){let{wrapper:t}=e.components||{};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(l,{...e})}):l(e)}return y(j);})();\n;return Component;"
    },
    "_id": "articles/2411.03706.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.03706.mdx",
      "sourceFileName": "2411.03706.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.03706"
    },
    "type": "Blog",
    "readingTime": {
      "text": "1 min read",
      "minutes": 0.905,
      "time": 54300,
      "words": 181
    },
    "slug": "2411.03706",
    "path": "articles/2411.03706",
    "filePath": "articles/2411.03706.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "3DGS-CD: 3D Gaussian Splatting-based Change Detection for Physical Object Rearrangement",
      "datePublished": "2024-11-06T00:00:00.000Z",
      "dateModified": "2024-11-06T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.03706"
    }
  },
  {
    "title": "AmazonQAC: A Large-Scale, Naturalistic Query Autocomplete Dataset",
    "date": "2024-11-08T00:00:00.000Z",
    "tags": [
      "search engines",
      "finetuning",
      "Query Autocomplete",
      "user-typed prefixes",
      "Prefix Trees",
      "semantic retrieval",
      "Large Language Models",
      "user needs",
      "context-dependent modeling",
      "AmazonQAC dataset",
      "large-scale datasets"
    ],
    "categories": [
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2411.04129",
    "arx_url": "https://arxiv.org/abs/2411.04129",
    "authors": [
      "Dante Everaert",
      "Rohit Patki",
      "Tianqi Zheng",
      "Christopher Potts"
    ],
    "affiliations_aligned": [
      "Amazon Search",
      "Amazon Search",
      "Amazon Search",
      "Stanford University"
    ],
    "affiliations": [
      "Amazon Search",
      "Stanford University"
    ],
    "problem": "absence of large-scale realistic datasets for QAC",
    "solution": "introduction of AmazonQAC dataset",
    "score": 5,
    "body": {
      "raw": "\nQuery Autocomplete (QAC) is a critical feature in modern search engines, facilitating user interaction by predicting search queries based on input prefixes. Despite its widespread adoption, the absence of large-scale, realistic datasets has hindered advancements in QAC system development. This paper addresses this gap by introducing AmazonQAC, a new QAC dataset sourced from Amazon Search logs, comprising 395M samples. The dataset includes actual sequences of user-typed prefixes leading to final search terms, as well as session IDs and timestamps that support modeling the context-dependent aspects of QAC. We assess Prefix Trees, semantic retrieval, and Large Language Models (LLMs) with and without finetuning. We find that finetuned LLMs perform best, particularly when incorporating contextual information. However, even our best system achieves only half of what we calculate is theoretically possible on our test data, which implies QAC is a challenging problem that is far from solved with existing systems. This contribution aims to stimulate further research on QAC systems to better serve user needs in diverse environments. We open-source this data on Hugging Face at https://huggingface.co/datasets/amazon/AmazonQAC.\n",
      "code": "var Component=(()=>{var h=Object.create;var i=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var f=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var A=(e,t)=>()=>(t||e((t={exports:{}}).exports,t),t.exports),x=(e,t)=>{for(var a in t)i(e,a,{get:t[a],enumerable:!0})},o=(e,t,a,r)=>{if(t&&typeof t==\"object\"||typeof t==\"function\")for(let n of p(t))!g.call(e,n)&&n!==a&&i(e,n,{get:()=>t[n],enumerable:!(r=m(t,n))||r.enumerable});return e};var y=(e,t,a)=>(a=e!=null?h(f(e)):{},o(t||!e||!e.__esModule?i(a,\"default\",{value:e,enumerable:!0}):a,e)),v=e=>o(i({},\"__esModule\",{value:!0}),e);var d=A((z,c)=>{c.exports=_jsx_runtime});var Q={};x(Q,{default:()=>u,frontmatter:()=>C});var s=y(d()),C={title:\"AmazonQAC: A Large-Scale, Naturalistic Query Autocomplete Dataset\",date:\"2024-11-08\",tags:[\"search engines\",\"finetuning\",\"Query Autocomplete\",\"user-typed prefixes\",\"Prefix Trees\",\"semantic retrieval\",\"Large Language Models\",\"user needs\",\"context-dependent modeling\",\"AmazonQAC dataset\",\"large-scale datasets\"],categories:[\"cs.AI\",\"cs.IR\",\"cs.LG\"],problem:\"absence of large-scale realistic datasets for QAC\",solution:\"introduction of AmazonQAC dataset\",pdf_url:\"https://arxiv.org/pdf/2411.04129\",arx_url:\"https://arxiv.org/abs/2411.04129\",score:5,authors:[\"Dante Everaert\",\"Rohit Patki\",\"Tianqi Zheng\",\"Christopher Potts\"],affiliations_aligned:[\"Amazon Search\",\"Amazon Search\",\"Amazon Search\",\"Stanford University\"],affiliations:[\"Amazon Search\",\"Stanford University\"]};function l(e){let t={a:\"a\",p:\"p\",...e.components};return(0,s.jsxs)(t.p,{children:[\"Query Autocomplete (QAC) is a critical feature in modern search engines, facilitating user interaction by predicting search queries based on input prefixes. Despite its widespread adoption, the absence of large-scale, realistic datasets has hindered advancements in QAC system development. This paper addresses this gap by introducing AmazonQAC, a new QAC dataset sourced from Amazon Search logs, comprising 395M samples. The dataset includes actual sequences of user-typed prefixes leading to final search terms, as well as session IDs and timestamps that support modeling the context-dependent aspects of QAC. We assess Prefix Trees, semantic retrieval, and Large Language Models (LLMs) with and without finetuning. We find that finetuned LLMs perform best, particularly when incorporating contextual information. However, even our best system achieves only half of what we calculate is theoretically possible on our test data, which implies QAC is a challenging problem that is far from solved with existing systems. This contribution aims to stimulate further research on QAC systems to better serve user needs in diverse environments. We open-source this data on Hugging Face at \",(0,s.jsx)(t.a,{href:\"https://huggingface.co/datasets/amazon/AmazonQAC\",children:\"https://huggingface.co/datasets/amazon/AmazonQAC\"}),\".\"]})}function u(e={}){let{wrapper:t}=e.components||{};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}return v(Q);})();\n;return Component;"
    },
    "_id": "articles/2411.04129.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.04129.mdx",
      "sourceFileName": "2411.04129.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.04129"
    },
    "type": "Blog",
    "readingTime": {
      "text": "1 min read",
      "minutes": 0.875,
      "time": 52500,
      "words": 175
    },
    "slug": "2411.04129",
    "path": "articles/2411.04129",
    "filePath": "articles/2411.04129.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "AmazonQAC: A Large-Scale, Naturalistic Query Autocomplete Dataset",
      "datePublished": "2024-11-08T00:00:00.000Z",
      "dateModified": "2024-11-08T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.04129"
    }
  },
  {
    "title": "Language Models are Hidden Reasoners: Unlocking Latent Reasoning Capabilities via Self-Rewarding",
    "date": "2024-11-08T00:00:00.000Z",
    "tags": [
      "Chain-of-Thought",
      "variational approaches",
      "prompt-based methods",
      "complex reasoning tasks",
      "zero-shot accuracy",
      "LaTent Reasoning Optimization",
      "large language models",
      "latent reasoning capabilities"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2411.04282",
    "arx_url": "https://arxiv.org/abs/2411.04282",
    "authors": [
      "Haolin Chen",
      "Yihao Feng",
      "Zuxin Liu",
      "Weiran Yao",
      "Akshara Prabhakar",
      "Shelby Heinecke",
      "Ricky Ho",
      "Phil Mui",
      "Silvio Savarese",
      "Caiming Xiong",
      "Huan Wang"
    ],
    "affiliations_aligned": [
      "Salesforce AI Research",
      "Salesforce AI Research",
      "Salesforce AI Research",
      "Salesforce AI Research",
      "Salesforce AI Research",
      "Salesforce AI Research",
      "Salesforce AI Research",
      "Salesforce AI Research",
      "Salesforce AI Research",
      "Salesforce AI Research",
      "Salesforce AI Research"
    ],
    "affiliations": [
      "Salesforce AI Research"
    ],
    "problem": "complex reasoning tasks requiring multiple steps",
    "solution": "LaTent Reasoning Optimization framework",
    "score": 6,
    "body": {
      "raw": "\nLarge language models (LLMs) have shown impressive capabilities, but still struggle with complex reasoning tasks requiring multiple steps. While prompt-based methods like Chain-of-Thought (CoT) can improve LLM reasoning at inference time, optimizing reasoning capabilities during training remains challenging. We introduce LaTent Reasoning Optimization (LaTRO), a principled framework that formulates reasoning as sampling from a latent distribution and optimizes it via variational approaches. LaTRO enables LLMs to concurrently improve both their reasoning process and ability to evaluate reasoning quality, without requiring external feedback or reward models. We validate LaTRO through experiments on GSM8K and ARC-Challenge datasets using multiple model architectures. On GSM8K, LaTRO improves zero-shot accuracy by an average of 12.5% over base models and 9.6% over supervised fine-tuning across Phi-3.5-mini, Mistral-7B, and Llama-3.1-8B. Our findings suggest that pre-trained LLMs possess latent reasoning capabilities that can be unlocked and enhanced through our proposed optimization approach in a self-improvement manner. The code of LaTRO is available at https://github.com/SalesforceAIResearch/LaTRO.\n",
      "code": "var Component=(()=>{var p=Object.create;var r=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var d=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var R=(e,a)=>()=>(a||e((a={exports:{}}).exports,a),a.exports),L=(e,a)=>{for(var i in a)r(e,i,{get:a[i],enumerable:!0})},o=(e,a,i,t)=>{if(a&&typeof a==\"object\"||typeof a==\"function\")for(let n of m(a))!f.call(e,n)&&n!==i&&r(e,n,{get:()=>a[n],enumerable:!(t=u(a,n))||t.enumerable});return e};var b=(e,a,i)=>(i=e!=null?p(d(e)):{},o(a||!e||!e.__esModule?r(i,\"default\",{value:e,enumerable:!0}):i,e)),v=e=>o(r({},\"__esModule\",{value:!0}),e);var c=R((I,l)=>{l.exports=_jsx_runtime});var x={};L(x,{default:()=>g,frontmatter:()=>S});var s=b(c()),S={title:\"Language Models are Hidden Reasoners: Unlocking Latent Reasoning Capabilities via Self-Rewarding\",date:\"2024-11-08\",tags:[\"Chain-of-Thought\",\"variational approaches\",\"prompt-based methods\",\"complex reasoning tasks\",\"zero-shot accuracy\",\"LaTent Reasoning Optimization\",\"large language models\",\"latent reasoning capabilities\"],categories:[\"cs.AI\",\"cs.CL\",\"stat.ML\",\"cs.LG\"],problem:\"complex reasoning tasks requiring multiple steps\",solution:\"LaTent Reasoning Optimization framework\",pdf_url:\"https://arxiv.org/pdf/2411.04282\",arx_url:\"https://arxiv.org/abs/2411.04282\",score:6,authors:[\"Haolin Chen\",\"Yihao Feng\",\"Zuxin Liu\",\"Weiran Yao\",\"Akshara Prabhakar\",\"Shelby Heinecke\",\"Ricky Ho\",\"Phil Mui\",\"Silvio Savarese\",\"Caiming Xiong\",\"Huan Wang\"],affiliations_aligned:[\"Salesforce AI Research\",\"Salesforce AI Research\",\"Salesforce AI Research\",\"Salesforce AI Research\",\"Salesforce AI Research\",\"Salesforce AI Research\",\"Salesforce AI Research\",\"Salesforce AI Research\",\"Salesforce AI Research\",\"Salesforce AI Research\",\"Salesforce AI Research\"],affiliations:[\"Salesforce AI Research\"]};function h(e){let a={a:\"a\",p:\"p\",...e.components};return(0,s.jsxs)(a.p,{children:[\"Large language models (LLMs) have shown impressive capabilities, but still struggle with complex reasoning tasks requiring multiple steps. While prompt-based methods like Chain-of-Thought (CoT) can improve LLM reasoning at inference time, optimizing reasoning capabilities during training remains challenging. We introduce LaTent Reasoning Optimization (LaTRO), a principled framework that formulates reasoning as sampling from a latent distribution and optimizes it via variational approaches. LaTRO enables LLMs to concurrently improve both their reasoning process and ability to evaluate reasoning quality, without requiring external feedback or reward models. We validate LaTRO through experiments on GSM8K and ARC-Challenge datasets using multiple model architectures. On GSM8K, LaTRO improves zero-shot accuracy by an average of 12.5% over base models and 9.6% over supervised fine-tuning across Phi-3.5-mini, Mistral-7B, and Llama-3.1-8B. Our findings suggest that pre-trained LLMs possess latent reasoning capabilities that can be unlocked and enhanced through our proposed optimization approach in a self-improvement manner. The code of LaTRO is available at \",(0,s.jsx)(a.a,{href:\"https://github.com/SalesforceAIResearch/LaTRO\",children:\"https://github.com/SalesforceAIResearch/LaTRO\"}),\".\"]})}function g(e={}){let{wrapper:a}=e.components||{};return a?(0,s.jsx)(a,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}return v(x);})();\n;return Component;"
    },
    "_id": "articles/2411.04282.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.04282.mdx",
      "sourceFileName": "2411.04282.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.04282"
    },
    "type": "Blog",
    "readingTime": {
      "text": "1 min read",
      "minutes": 0.78,
      "time": 46800,
      "words": 156
    },
    "slug": "2411.04282",
    "path": "articles/2411.04282",
    "filePath": "articles/2411.04282.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "Language Models are Hidden Reasoners: Unlocking Latent Reasoning Capabilities via Self-Rewarding",
      "datePublished": "2024-11-08T00:00:00.000Z",
      "dateModified": "2024-11-08T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.04282"
    }
  },
  {
    "title": "CodeTree: Agent-guided Tree Search for Code Generation with Large Language Models",
    "date": "2024-11-08T00:00:00.000Z",
    "tags": [
      "search space exploration",
      "multi-stage planning",
      "code generation",
      "execution-based feedback",
      "tree structure",
      "large language models",
      "benchmarks",
      "agent-guided approaches",
      "performance evaluation",
      "decision-making"
    ],
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2411.04329",
    "arx_url": "https://arxiv.org/abs/2411.04329",
    "authors": [
      "Jierui Li",
      "Hung Le",
      "Yinbo Zhou",
      "Caiming Xiong",
      "Silvio Savarese",
      "Doyen Sahoo"
    ],
    "affiliations_aligned": [
      "The University of Texas at Austin",
      "Salesforce Research",
      "Salesforce Research",
      "Salesforce Research",
      "Salesforce Research",
      "Salesforce Research"
    ],
    "affiliations": [
      "The University of Texas at Austin",
      "Salesforce Research"
    ],
    "problem": "challenges in multi-stage planning and debugging in code generation",
    "solution": "CodeTree framework for efficient search space exploration",
    "score": 6,
    "body": {
      "raw": "\n\nPre-trained on massive amounts of code and text data, large language models (LLMs) have demonstrated remarkable achievements in performing code generation tasks. With additional execution-based feedback, these models can act as agents with capabilities to self-refine and improve generated code autonomously. However, on challenging coding tasks with extremely large search space, current agentic approaches still struggle with multi-stage planning, generating, and debugging. To address this problem, we propose CodeTree, a framework for LLM agents to efficiently explore the search space in different stages of the code generation process. Specifically, we adopted a unified tree structure to explicitly explore different coding strategies, generate corresponding coding solutions, and subsequently refine the solutions. In each stage, critical decision-making (ranking, termination, expanding) of the exploration process is guided by both the environmental execution-based feedback and LLM-agent-generated feedback. We comprehensively evaluated CodeTree on 7 code generation benchmarks and demonstrated the significant performance gains of CodeTree against strong baselines. Using GPT-4o as the base model, we consistently achieved top results of 95.1 on HumanEval, 98.7 on MBPP, and 43.0 on CodeContests. On the challenging SWEBench benchmark, our approach led to significant performance gains.",
      "code": "var Component=(()=>{var f=Object.create;var o=Object.defineProperty;var h=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,m=Object.prototype.hasOwnProperty;var x=(e,a)=>()=>(a||e((a={exports:{}}).exports,a),a.exports),b=(e,a)=>{for(var n in a)o(e,n,{get:a[n],enumerable:!0})},s=(e,a,n,r)=>{if(a&&typeof a==\"object\"||typeof a==\"function\")for(let t of u(a))!m.call(e,t)&&t!==n&&o(e,t,{get:()=>a[t],enumerable:!(r=h(a,t))||r.enumerable});return e};var v=(e,a,n)=>(n=e!=null?f(p(e)):{},s(a||!e||!e.__esModule?o(n,\"default\",{value:e,enumerable:!0}):n,e)),k=e=>s(o({},\"__esModule\",{value:!0}),e);var d=x((S,c)=>{c.exports=_jsx_runtime});var C={};b(C,{default:()=>l,frontmatter:()=>y});var i=v(d()),y={title:\"CodeTree: Agent-guided Tree Search for Code Generation with Large Language Models\",date:\"2024-11-08\",tags:[\"search space exploration\",\"multi-stage planning\",\"code generation\",\"execution-based feedback\",\"tree structure\",\"large language models\",\"benchmarks\",\"agent-guided approaches\",\"performance evaluation\",\"decision-making\"],categories:[\"cs.CL\"],problem:\"challenges in multi-stage planning and debugging in code generation\",solution:\"CodeTree framework for efficient search space exploration\",pdf_url:\"https://arxiv.org/pdf/2411.04329\",arx_url:\"https://arxiv.org/abs/2411.04329\",score:6,authors:[\"Jierui Li\",\"Hung Le\",\"Yinbo Zhou\",\"Caiming Xiong\",\"Silvio Savarese\",\"Doyen Sahoo\"],affiliations_aligned:[\"The University of Texas at Austin\",\"Salesforce Research\",\"Salesforce Research\",\"Salesforce Research\",\"Salesforce Research\",\"Salesforce Research\"],affiliations:[\"The University of Texas at Austin\",\"Salesforce Research\"]};function g(e){let a={p:\"p\",...e.components};return(0,i.jsx)(a.p,{children:\"Pre-trained on massive amounts of code and text data, large language models (LLMs) have demonstrated remarkable achievements in performing code generation tasks. With additional execution-based feedback, these models can act as agents with capabilities to self-refine and improve generated code autonomously. However, on challenging coding tasks with extremely large search space, current agentic approaches still struggle with multi-stage planning, generating, and debugging. To address this problem, we propose CodeTree, a framework for LLM agents to efficiently explore the search space in different stages of the code generation process. Specifically, we adopted a unified tree structure to explicitly explore different coding strategies, generate corresponding coding solutions, and subsequently refine the solutions. In each stage, critical decision-making (ranking, termination, expanding) of the exploration process is guided by both the environmental execution-based feedback and LLM-agent-generated feedback. We comprehensively evaluated CodeTree on 7 code generation benchmarks and demonstrated the significant performance gains of CodeTree against strong baselines. Using GPT-4o as the base model, we consistently achieved top results of 95.1 on HumanEval, 98.7 on MBPP, and 43.0 on CodeContests. On the challenging SWEBench benchmark, our approach led to significant performance gains.\"})}function l(e={}){let{wrapper:a}=e.components||{};return a?(0,i.jsx)(a,{...e,children:(0,i.jsx)(g,{...e})}):g(e)}return k(C);})();\n;return Component;"
    },
    "_id": "articles/2411.04329.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.04329.mdx",
      "sourceFileName": "2411.04329.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.04329"
    },
    "type": "Blog",
    "readingTime": {
      "text": "1 min read",
      "minutes": 0.935,
      "time": 56100,
      "words": 187
    },
    "slug": "2411.04329",
    "path": "articles/2411.04329",
    "filePath": "articles/2411.04329.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "CodeTree: Agent-guided Tree Search for Code Generation with Large Language Models",
      "datePublished": "2024-11-08T00:00:00.000Z",
      "dateModified": "2024-11-08T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.04329"
    }
  },
  {
    "title": "Measuring short-form factuality in large language models",
    "date": "2024-11-08T00:00:00.000Z",
    "tags": [
      "fact-seeking questions",
      "language models",
      "evaluation",
      "adversarial collection",
      "benchmark",
      "grading responses",
      "model behavior"
    ],
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2411.04368",
    "arx_url": "https://arxiv.org/abs/2411.04368",
    "authors": [
      "Jason Wei",
      "Nguyen Karina",
      "Hyung Won Chung",
      "Yunxin Joy Jiao",
      "Spencer Papay",
      "Amelia Glaese",
      "John Schulman",
      "William Fedus"
    ],
    "affiliations_aligned": [
      "OpenAI",
      "OpenAI",
      "OpenAI",
      "OpenAI",
      "OpenAI",
      "OpenAI",
      "OpenAI",
      "OpenAI"
    ],
    "affiliations": [
      "OpenAI"
    ],
    "problem": "evaluating the ability of language models to answer short, fact-seeking questions",
    "solution": "SimpleQA benchmark",
    "score": 6,
    "body": {
      "raw": "\n\nWe present SimpleQA, a benchmark that evaluates the ability of language models to answer short, fact-seeking questions. We prioritized two properties in designing this eval. First, SimpleQA is challenging, as it is adversarially collected against GPT-4 responses. Second, responses are easy to grade, because questions are created such that there exists only a single, indisputable answer. Each answer in SimpleQA is graded as either correct, incorrect, or not attempted. A model with ideal behavior would get as many questions correct as possible while not attempting the questions for which it is not confident it knows the correct answer. SimpleQA is a simple, targeted evaluation for whether models \"know what they know,\" and our hope is that this benchmark will remain relevant for the next few generations of frontier models. SimpleQA can be found at https://github.com/openai/simple-evals.",
      "code": "var Component=(()=>{var u=Object.create;var i=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var g=Object.getOwnPropertyNames;var d=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var w=(e,t)=>()=>(t||e((t={exports:{}}).exports,t),t.exports),A=(e,t)=>{for(var a in t)i(e,a,{get:t[a],enumerable:!0})},r=(e,t,a,o)=>{if(t&&typeof t==\"object\"||typeof t==\"function\")for(let s of g(t))!f.call(e,s)&&s!==a&&i(e,s,{get:()=>t[s],enumerable:!(o=m(t,s))||o.enumerable});return e};var b=(e,t,a)=>(a=e!=null?u(d(e)):{},r(t||!e||!e.__esModule?i(a,\"default\",{value:e,enumerable:!0}):a,e)),x=e=>r(i({},\"__esModule\",{value:!0}),e);var c=w((I,l)=>{l.exports=_jsx_runtime});var y={};A(y,{default:()=>p,frontmatter:()=>v});var n=b(c()),v={title:\"Measuring short-form factuality in large language models\",date:\"2024-11-08\",tags:[\"fact-seeking questions\",\"language models\",\"evaluation\",\"adversarial collection\",\"benchmark\",\"grading responses\",\"model behavior\"],categories:[\"cs.CL\"],problem:\"evaluating the ability of language models to answer short, fact-seeking questions\",solution:\"SimpleQA benchmark\",pdf_url:\"https://arxiv.org/pdf/2411.04368\",arx_url:\"https://arxiv.org/abs/2411.04368\",score:6,authors:[\"Jason Wei\",\"Nguyen Karina\",\"Hyung Won Chung\",\"Yunxin Joy Jiao\",\"Spencer Papay\",\"Amelia Glaese\",\"John Schulman\",\"William Fedus\"],affiliations_aligned:[\"OpenAI\",\"OpenAI\",\"OpenAI\",\"OpenAI\",\"OpenAI\",\"OpenAI\",\"OpenAI\",\"OpenAI\"],affiliations:[\"OpenAI\"]};function h(e){let t={a:\"a\",p:\"p\",...e.components};return(0,n.jsxs)(t.p,{children:['We present SimpleQA, a benchmark that evaluates the ability of language models to answer short, fact-seeking questions. We prioritized two properties in designing this eval. First, SimpleQA is challenging, as it is adversarially collected against GPT-4 responses. Second, responses are easy to grade, because questions are created such that there exists only a single, indisputable answer. Each answer in SimpleQA is graded as either correct, incorrect, or not attempted. A model with ideal behavior would get as many questions correct as possible while not attempting the questions for which it is not confident it knows the correct answer. SimpleQA is a simple, targeted evaluation for whether models \"know what they know,\" and our hope is that this benchmark will remain relevant for the next few generations of frontier models. SimpleQA can be found at ',(0,n.jsx)(t.a,{href:\"https://github.com/openai/simple-evals\",children:\"https://github.com/openai/simple-evals\"}),\".\"]})}function p(e={}){let{wrapper:t}=e.components||{};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(h,{...e})}):h(e)}return x(y);})();\n;return Component;"
    },
    "_id": "articles/2411.04368.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.04368.mdx",
      "sourceFileName": "2411.04368.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.04368"
    },
    "type": "Blog",
    "readingTime": {
      "text": "1 min read",
      "minutes": 0.675,
      "time": 40500,
      "words": 135
    },
    "slug": "2411.04368",
    "path": "articles/2411.04368",
    "filePath": "articles/2411.04368.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "Measuring short-form factuality in large language models",
      "datePublished": "2024-11-08T00:00:00.000Z",
      "dateModified": "2024-11-08T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.04368"
    }
  },
  {
    "title": "ML-Promise: A Multilingual Dataset for Corporate Promise Verification",
    "date": "2024-11-08T00:00:00.000Z",
    "tags": [
      "Environmental Social Governance (ESG)",
      "greenwashing",
      "multilingual dataset",
      "accountability of public commitments",
      "retrieval-augmented generation (RAG)",
      "corporate promises",
      "Promise Verification"
    ],
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2411.04473",
    "arx_url": "https://arxiv.org/abs/2411.04473",
    "authors": [
      "Yohei Seki",
      "Hakusen Shu",
      "Ana\\\\\"is Lhuissier",
      "Hanwool Lee",
      "Juyeon Kang",
      "Min-Yuh Day",
      "Chung-Chi Chen",
      "AnaÃ¯s Lhuissier"
    ],
    "affiliations_aligned": [
      "Institute of Library, Information, and Media Science, University of Tsukuba, Japan",
      "College of Knowledge and Library Sciences, School of Informatics, University of Tsukuba, Japan",
      "",
      "Shinhan Securities Co., Korea",
      "3DS Outscale, France",
      "Graduate Institute of Information Management, National Taipei University, Taiwan",
      "AIST, Japan",
      "3DS Outscale, France"
    ],
    "affiliations": [
      "",
      "AIST, Japan",
      "Shinhan Securities Co., Korea",
      "3DS Outscale, France",
      "College of Knowledge and Library Sciences, School of Informatics, University of Tsukuba, Japan",
      "Graduate Institute of Information Management, National Taipei University, Taiwan",
      "Institute of Library, Information, and Media Science, University of Tsukuba, Japan"
    ],
    "problem": "difficulty in verifying fulfillment of promises",
    "solution": "multilingual dataset for promise verification",
    "score": 6,
    "body": {
      "raw": "\n\nPromises made by politicians, corporate leaders, and public figures have a significant impact on public perception, trust, and institutional reputation. However, the complexity and volume of such commitments, coupled with difficulties in verifying their fulfillment, necessitate innovative methods for assessing their credibility. This paper introduces the concept of Promise Verification, a systematic approach involving steps such as promise identification, evidence assessment, and the evaluation of timing for verification. We propose the first multilingual dataset, ML-Promise, which includes English, French, Chinese, Japanese, and Korean, aimed at facilitating in-depth verification of promises, particularly in the context of Environmental, Social, and Governance (ESG) reports. Given the growing emphasis on corporate environmental contributions, this dataset addresses the challenge of evaluating corporate promises, especially in light of practices like greenwashing. Our findings also explore textual and image-based baselines, with promising results from retrieval-augmented generation (RAG) approaches. This work aims to foster further discourse on the accountability of public commitments across multiple languages and domains.",
      "code": "var Component=(()=>{var m=Object.create;var t=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var h=Object.getOwnPropertyNames;var d=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var v=(e,i)=>()=>(i||e((i={exports:{}}).exports,i),i.exports),y=(e,i)=>{for(var a in i)t(e,a,{get:i[a],enumerable:!0})},s=(e,i,a,r)=>{if(i&&typeof i==\"object\"||typeof i==\"function\")for(let n of h(i))!g.call(e,n)&&n!==a&&t(e,n,{get:()=>i[n],enumerable:!(r=p(i,n))||r.enumerable});return e};var b=(e,i,a)=>(a=e!=null?m(d(e)):{},s(i||!e||!e.__esModule?t(a,\"default\",{value:e,enumerable:!0}):a,e)),S=e=>s(t({},\"__esModule\",{value:!0}),e);var l=v((I,c)=>{c.exports=_jsx_runtime});var x={};y(x,{default:()=>f,frontmatter:()=>w});var o=b(l()),w={title:\"ML-Promise: A Multilingual Dataset for Corporate Promise Verification\",date:\"2024-11-08\",tags:[\"Environmental Social Governance (ESG)\",\"greenwashing\",\"multilingual dataset\",\"accountability of public commitments\",\"retrieval-augmented generation (RAG)\",\"corporate promises\",\"Promise Verification\"],categories:[\"cs.CL\"],problem:\"difficulty in verifying fulfillment of promises\",solution:\"multilingual dataset for promise verification\",pdf_url:\"https://arxiv.org/pdf/2411.04473\",arx_url:\"https://arxiv.org/abs/2411.04473\",score:6,authors:[\"Yohei Seki\",\"Hakusen Shu\",'Ana\\\\\\\\\"is Lhuissier',\"Hanwool Lee\",\"Juyeon Kang\",\"Min-Yuh Day\",\"Chung-Chi Chen\",\"Ana\\xEFs Lhuissier\"],affiliations_aligned:[\"Institute of Library, Information, and Media Science, University of Tsukuba, Japan\",\"College of Knowledge and Library Sciences, School of Informatics, University of Tsukuba, Japan\",\"\",\"Shinhan Securities Co., Korea\",\"3DS Outscale, France\",\"Graduate Institute of Information Management, National Taipei University, Taiwan\",\"AIST, Japan\",\"3DS Outscale, France\"],affiliations:[\"\",\"AIST, Japan\",\"Shinhan Securities Co., Korea\",\"3DS Outscale, France\",\"College of Knowledge and Library Sciences, School of Informatics, University of Tsukuba, Japan\",\"Graduate Institute of Information Management, National Taipei University, Taiwan\",\"Institute of Library, Information, and Media Science, University of Tsukuba, Japan\"]};function u(e){let i={p:\"p\",...e.components};return(0,o.jsx)(i.p,{children:\"Promises made by politicians, corporate leaders, and public figures have a significant impact on public perception, trust, and institutional reputation. However, the complexity and volume of such commitments, coupled with difficulties in verifying their fulfillment, necessitate innovative methods for assessing their credibility. This paper introduces the concept of Promise Verification, a systematic approach involving steps such as promise identification, evidence assessment, and the evaluation of timing for verification. We propose the first multilingual dataset, ML-Promise, which includes English, French, Chinese, Japanese, and Korean, aimed at facilitating in-depth verification of promises, particularly in the context of Environmental, Social, and Governance (ESG) reports. Given the growing emphasis on corporate environmental contributions, this dataset addresses the challenge of evaluating corporate promises, especially in light of practices like greenwashing. Our findings also explore textual and image-based baselines, with promising results from retrieval-augmented generation (RAG) approaches. This work aims to foster further discourse on the accountability of public commitments across multiple languages and domains.\"})}function f(e={}){let{wrapper:i}=e.components||{};return i?(0,o.jsx)(i,{...e,children:(0,o.jsx)(u,{...e})}):u(e)}return S(x);})();\n;return Component;"
    },
    "_id": "articles/2411.04473.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.04473.mdx",
      "sourceFileName": "2411.04473.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.04473"
    },
    "type": "Blog",
    "readingTime": {
      "text": "1 min read",
      "minutes": 0.795,
      "time": 47700,
      "words": 159
    },
    "slug": "2411.04473",
    "path": "articles/2411.04473",
    "filePath": "articles/2411.04473.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "ML-Promise: A Multilingual Dataset for Corporate Promise Verification",
      "datePublished": "2024-11-08T00:00:00.000Z",
      "dateModified": "2024-11-08T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.04473"
    }
  },
  {
    "title": "BhasaAnuvaad: A Speech Translation Dataset for 14 Indian Languages",
    "date": "2024-11-08T00:00:00.000Z",
    "tags": [
      "web mining",
      "Automatic Speech Translation",
      "low-resource languages",
      "AST systems",
      "performance evaluation",
      "data generation",
      "Indian languages",
      "spontaneous speech",
      "colloquial language",
      "BhasaAnuvaad dataset"
    ],
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2411.04699",
    "arx_url": "https://arxiv.org/abs/2411.04699",
    "authors": [
      "Sparsh Jain",
      "Ashwin Sankar",
      "Devilal Choudhary",
      "Dhairya Suman",
      "Nikhil Narasimhan",
      "Mohammed Safi Ur Rahman Khan",
      "Anoop Kunchukuttan",
      "Mitesh M Khapra",
      "Raj Dabre"
    ],
    "affiliations_aligned": [
      "Nilekani Centre at AI4Bharat",
      "Nilekani Centre at AI4Bharat",
      "Delhi Technological University",
      "Indian Institute of Technology, Delhi",
      "Nilekani Centre at AI4Bharat",
      "Nilekani Centre at AI4Bharat",
      "Microsoft",
      "Indian Institute of Technology, Madras",
      "Indian Institute of Technology, Madras"
    ],
    "affiliations": [
      "Nilekani Centre at AI4Bharat",
      "Delhi Technological University",
      "Indian Institute of Technology, Madras",
      "Microsoft",
      "Indian Institute of Technology, Delhi"
    ],
    "problem": "scarcity of Automatic Speech Translation datasets for Indian languages",
    "solution": "BhasaAnuvaad dataset for speech translation involving 14 Indian languages",
    "score": 6,
    "body": {
      "raw": "\n\nAutomatic Speech Translation (AST) datasets for Indian languages remain critically scarce, with public resources covering fewer than 10 of the 22 official languages. This scarcity has resulted in AST systems for Indian languages lagging far behind those available for high-resource languages like English. In this paper, we first evaluate the performance of widely-used AST systems on Indian languages, identifying notable performance gaps and challenges. Our findings show that while these systems perform adequately on read speech, they struggle significantly with spontaneous speech, including disfluencies like pauses and hesitations. Additionally, there is a striking absence of systems capable of accurately translating colloquial and informal language, a key aspect of everyday communication. To this end, we introduce BhasaAnuvaad, the largest publicly available dataset for AST involving 14 scheduled Indian languages spanning over 44,400 hours and 17M text segments. BhasaAnuvaad contains data for English speech to Indic text, as well as Indic speech to English text. This dataset comprises three key categories: (1) Curated datasets from existing resources, (2) Large-scale web mining, and (3) Synthetic data generation. By offering this diverse and expansive dataset, we aim to bridge the resource gap and promote advancements in AST for low-resource Indian languages, especially in handling spontaneous and informal speech patterns.",
      "code": "var Component=(()=>{var g=Object.create;var i=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var f=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,m=Object.prototype.hasOwnProperty;var y=(a,e)=>()=>(e||a((e={exports:{}}).exports,e),e.exports),I=(a,e)=>{for(var n in e)i(a,n,{get:e[n],enumerable:!0})},r=(a,e,n,o)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let t of f(e))!m.call(a,t)&&t!==n&&i(a,t,{get:()=>e[t],enumerable:!(o=u(e,t))||o.enumerable});return a};var v=(a,e,n)=>(n=a!=null?g(p(a)):{},r(e||!a||!a.__esModule?i(n,\"default\",{value:a,enumerable:!0}):n,a)),A=a=>r(i({},\"__esModule\",{value:!0}),a);var c=y((S,l)=>{l.exports=_jsx_runtime});var w={};I(w,{default:()=>d,frontmatter:()=>T});var s=v(c()),T={title:\"BhasaAnuvaad: A Speech Translation Dataset for 14 Indian Languages\",date:\"2024-11-08\",tags:[\"web mining\",\"Automatic Speech Translation\",\"low-resource languages\",\"AST systems\",\"performance evaluation\",\"data generation\",\"Indian languages\",\"spontaneous speech\",\"colloquial language\",\"BhasaAnuvaad dataset\"],categories:[\"cs.CL\"],problem:\"scarcity of Automatic Speech Translation datasets for Indian languages\",solution:\"BhasaAnuvaad dataset for speech translation involving 14 Indian languages\",pdf_url:\"https://arxiv.org/pdf/2411.04699\",arx_url:\"https://arxiv.org/abs/2411.04699\",score:6,authors:[\"Sparsh Jain\",\"Ashwin Sankar\",\"Devilal Choudhary\",\"Dhairya Suman\",\"Nikhil Narasimhan\",\"Mohammed Safi Ur Rahman Khan\",\"Anoop Kunchukuttan\",\"Mitesh M Khapra\",\"Raj Dabre\"],affiliations_aligned:[\"Nilekani Centre at AI4Bharat\",\"Nilekani Centre at AI4Bharat\",\"Delhi Technological University\",\"Indian Institute of Technology, Delhi\",\"Nilekani Centre at AI4Bharat\",\"Nilekani Centre at AI4Bharat\",\"Microsoft\",\"Indian Institute of Technology, Madras\",\"Indian Institute of Technology, Madras\"],affiliations:[\"Nilekani Centre at AI4Bharat\",\"Delhi Technological University\",\"Indian Institute of Technology, Madras\",\"Microsoft\",\"Indian Institute of Technology, Delhi\"]};function h(a){let e={p:\"p\",...a.components};return(0,s.jsx)(e.p,{children:\"Automatic Speech Translation (AST) datasets for Indian languages remain critically scarce, with public resources covering fewer than 10 of the 22 official languages. This scarcity has resulted in AST systems for Indian languages lagging far behind those available for high-resource languages like English. In this paper, we first evaluate the performance of widely-used AST systems on Indian languages, identifying notable performance gaps and challenges. Our findings show that while these systems perform adequately on read speech, they struggle significantly with spontaneous speech, including disfluencies like pauses and hesitations. Additionally, there is a striking absence of systems capable of accurately translating colloquial and informal language, a key aspect of everyday communication. To this end, we introduce BhasaAnuvaad, the largest publicly available dataset for AST involving 14 scheduled Indian languages spanning over 44,400 hours and 17M text segments. BhasaAnuvaad contains data for English speech to Indic text, as well as Indic speech to English text. This dataset comprises three key categories: (1) Curated datasets from existing resources, (2) Large-scale web mining, and (3) Synthetic data generation. By offering this diverse and expansive dataset, we aim to bridge the resource gap and promote advancements in AST for low-resource Indian languages, especially in handling spontaneous and informal speech patterns.\"})}function d(a={}){let{wrapper:e}=a.components||{};return e?(0,s.jsx)(e,{...a,children:(0,s.jsx)(h,{...a})}):h(a)}return A(w);})();\n;return Component;"
    },
    "_id": "articles/2411.04699.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.04699.mdx",
      "sourceFileName": "2411.04699.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.04699"
    },
    "type": "Blog",
    "readingTime": {
      "text": "2 min read",
      "minutes": 1.025,
      "time": 61500,
      "words": 205
    },
    "slug": "2411.04699",
    "path": "articles/2411.04699",
    "filePath": "articles/2411.04699.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "BhasaAnuvaad: A Speech Translation Dataset for 14 Indian Languages",
      "datePublished": "2024-11-08T00:00:00.000Z",
      "dateModified": "2024-11-08T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.04699"
    }
  },
  {
    "title": "An Effective Pipeline for Whole-Slide Image Glomerulus Segmentation",
    "date": "2024-11-08T00:00:00.000Z",
    "tags": [
      "whole-slide images",
      "state-of-the-art methods",
      "detection coverage",
      "kidney diseases",
      "datasets",
      "segmentation models",
      "glomerulus segmentation"
    ],
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "pdf_url": "https://arxiv.org/pdf/2411.04782",
    "arx_url": "https://arxiv.org/abs/2411.04782",
    "authors": [
      "Quan Huu Cap"
    ],
    "affiliations_aligned": [
      ""
    ],
    "affiliations": [
      ""
    ],
    "problem": "glomerulus segmentation in whole-slide images",
    "solution": "practical pipeline for glomerulus segmentation",
    "score": 6,
    "body": {
      "raw": "\n\nWhole-slide images (WSI) glomerulus segmentation is essential for accurately diagnosing kidney diseases. In this work, we propose a practical pipeline for glomerulus segmentation that effectively enhances both patch-level and WSI-level segmentation tasks. Our approach leverages stitching on overlapping patches, increasing the detection coverage, especially when glomeruli are located near patch image borders. In addition, we conduct comprehensive evaluations from different segmentation models across two large and diverse datasets with over 30K glomerulus annotations. Experimental results demonstrate that models using our pipeline outperform the previous state-of-the-art method, achieving superior results across both datasets and setting a new benchmark for glomerulus segmentation in WSIs. The code and pre-trained models are available at https://github.com/huuquan1994/wsi_glomerulus_seg.",
      "code": "var Component=(()=>{var g=Object.create;var n=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var h=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var v=(e,t)=>()=>(t||e((t={exports:{}}).exports,t),t.exports),x=(e,t)=>{for(var s in t)n(e,s,{get:t[s],enumerable:!0})},r=(e,t,s,i)=>{if(t&&typeof t==\"object\"||typeof t==\"function\")for(let o of h(t))!f.call(e,o)&&o!==s&&n(e,o,{get:()=>t[o],enumerable:!(i=d(t,o))||i.enumerable});return e};var _=(e,t,s)=>(s=e!=null?g(p(e)):{},r(t||!e||!e.__esModule?n(s,\"default\",{value:e,enumerable:!0}):s,e)),w=e=>r(n({},\"__esModule\",{value:!0}),e);var u=v((j,l)=>{l.exports=_jsx_runtime});var I={};x(I,{default:()=>c,frontmatter:()=>b});var a=_(u()),b={title:\"An Effective Pipeline for Whole-Slide Image Glomerulus Segmentation\",date:\"2024-11-08\",tags:[\"whole-slide images\",\"state-of-the-art methods\",\"detection coverage\",\"kidney diseases\",\"datasets\",\"segmentation models\",\"glomerulus segmentation\"],categories:[\"cs.CV\",\"eess.IV\"],problem:\"glomerulus segmentation in whole-slide images\",solution:\"practical pipeline for glomerulus segmentation\",pdf_url:\"https://arxiv.org/pdf/2411.04782\",arx_url:\"https://arxiv.org/abs/2411.04782\",score:6,authors:[\"Quan Huu Cap\"],affiliations_aligned:[\"\"],affiliations:[\"\"]};function m(e){let t={a:\"a\",p:\"p\",...e.components};return(0,a.jsxs)(t.p,{children:[\"Whole-slide images (WSI) glomerulus segmentation is essential for accurately diagnosing kidney diseases. In this work, we propose a practical pipeline for glomerulus segmentation that effectively enhances both patch-level and WSI-level segmentation tasks. Our approach leverages stitching on overlapping patches, increasing the detection coverage, especially when glomeruli are located near patch image borders. In addition, we conduct comprehensive evaluations from different segmentation models across two large and diverse datasets with over 30K glomerulus annotations. Experimental results demonstrate that models using our pipeline outperform the previous state-of-the-art method, achieving superior results across both datasets and setting a new benchmark for glomerulus segmentation in WSIs. The code and pre-trained models are available at \",(0,a.jsx)(t.a,{href:\"https://github.com/huuquan1994/wsi_glomerulus_seg\",children:\"https://github.com/huuquan1994/wsi_glomerulus_seg\"}),\".\"]})}function c(e={}){let{wrapper:t}=e.components||{};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(m,{...e})}):m(e)}return w(I);})();\n;return Component;"
    },
    "_id": "articles/2411.04782.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.04782.mdx",
      "sourceFileName": "2411.04782.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.04782"
    },
    "type": "Blog",
    "readingTime": {
      "text": "1 min read",
      "minutes": 0.555,
      "time": 33300,
      "words": 111
    },
    "slug": "2411.04782",
    "path": "articles/2411.04782",
    "filePath": "articles/2411.04782.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "An Effective Pipeline for Whole-Slide Image Glomerulus Segmentation",
      "datePublished": "2024-11-08T00:00:00.000Z",
      "dateModified": "2024-11-08T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.04782"
    }
  },
  {
    "title": "M3DocRAG: Multi-modal Retrieval is What You Need for Multi-page Multi-document Understanding",
    "date": "2024-11-08T00:00:00.000Z",
    "tags": [
      "benchmark evaluation",
      "optical character recognition",
      "open-domain DocVQA",
      "multi-modal RAG framework",
      "indexing",
      "retrieval models",
      "Document visual question answering",
      "multi-modal language models",
      "text-based retrieval-augmented generation"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "pdf_url": "https://arxiv.org/pdf/2411.04952",
    "arx_url": "https://arxiv.org/abs/2411.04952",
    "authors": [
      "Jaemin Cho",
      "Debanjan Mahata",
      "Ozan Irsoy",
      "Yujie He",
      "Mohit Bansal",
      "Ozan Ä°rsoy"
    ],
    "affiliations_aligned": [
      "UNC Chapel Hill",
      "Bloomberg",
      "",
      "Bloomberg",
      "UNC Chapel Hill",
      "Bloomberg"
    ],
    "affiliations": [
      "",
      "UNC Chapel Hill",
      "Bloomberg"
    ],
    "problem": "difficulties in applying existing methods in real-world scenarios",
    "solution": "M3DocRAG multi-modal RAG framework",
    "score": 6,
    "body": {
      "raw": "\n\nDocument visual question answering (DocVQA) pipelines that answer questions from documents have broad applications. Existing methods focus on handling single-page documents with multi-modal language models (MLMs), or rely on text-based retrieval-augmented generation (RAG) that uses text extraction tools such as optical character recognition (OCR). However, there are difficulties in applying these methods in real-world scenarios: (a) questions often require information across different pages or documents, where MLMs cannot handle many long documents; (b) documents often have important information in visual elements such as figures, but text extraction tools ignore them. We introduce M3DocRAG, a novel multi-modal RAG framework that flexibly accommodates various document contexts (closed-domain and open-domain), question hops (single-hop and multi-hop), and evidence modalities (text, chart, figure, etc.). M3DocRAG finds relevant documents and answers questions using a multi-modal retriever and an MLM, so that it can efficiently handle single or many documents while preserving visual information. Since previous DocVQA datasets ask questions in the context of a specific document, we also present M3DocVQA, a new benchmark for evaluating open-domain DocVQA over 3,000+ PDF documents with 40,000+ pages. In three benchmarks (M3DocVQA/MMLongBench-Doc/MP-DocVQA), empirical results show that M3DocRAG with ColPali and Qwen2-VL 7B achieves superior performance than many strong baselines, including state-of-the-art performance in MP-DocVQA. We provide comprehensive analyses of different indexing, MLMs, and retrieval models. Lastly, we qualitatively show that M3DocRAG can successfully handle various scenarios, such as when relevant information exists across multiple pages and when answer evidence only exists in images.",
      "code": "var Component=(()=>{var d=Object.create;var a=Object.defineProperty;var h=Object.getOwnPropertyDescriptor;var g=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var v=(e,n)=>()=>(n||e((n={exports:{}}).exports,n),n.exports),M=(e,n)=>{for(var o in n)a(e,o,{get:n[o],enumerable:!0})},r=(e,n,o,s)=>{if(n&&typeof n==\"object\"||typeof n==\"function\")for(let t of g(n))!f.call(e,t)&&t!==o&&a(e,t,{get:()=>n[t],enumerable:!(s=h(n,t))||s.enumerable});return e};var x=(e,n,o)=>(o=e!=null?d(p(e)):{},r(n||!e||!e.__esModule?a(o,\"default\",{value:e,enumerable:!0}):o,e)),w=e=>r(a({},\"__esModule\",{value:!0}),e);var c=v((y,l)=>{l.exports=_jsx_runtime});var A={};M(A,{default:()=>u,frontmatter:()=>D});var i=x(c()),D={title:\"M3DocRAG: Multi-modal Retrieval is What You Need for Multi-page Multi-document Understanding\",date:\"2024-11-08\",tags:[\"benchmark evaluation\",\"optical character recognition\",\"open-domain DocVQA\",\"multi-modal RAG framework\",\"indexing\",\"retrieval models\",\"Document visual question answering\",\"multi-modal language models\",\"text-based retrieval-augmented generation\"],categories:[\"cs.AI\",\"cs.CL\",\"cs.CV\"],problem:\"difficulties in applying existing methods in real-world scenarios\",solution:\"M3DocRAG multi-modal RAG framework\",pdf_url:\"https://arxiv.org/pdf/2411.04952\",arx_url:\"https://arxiv.org/abs/2411.04952\",score:6,authors:[\"Jaemin Cho\",\"Debanjan Mahata\",\"Ozan Irsoy\",\"Yujie He\",\"Mohit Bansal\",\"Ozan \\u0130rsoy\"],affiliations_aligned:[\"UNC Chapel Hill\",\"Bloomberg\",\"\",\"Bloomberg\",\"UNC Chapel Hill\",\"Bloomberg\"],affiliations:[\"\",\"UNC Chapel Hill\",\"Bloomberg\"]};function m(e){let n={p:\"p\",...e.components};return(0,i.jsx)(n.p,{children:\"Document visual question answering (DocVQA) pipelines that answer questions from documents have broad applications. Existing methods focus on handling single-page documents with multi-modal language models (MLMs), or rely on text-based retrieval-augmented generation (RAG) that uses text extraction tools such as optical character recognition (OCR). However, there are difficulties in applying these methods in real-world scenarios: (a) questions often require information across different pages or documents, where MLMs cannot handle many long documents; (b) documents often have important information in visual elements such as figures, but text extraction tools ignore them. We introduce M3DocRAG, a novel multi-modal RAG framework that flexibly accommodates various document contexts (closed-domain and open-domain), question hops (single-hop and multi-hop), and evidence modalities (text, chart, figure, etc.). M3DocRAG finds relevant documents and answers questions using a multi-modal retriever and an MLM, so that it can efficiently handle single or many documents while preserving visual information. Since previous DocVQA datasets ask questions in the context of a specific document, we also present M3DocVQA, a new benchmark for evaluating open-domain DocVQA over 3,000+ PDF documents with 40,000+ pages. In three benchmarks (M3DocVQA/MMLongBench-Doc/MP-DocVQA), empirical results show that M3DocRAG with ColPali and Qwen2-VL 7B achieves superior performance than many strong baselines, including state-of-the-art performance in MP-DocVQA. We provide comprehensive analyses of different indexing, MLMs, and retrieval models. Lastly, we qualitatively show that M3DocRAG can successfully handle various scenarios, such as when relevant information exists across multiple pages and when answer evidence only exists in images.\"})}function u(e={}){let{wrapper:n}=e.components||{};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(m,{...e})}):m(e)}return w(A);})();\n;return Component;"
    },
    "_id": "articles/2411.04952.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.04952.mdx",
      "sourceFileName": "2411.04952.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.04952"
    },
    "type": "Blog",
    "readingTime": {
      "text": "2 min read",
      "minutes": 1.215,
      "time": 72900,
      "words": 243
    },
    "slug": "2411.04952",
    "path": "articles/2411.04952",
    "filePath": "articles/2411.04952.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "M3DocRAG: Multi-modal Retrieval is What You Need for Multi-page Multi-document Understanding",
      "datePublished": "2024-11-08T00:00:00.000Z",
      "dateModified": "2024-11-08T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.04952"
    }
  },
  {
    "title": "LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation",
    "date": "2024-11-08T00:00:00.000Z",
    "tags": [
      "multimodal foundational models",
      "output embeddings",
      "textual understanding",
      "image captions",
      "visual encoder",
      "CLIP",
      "LLMs",
      "contrastive learning",
      "large language models",
      "cross-modal tasks",
      "cross-modal representation learning"
    ],
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "pdf_url": "https://arxiv.org/pdf/2411.04997",
    "arx_url": "https://arxiv.org/abs/2411.04997",
    "authors": [
      "Weiquan Huang",
      "Aoqi Wu",
      "Yifan Yang",
      "Xufang Luo",
      "Yuqing Yang",
      "Liang Hu",
      "Qi Dai",
      "Xiyang Dai",
      "Dongdong Chen",
      "Chong Luo",
      "Lili Qiu"
    ],
    "affiliations_aligned": [
      "Tongji University",
      "Tongji University",
      "Microsoft Corporation",
      "Microsoft Corporation",
      "Microsoft Corporation",
      "Tongji University",
      "Microsoft Corporation",
      "Microsoft Corporation",
      "Microsoft Corporation",
      "Microsoft Corporation",
      "Microsoft Corporation"
    ],
    "affiliations": [
      "Tongji University",
      "Microsoft Corporation"
    ],
    "problem": "limitations of vanilla CLIP in processing long and complex texts",
    "solution": "LLM2CLIP approach leveraging LLMs for improved multimodal representation learning",
    "score": 7,
    "body": {
      "raw": "\n\nCLIP is one of the most important multimodal foundational models today. What powers CLIP's capabilities? The rich supervision signals provided by natural language, the carrier of human knowledge, shape a powerful cross-modal representation space. However, with the rapid advancements in large language models LLMs like GPT-4 and LLaMA, the boundaries of language comprehension and generation are continually being pushed. This raises an intriguing question: can the capabilities of LLMs be harnessed to further improve multimodal representation learning? The potential benefits of incorporating LLMs into CLIP are clear. LLMs' strong textual understanding can fundamentally improve CLIP's ability to handle image captions, drastically enhancing its ability to process long and complex texts, a well-known limitation of vanilla CLIP. Moreover, LLMs are trained on a vast corpus of text, possessing open-world knowledge. This allows them to expand on caption information during training, increasing the efficiency of the learning process. In this paper, we propose LLM2CLIP, a novel approach that embraces the power of LLMs to unlock CLIP's potential. By fine-tuning the LLM in the caption space with contrastive learning, we extract its textual capabilities into the output embeddings, significantly improving the output layer's textual discriminability. We then design an efficient training process where the fine-tuned LLM acts as a powerful teacher for CLIP's visual encoder. Thanks to the LLM's presence, we can now incorporate longer and more complex captions without being restricted by vanilla CLIP's text encoder's context window and ability limitations. Our experiments demonstrate that this approach brings substantial improvements in cross-modal tasks.",
      "code": "var Component=(()=>{var u=Object.create;var t=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var h=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var L=(e,n)=>()=>(n||e((n={exports:{}}).exports,n),n.exports),M=(e,n)=>{for(var o in n)t(e,o,{get:n[o],enumerable:!0})},s=(e,n,o,r)=>{if(n&&typeof n==\"object\"||typeof n==\"function\")for(let i of m(n))!f.call(e,i)&&i!==o&&t(e,i,{get:()=>n[i],enumerable:!(r=d(n,i))||r.enumerable});return e};var C=(e,n,o)=>(o=e!=null?u(h(e)):{},s(n||!e||!e.__esModule?t(o,\"default\",{value:e,enumerable:!0}):o,e)),v=e=>s(t({},\"__esModule\",{value:!0}),e);var c=L((y,l)=>{l.exports=_jsx_runtime});var w={};M(w,{default:()=>g,frontmatter:()=>x});var a=C(c()),x={title:\"LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation\",date:\"2024-11-08\",tags:[\"multimodal foundational models\",\"output embeddings\",\"textual understanding\",\"image captions\",\"visual encoder\",\"CLIP\",\"LLMs\",\"contrastive learning\",\"large language models\",\"cross-modal tasks\",\"cross-modal representation learning\"],categories:[\"cs.CL\",\"cs.CV\"],problem:\"limitations of vanilla CLIP in processing long and complex texts\",solution:\"LLM2CLIP approach leveraging LLMs for improved multimodal representation learning\",pdf_url:\"https://arxiv.org/pdf/2411.04997\",arx_url:\"https://arxiv.org/abs/2411.04997\",score:7,authors:[\"Weiquan Huang\",\"Aoqi Wu\",\"Yifan Yang\",\"Xufang Luo\",\"Yuqing Yang\",\"Liang Hu\",\"Qi Dai\",\"Xiyang Dai\",\"Dongdong Chen\",\"Chong Luo\",\"Lili Qiu\"],affiliations_aligned:[\"Tongji University\",\"Tongji University\",\"Microsoft Corporation\",\"Microsoft Corporation\",\"Microsoft Corporation\",\"Tongji University\",\"Microsoft Corporation\",\"Microsoft Corporation\",\"Microsoft Corporation\",\"Microsoft Corporation\",\"Microsoft Corporation\"],affiliations:[\"Tongji University\",\"Microsoft Corporation\"]};function p(e){let n={p:\"p\",...e.components};return(0,a.jsx)(n.p,{children:\"CLIP is one of the most important multimodal foundational models today. What powers CLIP's capabilities? The rich supervision signals provided by natural language, the carrier of human knowledge, shape a powerful cross-modal representation space. However, with the rapid advancements in large language models LLMs like GPT-4 and LLaMA, the boundaries of language comprehension and generation are continually being pushed. This raises an intriguing question: can the capabilities of LLMs be harnessed to further improve multimodal representation learning? The potential benefits of incorporating LLMs into CLIP are clear. LLMs' strong textual understanding can fundamentally improve CLIP's ability to handle image captions, drastically enhancing its ability to process long and complex texts, a well-known limitation of vanilla CLIP. Moreover, LLMs are trained on a vast corpus of text, possessing open-world knowledge. This allows them to expand on caption information during training, increasing the efficiency of the learning process. In this paper, we propose LLM2CLIP, a novel approach that embraces the power of LLMs to unlock CLIP's potential. By fine-tuning the LLM in the caption space with contrastive learning, we extract its textual capabilities into the output embeddings, significantly improving the output layer's textual discriminability. We then design an efficient training process where the fine-tuned LLM acts as a powerful teacher for CLIP's visual encoder. Thanks to the LLM's presence, we can now incorporate longer and more complex captions without being restricted by vanilla CLIP's text encoder's context window and ability limitations. Our experiments demonstrate that this approach brings substantial improvements in cross-modal tasks.\"})}function g(e={}){let{wrapper:n}=e.components||{};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(p,{...e})}):p(e)}return v(w);})();\n;return Component;"
    },
    "_id": "articles/2411.04997.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.04997.mdx",
      "sourceFileName": "2411.04997.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.04997"
    },
    "type": "Blog",
    "readingTime": {
      "text": "2 min read",
      "minutes": 1.255,
      "time": 75300,
      "words": 251
    },
    "slug": "2411.04997",
    "path": "articles/2411.04997",
    "filePath": "articles/2411.04997.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation",
      "datePublished": "2024-11-08T00:00:00.000Z",
      "dateModified": "2024-11-08T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.04997"
    }
  },
  {
    "title": "PadChest-GR: A Bilingual Chest X-ray Dataset for Grounded Radiology Report Generation",
    "date": "2024-11-07T00:00:00.000Z",
    "tags": [
      "grounded radiology report generation",
      "CXR images",
      "model training",
      "chest X-ray dataset",
      "bi-lingual dataset",
      "radiology report generation",
      "clinical imaging",
      "annotation",
      "dataset curation",
      "localization"
    ],
    "categories": [
      "cs.CL",
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2411.05085v1",
    "arx_url": "http://arxiv.org/abs/2411.05085v1",
    "authors": [
      "Daniel C. Castro",
      "Aurelia Bustos",
      "Shruthi Bannur",
      "Stephanie L. Hyland",
      "Kenza Bouzid",
      "Maria Teodora Wetscherek",
      "Maria Dolores SÃ¡nchez-Valverde",
      "Lara Jaques-PÃ©rez",
      "Lourdes PÃ©rez-RodrÃ­guez",
      "Kenji Takeda",
      "JosÃ© MarÃ­a Salinas",
      "Javier Alvarez-Valle",
      "JoaquÃ­n Galant Herrero",
      "Antonio Pertusa",
      "Maria Dolores SÃ¡nchez Valverde",
      "Lara Jaques PÃ©rez",
      "Lourdes PÃ©rez RodrÃ­guez"
    ],
    "affiliations_aligned": [
      "Microsoft Research, Cambridge, UK",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "Department of Radiology, University Hospital Sant Joan dâAlacant, Spain",
      "Department of Radiology, University Hospital Sant Joan dâAlacant, Spain",
      "Department of Radiology, University Hospital Sant Joan dâAlacant, Spain"
    ],
    "affiliations": [
      "",
      "Microsoft Research, Cambridge, UK",
      "Department of Radiology, University Hospital Sant Joan dâAlacant, Spain"
    ],
    "problem": "lack of manually annotated chest X-ray datasets for training GRRG models",
    "solution": "PadChest-GR dataset for training GRRG models",
    "score": 7,
    "body": {
      "raw": "\n\nRadiology report generation (RRG) aims to create free-text radiology reports from clinical imaging. Grounded radiology report generation (GRRG) extends RRG by including the localisation of individual findings on the image. Currently, there are no manually annotated chest X-ray (CXR) datasets to train GRRG models. In this work, we present a dataset called PadChest-GR (Grounded-Reporting) derived from PadChest aimed at training GRRG models for CXR images. We curate a public bi-lingual dataset of 4,555 CXR studies with grounded reports (3,099 abnormal and 1,456 normal), each containing complete lists of sentences describing individual present (positive) and absent (negative) findings in English and Spanish. In total, PadChest-GR contains 7,037 positive and 3,422 negative finding sentences. Every positive finding sentence is associated with up to two independent sets of bounding boxes labelled by different readers and has categorical labels for finding type, locations, and progression. To the best of our knowledge, PadChest-GR is the first manually curated dataset designed to train GRRG models for understanding and interpreting radiological images and generated text. By including detailed localization and comprehensive annotations of all clinically relevant findings, it provides a valuable resource for developing and evaluating GRRG models from CXR images. PadChest-GR can be downloaded under request from https://bimcv.cipf.es/bimcv-projects/padchest-gr/",
      "code": "var Component=(()=>{var p=Object.create;var o=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var f=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,h=Object.prototype.hasOwnProperty;var R=(e,a)=>()=>(a||e((a={exports:{}}).exports,a),a.exports),v=(e,a)=>{for(var t in a)o(e,t,{get:a[t],enumerable:!0})},s=(e,a,t,r)=>{if(a&&typeof a==\"object\"||typeof a==\"function\")for(let i of f(a))!h.call(e,i)&&i!==t&&o(e,i,{get:()=>a[i],enumerable:!(r=u(a,i))||r.enumerable});return e};var y=(e,a,t)=>(t=e!=null?p(m(e)):{},s(a||!e||!e.__esModule?o(t,\"default\",{value:e,enumerable:!0}):t,e)),G=e=>s(o({},\"__esModule\",{value:!0}),e);var l=R((S,d)=>{d.exports=_jsx_runtime});var C={};v(C,{default:()=>g,frontmatter:()=>b});var n=y(l()),b={title:\"PadChest-GR: A Bilingual Chest X-ray Dataset for Grounded Radiology Report Generation\",date:\"2024-11-07\",tags:[\"grounded radiology report generation\",\"CXR images\",\"model training\",\"chest X-ray dataset\",\"bi-lingual dataset\",\"radiology report generation\",\"clinical imaging\",\"annotation\",\"dataset curation\",\"localization\"],categories:[\"cs.CL\",\"cs.CV\",\"cs.AI\"],problem:\"lack of manually annotated chest X-ray datasets for training GRRG models\",solution:\"PadChest-GR dataset for training GRRG models\",pdf_url:\"http://arxiv.org/pdf/2411.05085v1\",arx_url:\"http://arxiv.org/abs/2411.05085v1\",score:7,authors:[\"Daniel C. Castro\",\"Aurelia Bustos\",\"Shruthi Bannur\",\"Stephanie L. Hyland\",\"Kenza Bouzid\",\"Maria Teodora Wetscherek\",\"Maria Dolores S\\xE1nchez-Valverde\",\"Lara Jaques-P\\xE9rez\",\"Lourdes P\\xE9rez-Rodr\\xEDguez\",\"Kenji Takeda\",\"Jos\\xE9 Mar\\xEDa Salinas\",\"Javier Alvarez-Valle\",\"Joaqu\\xEDn Galant Herrero\",\"Antonio Pertusa\",\"Maria Dolores S\\xE1nchez Valverde\",\"Lara Jaques P\\xE9rez\",\"Lourdes P\\xE9rez Rodr\\xEDguez\"],affiliations_aligned:[\"Microsoft Research, Cambridge, UK\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"Department of Radiology, University Hospital Sant Joan d\\u2019Alacant, Spain\",\"Department of Radiology, University Hospital Sant Joan d\\u2019Alacant, Spain\",\"Department of Radiology, University Hospital Sant Joan d\\u2019Alacant, Spain\"],affiliations:[\"\",\"Microsoft Research, Cambridge, UK\",\"Department of Radiology, University Hospital Sant Joan d\\u2019Alacant, Spain\"]};function c(e){let a={a:\"a\",p:\"p\",...e.components};return(0,n.jsxs)(a.p,{children:[\"Radiology report generation (RRG) aims to create free-text radiology reports from clinical imaging. Grounded radiology report generation (GRRG) extends RRG by including the localisation of individual findings on the image. Currently, there are no manually annotated chest X-ray (CXR) datasets to train GRRG models. In this work, we present a dataset called PadChest-GR (Grounded-Reporting) derived from PadChest aimed at training GRRG models for CXR images. We curate a public bi-lingual dataset of 4,555 CXR studies with grounded reports (3,099 abnormal and 1,456 normal), each containing complete lists of sentences describing individual present (positive) and absent (negative) findings in English and Spanish. In total, PadChest-GR contains 7,037 positive and 3,422 negative finding sentences. Every positive finding sentence is associated with up to two independent sets of bounding boxes labelled by different readers and has categorical labels for finding type, locations, and progression. To the best of our knowledge, PadChest-GR is the first manually curated dataset designed to train GRRG models for understanding and interpreting radiological images and generated text. By including detailed localization and comprehensive annotations of all clinically relevant findings, it provides a valuable resource for developing and evaluating GRRG models from CXR images. PadChest-GR can be downloaded under request from \",(0,n.jsx)(a.a,{href:\"https://bimcv.cipf.es/bimcv-projects/padchest-gr/\",children:\"https://bimcv.cipf.es/bimcv-projects/padchest-gr/\"})]})}function g(e={}){let{wrapper:a}=e.components||{};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}return G(C);})();\n;return Component;"
    },
    "_id": "articles/2411.05085.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.05085.mdx",
      "sourceFileName": "2411.05085.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.05085"
    },
    "type": "Blog",
    "readingTime": {
      "text": "2 min read",
      "minutes": 1.01,
      "time": 60600,
      "words": 202
    },
    "slug": "2411.05085",
    "path": "articles/2411.05085",
    "filePath": "articles/2411.05085.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "PadChest-GR: A Bilingual Chest X-ray Dataset for Grounded Radiology Report Generation",
      "datePublished": "2024-11-07T00:00:00.000Z",
      "dateModified": "2024-11-07T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.05085"
    }
  },
  {
    "title": "ImpScore: A Learnable Metric For Quantifying The Implicitness Level of Language",
    "date": "2024-11-07T00:00:00.000Z",
    "tags": [
      "implicitness",
      "text understanding",
      "natural language processing",
      "large language models",
      "pairwise contrastive learning",
      "implicit language",
      "pragmatic interpretation",
      "semantic meaning",
      "scalar metric",
      "hate speech detection",
      "comprehension capabilities"
    ],
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2411.05172v1",
    "arx_url": "http://arxiv.org/abs/2411.05172v1",
    "authors": [
      "Yuxin Wang",
      "Xiaomeng Zhu",
      "Weimin Lyu",
      "Saeed Hassanpour",
      "Soroush Vosoughi"
    ],
    "affiliations_aligned": [
      "Department of Computer Science, Dartmouth College",
      "Department of Linguistics, Yale University",
      "Department of Computer Science, Stony Brook University",
      "Department of Computer Science, Dartmouth College",
      "Department of Computer Science, Dartmouth College"
    ],
    "affiliations": [
      "Department of Linguistics, Yale University",
      "Department of Computer Science, Dartmouth College",
      "Department of Computer Science, Stony Brook University"
    ],
    "problem": "absence of a robust metric for accurately measuring the implicitness of language",
    "solution": "ImpScore, a novel reference-free metric formulated through an interpretable regression model",
    "score": 6,
    "body": {
      "raw": "\n\nHandling implicit language is essential for natural language processing systems to achieve precise text understanding and facilitate natural interactions with users. Despite its importance, the absence of a robust metric for accurately measuring the implicitness of language significantly constrains the depth of analysis possible in evaluating models' comprehension capabilities. This paper addresses this gap by developing a scalar metric that quantifies the implicitness level of language without relying on external references. Drawing on principles from traditional linguistics, we define ''implicitness'' as the divergence between semantic meaning and pragmatic interpretation. To operationalize this definition, we introduce ImpScore, a novel, reference-free metric formulated through an interpretable regression model. This model is trained using pairwise contrastive learning on a specially curated dataset comprising $112,580$ (implicit sentence, explicit sentence) pairs. We validate ImpScore through a user study that compares its assessments with human evaluations on out-of-distribution data, demonstrating its accuracy and strong correlation with human judgments. Additionally, we apply ImpScore to hate speech detection datasets, illustrating its utility and highlighting significant limitations in current large language models' ability to understand highly implicit content. The metric model and its training data are available at https://github.com/audreycs/ImpScore.",
      "code": "var Component=(()=>{var u=Object.create;var r=Object.defineProperty;var g=Object.getOwnPropertyDescriptor;var h=Object.getOwnPropertyNames;var d=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var y=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),x=(t,e)=>{for(var i in e)r(t,i,{get:e[i],enumerable:!0})},o=(t,e,i,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let n of h(e))!f.call(t,n)&&n!==i&&r(t,n,{get:()=>e[n],enumerable:!(s=g(e,n))||s.enumerable});return t};var v=(t,e,i)=>(i=t!=null?u(d(t)):{},o(e||!t||!t.__esModule?r(i,\"default\",{value:t,enumerable:!0}):i,t)),b=t=>o(r({},\"__esModule\",{value:!0}),t);var l=y((C,c)=>{c.exports=_jsx_runtime});var S={};x(S,{default:()=>p,frontmatter:()=>w});var a=v(l()),w={title:\"ImpScore: A Learnable Metric For Quantifying The Implicitness Level of Language\",date:\"2024-11-07\",tags:[\"implicitness\",\"text understanding\",\"natural language processing\",\"large language models\",\"pairwise contrastive learning\",\"implicit language\",\"pragmatic interpretation\",\"semantic meaning\",\"scalar metric\",\"hate speech detection\",\"comprehension capabilities\"],categories:[\"cs.CL\"],problem:\"absence of a robust metric for accurately measuring the implicitness of language\",solution:\"ImpScore, a novel reference-free metric formulated through an interpretable regression model\",pdf_url:\"http://arxiv.org/pdf/2411.05172v1\",arx_url:\"http://arxiv.org/abs/2411.05172v1\",score:6,authors:[\"Yuxin Wang\",\"Xiaomeng Zhu\",\"Weimin Lyu\",\"Saeed Hassanpour\",\"Soroush Vosoughi\"],affiliations_aligned:[\"Department of Computer Science, Dartmouth College\",\"Department of Linguistics, Yale University\",\"Department of Computer Science, Stony Brook University\",\"Department of Computer Science, Dartmouth College\",\"Department of Computer Science, Dartmouth College\"],affiliations:[\"Department of Linguistics, Yale University\",\"Department of Computer Science, Dartmouth College\",\"Department of Computer Science, Stony Brook University\"]};function m(t){let e={a:\"a\",annotation:\"annotation\",math:\"math\",mn:\"mn\",mo:\"mo\",mrow:\"mrow\",p:\"p\",semantics:\"semantics\",span:\"span\",...t.components};return(0,a.jsxs)(e.p,{children:[\"Handling implicit language is essential for natural language processing systems to achieve precise text understanding and facilitate natural interactions with users. Despite its importance, the absence of a robust metric for accurately measuring the implicitness of language significantly constrains the depth of analysis possible in evaluating models' comprehension capabilities. This paper addresses this gap by developing a scalar metric that quantifies the implicitness level of language without relying on external references. Drawing on principles from traditional linguistics, we define ''implicitness'' as the divergence between semantic meaning and pragmatic interpretation. To operationalize this definition, we introduce ImpScore, a novel, reference-free metric formulated through an interpretable regression model. This model is trained using pairwise contrastive learning on a specially curated dataset comprising \",(0,a.jsxs)(e.span,{className:\"katex\",translate:\"no\",children:[(0,a.jsx)(e.span,{className:\"katex-mathml\",children:(0,a.jsx)(e.math,{xmlns:\"http://www.w3.org/1998/Math/MathML\",children:(0,a.jsxs)(e.semantics,{children:[(0,a.jsxs)(e.mrow,{children:[(0,a.jsx)(e.mn,{children:\"112\"}),(0,a.jsx)(e.mo,{separator:\"true\",children:\",\"}),(0,a.jsx)(e.mn,{children:\"580\"})]}),(0,a.jsx)(e.annotation,{encoding:\"application/x-tex\",children:\"112,580\"})]})})}),(0,a.jsx)(e.span,{className:\"katex-html\",\"aria-hidden\":\"true\",children:(0,a.jsxs)(e.span,{className:\"base\",children:[(0,a.jsx)(e.span,{className:\"strut\",style:{height:\".8389em\",verticalAlign:\"-.1944em\"}}),(0,a.jsx)(e.span,{className:\"mord\",children:\"112\"}),(0,a.jsx)(e.span,{className:\"mpunct\",children:\",\"}),(0,a.jsx)(e.span,{className:\"mspace\",style:{marginRight:\".1667em\"}}),(0,a.jsx)(e.span,{className:\"mord\",children:\"580\"})]})})]}),\" (implicit sentence, explicit sentence) pairs. We validate ImpScore through a user study that compares its assessments with human evaluations on out-of-distribution data, demonstrating its accuracy and strong correlation with human judgments. Additionally, we apply ImpScore to hate speech detection datasets, illustrating its utility and highlighting significant limitations in current large language models' ability to understand highly implicit content. The metric model and its training data are available at \",(0,a.jsx)(e.a,{href:\"https://github.com/audreycs/ImpScore\",children:\"https://github.com/audreycs/ImpScore\"}),\".\"]})}function p(t={}){let{wrapper:e}=t.components||{};return e?(0,a.jsx)(e,{...t,children:(0,a.jsx)(m,{...t})}):m(t)}return b(S);})();\n;return Component;"
    },
    "_id": "articles/2411.05172.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.05172.mdx",
      "sourceFileName": "2411.05172.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.05172"
    },
    "type": "Blog",
    "readingTime": {
      "text": "1 min read",
      "minutes": 0.95,
      "time": 57000,
      "words": 190
    },
    "slug": "2411.05172",
    "path": "articles/2411.05172",
    "filePath": "articles/2411.05172.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "ImpScore: A Learnable Metric For Quantifying The Implicitness Level of Language",
      "datePublished": "2024-11-07T00:00:00.000Z",
      "dateModified": "2024-11-07T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.05172"
    }
  },
  {
    "title": "An Early FIRST Reproduction and Improvements to Single-Token Decoding for Fast Listwise Reranking",
    "date": "2024-11-11T00:00:00.000Z",
    "tags": [
      "backbone models",
      "learning-to-rank objective",
      "zero-shot single-token reranking",
      "listwise reranking",
      "large language models",
      "computational demands",
      "inference latency",
      "TREC Deep Learning datasets",
      "LM pre-training",
      "first-stage retrievers"
    ],
    "categories": [
      "cs.IR",
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2411.05508",
    "arx_url": "https://arxiv.org/abs/2411.05508",
    "authors": [
      "Zijian Chen",
      "Ronak Pradeep",
      "Jimmy Lin"
    ],
    "affiliations_aligned": [
      "David R. Cheriton School of Computer Science, University of Waterloo",
      "David R. Cheriton School of Computer Science, University of Waterloo",
      "David R. Cheriton School of Computer Science, University of Waterloo"
    ],
    "affiliations": [
      "David R. Cheriton School of Computer Science, University of Waterloo"
    ],
    "problem": "high computational demands of large language models for listwise reranking",
    "solution": "FIRST approach integrating learning-to-rank objective and single-token logits",
    "score": 6,
    "body": {
      "raw": "\n\nRecent advances have demonstrated that large language models (LLMs) excel as listwise rerankers, but their high computational demands remain a barrier to widespread adoption. Further, the traditional language modeling (LM) objective is not ideally suited for reranking tasks. FIRST is a novel approach that addresses these challenges by integrating a learning-to-rank objective and leveraging the logits of only the first generated token, thereby significantly reducing inference latency compared to traditional LLM rerankers. In this study, we extend the evaluation of FIRST to the TREC Deep Learning datasets (DL19-22), validating its robustness across diverse domains. We investigate the influence of different first-stage retrievers on FIRST rerankers, observing diminishing returns and patterns consistent with traditional LLM rerankers. Through applying the FIRST objective to a broader range of backbone models, we achieve effectiveness surpassing the original implementation. Our experiments confirm that fast reranking with single-token logits does not compromise out-of-domain reranking quality. To better quantify the computational savings in the original study, we measure and compare latency to find a 21%-42% gain across various models and benchmarks. Moreover, while LM training implicitly improves zero-shot single-token reranking, our experiments also raise questions about whether LM pre-training may hinder subsequent fine-tuning with the FIRST objective. These findings pave the way for more efficient and effective listwise reranking in future applications.",
      "code": "var Component=(()=>{var h=Object.create;var a=Object.defineProperty;var f=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,p=Object.prototype.hasOwnProperty;var v=(e,n)=>()=>(n||e((n={exports:{}}).exports,n),n.exports),k=(e,n)=>{for(var t in n)a(e,t,{get:n[t],enumerable:!0})},s=(e,n,t,o)=>{if(n&&typeof n==\"object\"||typeof n==\"function\")for(let i of m(n))!p.call(e,i)&&i!==t&&a(e,i,{get:()=>n[i],enumerable:!(o=f(n,i))||o.enumerable});return e};var b=(e,n,t)=>(t=e!=null?h(u(e)):{},s(n||!e||!e.__esModule?a(t,\"default\",{value:e,enumerable:!0}):t,e)),y=e=>s(a({},\"__esModule\",{value:!0}),e);var g=v((S,l)=>{l.exports=_jsx_runtime});var L={};k(L,{default:()=>d,frontmatter:()=>R});var r=b(g()),R={title:\"An Early FIRST Reproduction and Improvements to Single-Token Decoding for Fast Listwise Reranking\",date:\"2024-11-11\",tags:[\"backbone models\",\"learning-to-rank objective\",\"zero-shot single-token reranking\",\"listwise reranking\",\"large language models\",\"computational demands\",\"inference latency\",\"TREC Deep Learning datasets\",\"LM pre-training\",\"first-stage retrievers\"],categories:[\"cs.IR\",\"cs.CL\"],problem:\"high computational demands of large language models for listwise reranking\",solution:\"FIRST approach integrating learning-to-rank objective and single-token logits\",pdf_url:\"https://arxiv.org/pdf/2411.05508\",arx_url:\"https://arxiv.org/abs/2411.05508\",score:6,authors:[\"Zijian Chen\",\"Ronak Pradeep\",\"Jimmy Lin\"],affiliations_aligned:[\"David R. Cheriton School of Computer Science, University of Waterloo\",\"David R. Cheriton School of Computer Science, University of Waterloo\",\"David R. Cheriton School of Computer Science, University of Waterloo\"],affiliations:[\"David R. Cheriton School of Computer Science, University of Waterloo\"]};function c(e){let n={p:\"p\",...e.components};return(0,r.jsx)(n.p,{children:\"Recent advances have demonstrated that large language models (LLMs) excel as listwise rerankers, but their high computational demands remain a barrier to widespread adoption. Further, the traditional language modeling (LM) objective is not ideally suited for reranking tasks. FIRST is a novel approach that addresses these challenges by integrating a learning-to-rank objective and leveraging the logits of only the first generated token, thereby significantly reducing inference latency compared to traditional LLM rerankers. In this study, we extend the evaluation of FIRST to the TREC Deep Learning datasets (DL19-22), validating its robustness across diverse domains. We investigate the influence of different first-stage retrievers on FIRST rerankers, observing diminishing returns and patterns consistent with traditional LLM rerankers. Through applying the FIRST objective to a broader range of backbone models, we achieve effectiveness surpassing the original implementation. Our experiments confirm that fast reranking with single-token logits does not compromise out-of-domain reranking quality. To better quantify the computational savings in the original study, we measure and compare latency to find a 21%-42% gain across various models and benchmarks. Moreover, while LM training implicitly improves zero-shot single-token reranking, our experiments also raise questions about whether LM pre-training may hinder subsequent fine-tuning with the FIRST objective. These findings pave the way for more efficient and effective listwise reranking in future applications.\"})}function d(e={}){let{wrapper:n}=e.components||{};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}return y(L);})();\n;return Component;"
    },
    "_id": "articles/2411.05508.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.05508.mdx",
      "sourceFileName": "2411.05508.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.05508"
    },
    "type": "Blog",
    "readingTime": {
      "text": "2 min read",
      "minutes": 1.075,
      "time": 64500,
      "words": 215
    },
    "slug": "2411.05508",
    "path": "articles/2411.05508",
    "filePath": "articles/2411.05508.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "An Early FIRST Reproduction and Improvements to Single-Token Decoding for Fast Listwise Reranking",
      "datePublished": "2024-11-11T00:00:00.000Z",
      "dateModified": "2024-11-11T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.05508"
    }
  },
  {
    "title": "ASL STEM Wiki: Dataset and Benchmark for Interpreting STEM Articles",
    "date": "2024-11-11T00:00:00.000Z",
    "tags": [
      "technical concepts",
      "interpreters",
      "parallel corpus",
      "signed languages",
      "fingerspelling",
      "ASL STEM Wiki",
      "Deaf and hard-of-hearing students",
      "AI resources",
      "STEM education"
    ],
    "categories": [
      "cs.HC",
      "cs.CV",
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2411.05783",
    "arx_url": "https://arxiv.org/abs/2411.05783",
    "authors": [
      "Kayo Yin",
      "Chinmay Singh",
      "Fyodor O. Minakov",
      "Vanessa Milan",
      "Hal Daum\\'e III",
      "Cyril Zhang",
      "Alex X. Lu",
      "Danielle Bragg",
      "Hal DaumÃ© III"
    ],
    "affiliations_aligned": [
      "University of California, Berkeley",
      "Microsoft Research",
      "Microsoft Research",
      "Microsoft Research",
      "",
      "Microsoft Research",
      "Microsoft Research",
      "Microsoft Research",
      "University of Maryland"
    ],
    "affiliations": [
      "",
      "Microsoft Research",
      "University of California, Berkeley",
      "University of Maryland"
    ],
    "problem": "barriers in accessing STEM education for DHH students",
    "solution": "ASL STEM Wiki dataset and models for identifying fingerspelled words",
    "score": 6,
    "body": {
      "raw": "\n\nDeaf and hard-of-hearing (DHH) students face significant barriers in accessing science, technology, engineering, and mathematics (STEM) education, notably due to the scarcity of STEM resources in signed languages. To help address this, we introduce ASL STEM Wiki: a parallel corpus of 254 Wikipedia articles on STEM topics in English, interpreted into over 300 hours of American Sign Language (ASL). ASL STEM Wiki is the first continuous signing dataset focused on STEM, facilitating the development of AI resources for STEM education in ASL. We identify several use cases of ASL STEM Wiki with human-centered applications. For example, because this dataset highlights the frequent use of fingerspelling for technical concepts, which inhibits DHH students' ability to learn, we develop models to identify fingerspelled words -- which can later be used to query for appropriate ASL signs to suggest to interpreters.",
      "code": "var Component=(()=>{var u=Object.create;var s=Object.defineProperty;var g=Object.getOwnPropertyDescriptor;var h=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,M=Object.prototype.hasOwnProperty;var S=(e,i)=>()=>(i||e((i={exports:{}}).exports,i),i.exports),y=(e,i)=>{for(var t in i)s(e,t,{get:i[t],enumerable:!0})},o=(e,i,t,a)=>{if(i&&typeof i==\"object\"||typeof i==\"function\")for(let n of h(i))!M.call(e,n)&&n!==t&&s(e,n,{get:()=>i[n],enumerable:!(a=g(i,n))||a.enumerable});return e};var m=(e,i,t)=>(t=e!=null?u(p(e)):{},o(i||!e||!e.__esModule?s(t,\"default\",{value:e,enumerable:!0}):t,e)),A=e=>o(s({},\"__esModule\",{value:!0}),e);var l=S((x,c)=>{c.exports=_jsx_runtime});var T={};y(T,{default:()=>d,frontmatter:()=>E});var r=m(l()),E={title:\"ASL STEM Wiki: Dataset and Benchmark for Interpreting STEM Articles\",date:\"2024-11-11\",tags:[\"technical concepts\",\"interpreters\",\"parallel corpus\",\"signed languages\",\"fingerspelling\",\"ASL STEM Wiki\",\"Deaf and hard-of-hearing students\",\"AI resources\",\"STEM education\"],categories:[\"cs.HC\",\"cs.CV\",\"cs.CL\",\"cs.AI\"],problem:\"barriers in accessing STEM education for DHH students\",solution:\"ASL STEM Wiki dataset and models for identifying fingerspelled words\",pdf_url:\"https://arxiv.org/pdf/2411.05783\",arx_url:\"https://arxiv.org/abs/2411.05783\",score:6,authors:[\"Kayo Yin\",\"Chinmay Singh\",\"Fyodor O. Minakov\",\"Vanessa Milan\",\"Hal Daum\\\\'e III\",\"Cyril Zhang\",\"Alex X. Lu\",\"Danielle Bragg\",\"Hal Daum\\xE9 III\"],affiliations_aligned:[\"University of California, Berkeley\",\"Microsoft Research\",\"Microsoft Research\",\"Microsoft Research\",\"\",\"Microsoft Research\",\"Microsoft Research\",\"Microsoft Research\",\"University of Maryland\"],affiliations:[\"\",\"Microsoft Research\",\"University of California, Berkeley\",\"University of Maryland\"]};function f(e){let i={p:\"p\",...e.components};return(0,r.jsx)(i.p,{children:\"Deaf and hard-of-hearing (DHH) students face significant barriers in accessing science, technology, engineering, and mathematics (STEM) education, notably due to the scarcity of STEM resources in signed languages. To help address this, we introduce ASL STEM Wiki: a parallel corpus of 254 Wikipedia articles on STEM topics in English, interpreted into over 300 hours of American Sign Language (ASL). ASL STEM Wiki is the first continuous signing dataset focused on STEM, facilitating the development of AI resources for STEM education in ASL. We identify several use cases of ASL STEM Wiki with human-centered applications. For example, because this dataset highlights the frequent use of fingerspelling for technical concepts, which inhibits DHH students' ability to learn, we develop models to identify fingerspelled words -- which can later be used to query for appropriate ASL signs to suggest to interpreters.\"})}function d(e={}){let{wrapper:i}=e.components||{};return i?(0,r.jsx)(i,{...e,children:(0,r.jsx)(f,{...e})}):f(e)}return A(T);})();\n;return Component;"
    },
    "_id": "articles/2411.05783.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.05783.mdx",
      "sourceFileName": "2411.05783.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.05783"
    },
    "type": "Blog",
    "readingTime": {
      "text": "1 min read",
      "minutes": 0.69,
      "time": 41400,
      "words": 138
    },
    "slug": "2411.05783",
    "path": "articles/2411.05783",
    "filePath": "articles/2411.05783.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "ASL STEM Wiki: Dataset and Benchmark for Interpreting STEM Articles",
      "datePublished": "2024-11-11T00:00:00.000Z",
      "dateModified": "2024-11-11T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.05783"
    }
  },
  {
    "title": "Autoregressive Models in Vision: A Survey",
    "date": "2024-11-12T00:00:00.000Z",
    "tags": [
      "3D generation",
      "multi-modal generation",
      "embodied AI",
      "video generation",
      "autoregessive modeling",
      "sequence representation",
      "3D medical AI",
      "token-based models",
      "generative models",
      "natural language processing",
      "image generation",
      "visual content generation",
      "research challenges",
      "pixel-based models",
      "computer vision",
      "scale-based models"
    ],
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "pdf_url": "https://arxiv.org/pdf/2411.05902",
    "arx_url": "https://arxiv.org/abs/2411.05902",
    "authors": [
      "Jing Xiong",
      "Gongye Liu",
      "Lun Huang",
      "Chengyue Wu",
      "Taiqiang Wu",
      "Yao Mu",
      "Yuan Yao",
      "Hui Shen",
      "Zhongwei Wan",
      "Jinfa Huang",
      "Chaofan Tao",
      "Shen Yan",
      "Huaxiu Yao",
      "Lingpeng Kong",
      "Hongxia Yang",
      "Mi Zhang",
      "Guillermo Sapiro",
      "Jiebo Luo",
      "Ping Luo",
      "Ngai Wong"
    ],
    "affiliations_aligned": [
      "The University of Hong Kong",
      "Tsinghua University",
      "Duke University",
      "The University of Hong Kong",
      "The University of Hong Kong",
      "The University of Hong Kong",
      "University of Rochester",
      "The Ohio State University",
      "The Ohio State University",
      "University of Rochester",
      "The University of Hong Kong",
      "Bytedance",
      "The University of North Carolina at Chapel Hill",
      "The University of Hong Kong",
      "The Hong Kong Polytechnic University",
      "The Ohio State University",
      "Apple",
      "University of Rochester",
      "The University of Hong Kong",
      "The University of Hong Kong"
    ],
    "affiliations": [
      "Tsinghua University",
      "The University of Hong Kong",
      "University of Rochester",
      "The Hong Kong Polytechnic University",
      "Apple",
      "Duke University",
      "Bytedance",
      "The University of North Carolina at Chapel Hill",
      "The Ohio State University"
    ],
    "problem": "current challenges to autoregressive models in vision",
    "solution": "multi-faceted categorization of autoregressive models in computer vision",
    "score": 7,
    "body": {
      "raw": "\nAutoregressive modeling has been a huge success in the field of natural language processing (NLP). Recently, autoregressive models have emerged as a significant area of focus in computer vision, where they excel in producing high-quality visual content. Autoregressive models in NLP typically operate on subword tokens. However, the representation strategy in computer vision can vary in different levels, pixel-level, token-level, or scale-level, reflecting the diverse and hierarchical nature of visual data compared to the sequential structure of language. This survey comprehensively examines the literature on autoregressive models applied to vision. To improve readability for researchers from diverse research backgrounds, we start with preliminary sequence representation and modeling in vision. Next, we divide the fundamental frameworks of visual autoregressive models into three general sub-categories, including pixel-based, token-based, and scale-based models based on the strategy of representation. We then explore the interconnections between autoregressive models and other generative models. Furthermore, we present a multi-faceted categorization of autoregressive models in computer vision, including image generation, video generation, 3D generation, and multi-modal generation. We also elaborate on their applications in diverse domains, including emerging domains such as embodied AI and 3D medical AI, with about 250 related references. Finally, we highlight the current challenges to autoregressive models in vision with suggestions about potential research directions. We have also set up a Github repository to organize the papers included in this survey at https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey.\n",
      "code": "var Component=(()=>{var c=Object.create;var a=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var v=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,p=Object.prototype.hasOwnProperty;var f=(e,i)=>()=>(i||e((i={exports:{}}).exports,i),i.exports),y=(e,i)=>{for(var n in i)a(e,n,{get:i[n],enumerable:!0})},s=(e,i,n,r)=>{if(i&&typeof i==\"object\"||typeof i==\"function\")for(let t of v(i))!p.call(e,t)&&t!==n&&a(e,t,{get:()=>i[t],enumerable:!(r=d(i,t))||r.enumerable});return e};var T=(e,i,n)=>(n=e!=null?c(m(e)):{},s(i||!e||!e.__esModule?a(n,\"default\",{value:e,enumerable:!0}):n,e)),U=e=>s(a({},\"__esModule\",{value:!0}),e);var g=f((w,l)=>{l.exports=_jsx_runtime});var x={};y(x,{default:()=>h,frontmatter:()=>b});var o=T(g()),b={title:\"Autoregressive Models in Vision: A Survey\",date:\"2024-11-12\",tags:[\"3D generation\",\"multi-modal generation\",\"embodied AI\",\"video generation\",\"autoregessive modeling\",\"sequence representation\",\"3D medical AI\",\"token-based models\",\"generative models\",\"natural language processing\",\"image generation\",\"visual content generation\",\"research challenges\",\"pixel-based models\",\"computer vision\",\"scale-based models\"],categories:[\"cs.CL\",\"cs.CV\"],problem:\"current challenges to autoregressive models in vision\",solution:\"multi-faceted categorization of autoregressive models in computer vision\",pdf_url:\"https://arxiv.org/pdf/2411.05902\",arx_url:\"https://arxiv.org/abs/2411.05902\",score:7,authors:[\"Jing Xiong\",\"Gongye Liu\",\"Lun Huang\",\"Chengyue Wu\",\"Taiqiang Wu\",\"Yao Mu\",\"Yuan Yao\",\"Hui Shen\",\"Zhongwei Wan\",\"Jinfa Huang\",\"Chaofan Tao\",\"Shen Yan\",\"Huaxiu Yao\",\"Lingpeng Kong\",\"Hongxia Yang\",\"Mi Zhang\",\"Guillermo Sapiro\",\"Jiebo Luo\",\"Ping Luo\",\"Ngai Wong\"],affiliations_aligned:[\"The University of Hong Kong\",\"Tsinghua University\",\"Duke University\",\"The University of Hong Kong\",\"The University of Hong Kong\",\"The University of Hong Kong\",\"University of Rochester\",\"The Ohio State University\",\"The Ohio State University\",\"University of Rochester\",\"The University of Hong Kong\",\"Bytedance\",\"The University of North Carolina at Chapel Hill\",\"The University of Hong Kong\",\"The Hong Kong Polytechnic University\",\"The Ohio State University\",\"Apple\",\"University of Rochester\",\"The University of Hong Kong\",\"The University of Hong Kong\"],affiliations:[\"Tsinghua University\",\"The University of Hong Kong\",\"University of Rochester\",\"The Hong Kong Polytechnic University\",\"Apple\",\"Duke University\",\"Bytedance\",\"The University of North Carolina at Chapel Hill\",\"The Ohio State University\"]};function u(e){let i={a:\"a\",p:\"p\",...e.components};return(0,o.jsxs)(i.p,{children:[\"Autoregressive modeling has been a huge success in the field of natural language processing (NLP). Recently, autoregressive models have emerged as a significant area of focus in computer vision, where they excel in producing high-quality visual content. Autoregressive models in NLP typically operate on subword tokens. However, the representation strategy in computer vision can vary in different levels, pixel-level, token-level, or scale-level, reflecting the diverse and hierarchical nature of visual data compared to the sequential structure of language. This survey comprehensively examines the literature on autoregressive models applied to vision. To improve readability for researchers from diverse research backgrounds, we start with preliminary sequence representation and modeling in vision. Next, we divide the fundamental frameworks of visual autoregressive models into three general sub-categories, including pixel-based, token-based, and scale-based models based on the strategy of representation. We then explore the interconnections between autoregressive models and other generative models. Furthermore, we present a multi-faceted categorization of autoregressive models in computer vision, including image generation, video generation, 3D generation, and multi-modal generation. We also elaborate on their applications in diverse domains, including emerging domains such as embodied AI and 3D medical AI, with about 250 related references. Finally, we highlight the current challenges to autoregressive models in vision with suggestions about potential research directions. We have also set up a Github repository to organize the papers included in this survey at \",(0,o.jsx)(i.a,{href:\"https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey\",children:\"https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey\"}),\".\"]})}function h(e={}){let{wrapper:i}=e.components||{};return i?(0,o.jsx)(i,{...e,children:(0,o.jsx)(u,{...e})}):u(e)}return U(x);})();\n;return Component;"
    },
    "_id": "articles/2411.05902.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.05902.mdx",
      "sourceFileName": "2411.05902.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.05902"
    },
    "type": "Blog",
    "readingTime": {
      "text": "2 min read",
      "minutes": 1.145,
      "time": 68700,
      "words": 229
    },
    "slug": "2411.05902",
    "path": "articles/2411.05902",
    "filePath": "articles/2411.05902.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "Autoregressive Models in Vision: A Survey",
      "datePublished": "2024-11-12T00:00:00.000Z",
      "dateModified": "2024-11-12T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.05902"
    }
  },
  {
    "title": "GUIDEQ: Framework for Guided Questioning for progressive informational collection and classification",
    "date": "2024-11-12T00:00:00.000Z",
    "tags": [
      "classification accuracy",
      "Question Answering",
      "customer support",
      "explainability",
      "guided questions",
      "classifier model",
      "information gathering",
      "text classification",
      "legal services",
      "F1-Score",
      "prompting strategy",
      "healthcare",
      "LLMs"
    ],
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2411.05991",
    "arx_url": "https://arxiv.org/abs/2411.05991",
    "authors": [
      "Priya Mishra",
      "Suraj Racha",
      "Kaustubh Ponkshe",
      "Adit Akarsh",
      "Ganesh Ramakrishnan"
    ],
    "affiliations_aligned": [
      "Indian Institute of Technology Bombay",
      "Indian Institute of Technology Bombay",
      "Indian Institute of Technology Bombay",
      "Indian Institute of Technology Bombay",
      "Indian Institute of Technology Bombay"
    ],
    "affiliations": [
      "Indian Institute of Technology Bombay"
    ],
    "problem": "insufficient or missing information for classification",
    "solution": "GUIDEQ framework for guided questioning",
    "score": 6,
    "body": {
      "raw": "\n\nQuestion Answering (QA) is an important part of tasks like text classification through information gathering. These are finding increasing use in sectors like healthcare, customer support, legal services, etc., to collect and classify responses into actionable categories. LLMs, although can support QA systems, they face a significant challenge of insufficient or missing information for classification. Although LLMs excel in reasoning, the models rely on their parametric knowledge to answer. However, questioning the user requires domain-specific information aiding to collect accurate information. Our work, GUIDEQ, presents a novel framework for asking guided questions to further progress a partial information. We leverage the explainability derived from the classifier model for along with LLMs for asking guided questions to further enhance the information. This further information helps in more accurate classification of a text. GUIDEQ derives the most significant key-words representative of a label using occlusions. We develop GUIDEQ's prompting strategy for guided questions based on the top-3 classifier label outputs and the significant words, to seek specific and relevant information, and classify in a targeted manner. Through our experimental results, we demonstrate that GUIDEQ outperforms other LLM-based baselines, yielding improved F1-Score through the accurate collection of relevant further information. We perform various analytical studies and also report better question quality compared to our method.",
      "code": "var Component=(()=>{var h=Object.create;var n=Object.defineProperty;var g=Object.getOwnPropertyDescriptor;var d=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,p=Object.prototype.hasOwnProperty;var y=(e,i)=>()=>(i||e((i={exports:{}}).exports,i),i.exports),b=(e,i)=>{for(var t in i)n(e,t,{get:i[t],enumerable:!0})},s=(e,i,t,r)=>{if(i&&typeof i==\"object\"||typeof i==\"function\")for(let o of d(i))!p.call(e,o)&&o!==t&&n(e,o,{get:()=>i[o],enumerable:!(r=g(i,o))||r.enumerable});return e};var I=(e,i,t)=>(t=e!=null?h(m(e)):{},s(i||!e||!e.__esModule?n(t,\"default\",{value:e,enumerable:!0}):t,e)),x=e=>s(n({},\"__esModule\",{value:!0}),e);var l=y((L,c)=>{c.exports=_jsx_runtime});var k={};b(k,{default:()=>u,frontmatter:()=>v});var a=I(l()),v={title:\"GUIDEQ: Framework for Guided Questioning for progressive informational collection and classification\",date:\"2024-11-12\",tags:[\"classification accuracy\",\"Question Answering\",\"customer support\",\"explainability\",\"guided questions\",\"classifier model\",\"information gathering\",\"text classification\",\"legal services\",\"F1-Score\",\"prompting strategy\",\"healthcare\",\"LLMs\"],categories:[\"cs.CL\"],problem:\"insufficient or missing information for classification\",solution:\"GUIDEQ framework for guided questioning\",pdf_url:\"https://arxiv.org/pdf/2411.05991\",arx_url:\"https://arxiv.org/abs/2411.05991\",score:6,authors:[\"Priya Mishra\",\"Suraj Racha\",\"Kaustubh Ponkshe\",\"Adit Akarsh\",\"Ganesh Ramakrishnan\"],affiliations_aligned:[\"Indian Institute of Technology Bombay\",\"Indian Institute of Technology Bombay\",\"Indian Institute of Technology Bombay\",\"Indian Institute of Technology Bombay\",\"Indian Institute of Technology Bombay\"],affiliations:[\"Indian Institute of Technology Bombay\"]};function f(e){let i={p:\"p\",...e.components};return(0,a.jsx)(i.p,{children:\"Question Answering (QA) is an important part of tasks like text classification through information gathering. These are finding increasing use in sectors like healthcare, customer support, legal services, etc., to collect and classify responses into actionable categories. LLMs, although can support QA systems, they face a significant challenge of insufficient or missing information for classification. Although LLMs excel in reasoning, the models rely on their parametric knowledge to answer. However, questioning the user requires domain-specific information aiding to collect accurate information. Our work, GUIDEQ, presents a novel framework for asking guided questions to further progress a partial information. We leverage the explainability derived from the classifier model for along with LLMs for asking guided questions to further enhance the information. This further information helps in more accurate classification of a text. GUIDEQ derives the most significant key-words representative of a label using occlusions. We develop GUIDEQ's prompting strategy for guided questions based on the top-3 classifier label outputs and the significant words, to seek specific and relevant information, and classify in a targeted manner. Through our experimental results, we demonstrate that GUIDEQ outperforms other LLM-based baselines, yielding improved F1-Score through the accurate collection of relevant further information. We perform various analytical studies and also report better question quality compared to our method.\"})}function u(e={}){let{wrapper:i}=e.components||{};return i?(0,a.jsx)(i,{...e,children:(0,a.jsx)(f,{...e})}):f(e)}return x(k);})();\n;return Component;"
    },
    "_id": "articles/2411.05991.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.05991.mdx",
      "sourceFileName": "2411.05991.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.05991"
    },
    "type": "Blog",
    "readingTime": {
      "text": "2 min read",
      "minutes": 1.06,
      "time": 63600,
      "words": 212
    },
    "slug": "2411.05991",
    "path": "articles/2411.05991",
    "filePath": "articles/2411.05991.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "GUIDEQ: Framework for Guided Questioning for progressive informational collection and classification",
      "datePublished": "2024-11-12T00:00:00.000Z",
      "dateModified": "2024-11-12T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.05991"
    }
  },
  {
    "title": "M-Longdoc: A Benchmark For Multimodal Super-Long Document Understanding And A Retrieval-Aware Tuning Framework",
    "date": "2024-11-12T00:00:00.000Z",
    "tags": [
      "question-answering",
      "large multimodal models",
      "multimodal document understanding",
      "retrieval-aware tuning",
      "automated methods",
      "benchmarking"
    ],
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2411.06176",
    "arx_url": "https://arxiv.org/abs/2411.06176",
    "authors": [
      "Yew Ken Chia",
      "Liying Cheng",
      "Hou Pong Chan",
      "Chaoqun Liu",
      "Maojia Song",
      "Sharifah Mahani Aljunied",
      "Soujanya Poria",
      "Lidong Bing"
    ],
    "affiliations_aligned": [
      "Singapore University of Technology and Design",
      "DAMO Academy, Alibaba Group, Singapore",
      "Nanyang Technological University, Singapore",
      "DAMO Academy, Alibaba Group, Singapore",
      "Singapore University of Technology and Design",
      "DAMO Academy, Alibaba Group, Singapore",
      "Singapore University of Technology and Design",
      "DAMO Academy, Alibaba Group, Singapore"
    ],
    "affiliations": [
      "Nanyang Technological University, Singapore",
      "DAMO Academy, Alibaba Group, Singapore",
      "Singapore University of Technology and Design"
    ],
    "problem": "understanding and answering questions over lengthy multimodal documents",
    "solution": "M-LongDoc benchmark and retrieval-aware tuning framework",
    "score": 6,
    "body": {
      "raw": "\n\nThe ability to understand and answer questions over documents can be useful in many business and practical applications. However, documents often contain lengthy and diverse multimodal contents such as texts, figures, and tables, which are very time-consuming for humans to read thoroughly. Hence, there is an urgent need to develop effective and automated methods to aid humans in this task. In this work, we introduce M-LongDoc, a benchmark of 851 samples, and an automated framework to evaluate the performance of large multimodal models. We further propose a retrieval-aware tuning approach for efficient and effective multimodal document reading. Compared to existing works, our benchmark consists of more recent and lengthy documents with hundreds of pages, while also requiring open-ended solutions and not just extractive answers. To our knowledge, our training framework is the first to directly address the retrieval setting for multimodal long documents. To enable tuning open-source models, we construct a training corpus in a fully automatic manner for the question-answering task over such documents. Experiments show that our tuning approach achieves a relative improvement of 4.6% for the correctness of model responses, compared to the baseline open-source models. Our data, code, and models are available at https://multimodal-documents.github.io.",
      "code": "var Component=(()=>{var m=Object.create;var i=Object.defineProperty;var g=Object.getOwnPropertyDescriptor;var h=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var y=(e,n)=>()=>(n||e((n={exports:{}}).exports,n),n.exports),v=(e,n)=>{for(var a in n)i(e,a,{get:n[a],enumerable:!0})},s=(e,n,a,r)=>{if(n&&typeof n==\"object\"||typeof n==\"function\")for(let t of h(n))!f.call(e,t)&&t!==a&&i(e,t,{get:()=>n[t],enumerable:!(r=g(n,t))||r.enumerable});return e};var b=(e,n,a)=>(a=e!=null?m(p(e)):{},s(n||!e||!e.__esModule?i(a,\"default\",{value:e,enumerable:!0}):a,e)),w=e=>s(i({},\"__esModule\",{value:!0}),e);var u=y((k,d)=>{d.exports=_jsx_runtime});var x={};v(x,{default:()=>c,frontmatter:()=>A});var o=b(u()),A={title:\"M-Longdoc: A Benchmark For Multimodal Super-Long Document Understanding And A Retrieval-Aware Tuning Framework\",date:\"2024-11-12\",tags:[\"question-answering\",\"large multimodal models\",\"multimodal document understanding\",\"retrieval-aware tuning\",\"automated methods\",\"benchmarking\"],categories:[\"cs.CL\"],problem:\"understanding and answering questions over lengthy multimodal documents\",solution:\"M-LongDoc benchmark and retrieval-aware tuning framework\",pdf_url:\"https://arxiv.org/pdf/2411.06176\",arx_url:\"https://arxiv.org/abs/2411.06176\",score:6,authors:[\"Yew Ken Chia\",\"Liying Cheng\",\"Hou Pong Chan\",\"Chaoqun Liu\",\"Maojia Song\",\"Sharifah Mahani Aljunied\",\"Soujanya Poria\",\"Lidong Bing\"],affiliations_aligned:[\"Singapore University of Technology and Design\",\"DAMO Academy, Alibaba Group, Singapore\",\"Nanyang Technological University, Singapore\",\"DAMO Academy, Alibaba Group, Singapore\",\"Singapore University of Technology and Design\",\"DAMO Academy, Alibaba Group, Singapore\",\"Singapore University of Technology and Design\",\"DAMO Academy, Alibaba Group, Singapore\"],affiliations:[\"Nanyang Technological University, Singapore\",\"DAMO Academy, Alibaba Group, Singapore\",\"Singapore University of Technology and Design\"]};function l(e){let n={a:\"a\",p:\"p\",...e.components};return(0,o.jsxs)(n.p,{children:[\"The ability to understand and answer questions over documents can be useful in many business and practical applications. However, documents often contain lengthy and diverse multimodal contents such as texts, figures, and tables, which are very time-consuming for humans to read thoroughly. Hence, there is an urgent need to develop effective and automated methods to aid humans in this task. In this work, we introduce M-LongDoc, a benchmark of 851 samples, and an automated framework to evaluate the performance of large multimodal models. We further propose a retrieval-aware tuning approach for efficient and effective multimodal document reading. Compared to existing works, our benchmark consists of more recent and lengthy documents with hundreds of pages, while also requiring open-ended solutions and not just extractive answers. To our knowledge, our training framework is the first to directly address the retrieval setting for multimodal long documents. To enable tuning open-source models, we construct a training corpus in a fully automatic manner for the question-answering task over such documents. Experiments show that our tuning approach achieves a relative improvement of 4.6% for the correctness of model responses, compared to the baseline open-source models. Our data, code, and models are available at \",(0,o.jsx)(n.a,{href:\"https://multimodal-documents.github.io\",children:\"https://multimodal-documents.github.io\"}),\".\"]})}function c(e={}){let{wrapper:n}=e.components||{};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(l,{...e})}):l(e)}return w(x);})();\n;return Component;"
    },
    "_id": "articles/2411.06176.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.06176.mdx",
      "sourceFileName": "2411.06176.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.06176"
    },
    "type": "Blog",
    "readingTime": {
      "text": "1 min read",
      "minutes": 0.99,
      "time": 59400,
      "words": 198
    },
    "slug": "2411.06176",
    "path": "articles/2411.06176",
    "filePath": "articles/2411.06176.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "M-Longdoc: A Benchmark For Multimodal Super-Long Document Understanding And A Retrieval-Aware Tuning Framework",
      "datePublished": "2024-11-12T00:00:00.000Z",
      "dateModified": "2024-11-12T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.06176"
    }
  },
  {
    "title": "Exploring Knowledge Boundaries in Large Language Models for Retrieval Judgment",
    "date": "2024-11-12T00:00:00.000Z",
    "tags": [
      "dynamic knowledge",
      "multi-hop problems",
      "Large Language Models",
      "performance improvement",
      "Retrieval-Augmented Generation",
      "Knowledge Boundary Model",
      "knowledge boundary",
      "question answering capabilities",
      "long-tail static knowledge",
      "computational costs"
    ],
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2411.06207",
    "arx_url": "https://arxiv.org/abs/2411.06207",
    "authors": [
      "Zhen Zhang",
      "Xinyu Wang",
      "Yong Jiang",
      "Zhuo Chen",
      "Feiteng Mu",
      "Mengting Hu",
      "Pengjun Xie",
      "Fei Huang"
    ],
    "affiliations_aligned": [
      "Alibaba Group",
      "Alibaba Group",
      "Alibaba Group",
      "Alibaba Group",
      "Alibaba Group",
      "Alibaba Group",
      "Alibaba Group",
      "Alibaba Group"
    ],
    "affiliations": [
      "Alibaba Group"
    ],
    "problem": "challenges in dynamically changing knowledge and managing unknown static knowledge",
    "solution": "Knowledge Boundary Model",
    "score": 8,
    "body": {
      "raw": "\n\nLarge Language Models (LLMs) are increasingly recognized for their practical applications. However, these models often encounter challenges in dynamically changing knowledge, as well as in managing unknown static knowledge. Retrieval-Augmented Generation (RAG) tackles this challenge and has shown a significant impact on LLMs. Actually, we find that the impact of RAG on the question answering capabilities of LLMs can be categorized into three groups: beneficial, neutral, and harmful. By minimizing retrieval requests that yield neutral or harmful results, we can effectively reduce both time and computational costs, while also improving the overall performance of LLMs. This insight motivates us to differentiate between types of questions using certain metrics as indicators, to decrease the retrieval ratio without compromising performance. In our work, we propose a method that is able to identify different types of questions from this view by training a Knowledge Boundary Model (KBM). Experiments conducted on 11 English and Chinese datasets illustrate that the KBM effectively delineates the knowledge boundary, significantly decreasing the proportion of retrievals required for optimal end-to-end performance. Specifically, we evaluate the effectiveness of KBM in three complex scenarios: dynamic knowledge, long-tail static knowledge, and multi-hop problems, as well as its functionality as an external LLM plug-in.",
      "code": "var Component=(()=>{var d=Object.create;var i=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var f=Object.getPrototypeOf,h=Object.prototype.hasOwnProperty;var b=(e,n)=>()=>(n||e((n={exports:{}}).exports,n),n.exports),w=(e,n)=>{for(var a in n)i(e,a,{get:n[a],enumerable:!0})},l=(e,n,a,r)=>{if(n&&typeof n==\"object\"||typeof n==\"function\")for(let t of m(n))!h.call(e,t)&&t!==a&&i(e,t,{get:()=>n[t],enumerable:!(r=p(n,t))||r.enumerable});return e};var y=(e,n,a)=>(a=e!=null?d(f(e)):{},l(n||!e||!e.__esModule?i(a,\"default\",{value:e,enumerable:!0}):a,e)),M=e=>l(i({},\"__esModule\",{value:!0}),e);var g=b((x,s)=>{s.exports=_jsx_runtime});var L={};w(L,{default:()=>u,frontmatter:()=>v});var o=y(g()),v={title:\"Exploring Knowledge Boundaries in Large Language Models for Retrieval Judgment\",date:\"2024-11-12\",tags:[\"dynamic knowledge\",\"multi-hop problems\",\"Large Language Models\",\"performance improvement\",\"Retrieval-Augmented Generation\",\"Knowledge Boundary Model\",\"knowledge boundary\",\"question answering capabilities\",\"long-tail static knowledge\",\"computational costs\"],categories:[\"cs.CL\"],problem:\"challenges in dynamically changing knowledge and managing unknown static knowledge\",solution:\"Knowledge Boundary Model\",pdf_url:\"https://arxiv.org/pdf/2411.06207\",arx_url:\"https://arxiv.org/abs/2411.06207\",score:8,authors:[\"Zhen Zhang\",\"Xinyu Wang\",\"Yong Jiang\",\"Zhuo Chen\",\"Feiteng Mu\",\"Mengting Hu\",\"Pengjun Xie\",\"Fei Huang\"],affiliations_aligned:[\"Alibaba Group\",\"Alibaba Group\",\"Alibaba Group\",\"Alibaba Group\",\"Alibaba Group\",\"Alibaba Group\",\"Alibaba Group\",\"Alibaba Group\"],affiliations:[\"Alibaba Group\"]};function c(e){let n={p:\"p\",...e.components};return(0,o.jsx)(n.p,{children:\"Large Language Models (LLMs) are increasingly recognized for their practical applications. However, these models often encounter challenges in dynamically changing knowledge, as well as in managing unknown static knowledge. Retrieval-Augmented Generation (RAG) tackles this challenge and has shown a significant impact on LLMs. Actually, we find that the impact of RAG on the question answering capabilities of LLMs can be categorized into three groups: beneficial, neutral, and harmful. By minimizing retrieval requests that yield neutral or harmful results, we can effectively reduce both time and computational costs, while also improving the overall performance of LLMs. This insight motivates us to differentiate between types of questions using certain metrics as indicators, to decrease the retrieval ratio without compromising performance. In our work, we propose a method that is able to identify different types of questions from this view by training a Knowledge Boundary Model (KBM). Experiments conducted on 11 English and Chinese datasets illustrate that the KBM effectively delineates the knowledge boundary, significantly decreasing the proportion of retrievals required for optimal end-to-end performance. Specifically, we evaluate the effectiveness of KBM in three complex scenarios: dynamic knowledge, long-tail static knowledge, and multi-hop problems, as well as its functionality as an external LLM plug-in.\"})}function u(e={}){let{wrapper:n}=e.components||{};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}return M(L);})();\n;return Component;"
    },
    "_id": "articles/2411.06207.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.06207.mdx",
      "sourceFileName": "2411.06207.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.06207"
    },
    "type": "Blog",
    "readingTime": {
      "text": "1 min read",
      "minutes": 1.005,
      "time": 60300,
      "words": 201
    },
    "slug": "2411.06207",
    "path": "articles/2411.06207",
    "filePath": "articles/2411.06207.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "Exploring Knowledge Boundaries in Large Language Models for Retrieval Judgment",
      "datePublished": "2024-11-12T00:00:00.000Z",
      "dateModified": "2024-11-12T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.06207"
    }
  },
  {
    "title": "Explore the Reasoning Capability of LLMs in the Chess Testbed",
    "date": "2024-11-12T00:00:00.000Z",
    "tags": [
      "long-term strategic play",
      "large language models",
      "short-term tactical play",
      "finetuning LLaMA-3-8B",
      "annotated strategy and tactic",
      "chess",
      "comparison with commercial language models",
      "language explanations",
      "dataset MATE",
      "reasoning capability"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2411.06655",
    "arx_url": "https://arxiv.org/abs/2411.06655",
    "authors": [
      "Shu Wang",
      "Lei Ji",
      "Renxi Wang",
      "Wenxiao Zhao",
      "Haokun Liu",
      "Yifan Hou",
      "Ying Nian Wu"
    ],
    "affiliations_aligned": [
      "UCLA",
      "Microsoft Research",
      "MBZUAI",
      "UCLA",
      "University of Toronto",
      "Peking University",
      "UCLA"
    ],
    "affiliations": [
      "Microsoft Research",
      "UCLA",
      "University of Toronto",
      "Peking University",
      "MBZUAI"
    ],
    "problem": "long-term complex reasoning tasks in chess",
    "solution": "integration of annotated strategy and tactic in large language models",
    "score": 6,
    "body": {
      "raw": "\n\nReasoning is a central capability of human intelligence. In recent years, with the advent of large-scale datasets, pretrained large language models have emerged with new capabilities, including reasoning. However, these models still struggle with long-term, complex reasoning tasks, such as playing chess. Based on the observation that expert chess players employ a dual approach combining long-term strategic play with short-term tactical play along with language explanation, we propose improving the reasoning capability of large language models in chess by integrating annotated strategy and tactic. Specifically, we collect a dataset named MATE, which consists of 1 million chess positions with candidate moves annotated by chess experts for strategy and tactics. We finetune the LLaMA-3-8B model and compare it against state-of-the-art commercial language models in the task of selecting better chess moves. Our experiments show that our models perform better than GPT, Claude, and Gemini models. We find that language explanations can enhance the reasoning capability of large language models.",
      "code": "var Component=(()=>{var m=Object.create;var i=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var y=(e,a)=>()=>(a||e((a={exports:{}}).exports,a),a.exports),x=(e,a)=>{for(var t in a)i(e,t,{get:a[t],enumerable:!0})},r=(e,a,t,s)=>{if(a&&typeof a==\"object\"||typeof a==\"function\")for(let n of p(a))!f.call(e,n)&&n!==t&&i(e,n,{get:()=>a[n],enumerable:!(s=d(a,n))||s.enumerable});return e};var b=(e,a,t)=>(t=e!=null?m(u(e)):{},r(a||!e||!e.__esModule?i(t,\"default\",{value:e,enumerable:!0}):t,e)),w=e=>r(i({},\"__esModule\",{value:!0}),e);var c=y((A,l)=>{l.exports=_jsx_runtime});var v={};x(v,{default:()=>h,frontmatter:()=>L});var o=b(c()),L={title:\"Explore the Reasoning Capability of LLMs in the Chess Testbed\",date:\"2024-11-12\",tags:[\"long-term strategic play\",\"large language models\",\"short-term tactical play\",\"finetuning LLaMA-3-8B\",\"annotated strategy and tactic\",\"chess\",\"comparison with commercial language models\",\"language explanations\",\"dataset MATE\",\"reasoning capability\"],categories:[\"cs.AI\",\"cs.CL\"],problem:\"long-term complex reasoning tasks in chess\",solution:\"integration of annotated strategy and tactic in large language models\",pdf_url:\"https://arxiv.org/pdf/2411.06655\",arx_url:\"https://arxiv.org/abs/2411.06655\",score:6,authors:[\"Shu Wang\",\"Lei Ji\",\"Renxi Wang\",\"Wenxiao Zhao\",\"Haokun Liu\",\"Yifan Hou\",\"Ying Nian Wu\"],affiliations_aligned:[\"UCLA\",\"Microsoft Research\",\"MBZUAI\",\"UCLA\",\"University of Toronto\",\"Peking University\",\"UCLA\"],affiliations:[\"Microsoft Research\",\"UCLA\",\"University of Toronto\",\"Peking University\",\"MBZUAI\"]};function g(e){let a={p:\"p\",...e.components};return(0,o.jsx)(a.p,{children:\"Reasoning is a central capability of human intelligence. In recent years, with the advent of large-scale datasets, pretrained large language models have emerged with new capabilities, including reasoning. However, these models still struggle with long-term, complex reasoning tasks, such as playing chess. Based on the observation that expert chess players employ a dual approach combining long-term strategic play with short-term tactical play along with language explanation, we propose improving the reasoning capability of large language models in chess by integrating annotated strategy and tactic. Specifically, we collect a dataset named MATE, which consists of 1 million chess positions with candidate moves annotated by chess experts for strategy and tactics. We finetune the LLaMA-3-8B model and compare it against state-of-the-art commercial language models in the task of selecting better chess moves. Our experiments show that our models perform better than GPT, Claude, and Gemini models. We find that language explanations can enhance the reasoning capability of large language models.\"})}function h(e={}){let{wrapper:a}=e.components||{};return a?(0,o.jsx)(a,{...e,children:(0,o.jsx)(g,{...e})}):g(e)}return w(v);})();\n;return Component;"
    },
    "_id": "articles/2411.06655.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.06655.mdx",
      "sourceFileName": "2411.06655.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.06655"
    },
    "type": "Blog",
    "readingTime": {
      "text": "1 min read",
      "minutes": 0.79,
      "time": 47400,
      "words": 158
    },
    "slug": "2411.06655",
    "path": "articles/2411.06655",
    "filePath": "articles/2411.06655.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "Explore the Reasoning Capability of LLMs in the Chess Testbed",
      "datePublished": "2024-11-12T00:00:00.000Z",
      "dateModified": "2024-11-12T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.06655"
    }
  },
  {
    "title": "Greenback Bears and Fiscal Hawks: Finance is a Jungle and Text Embeddings Must Adapt",
    "date": "2024-11-12T00:00:00.000Z",
    "tags": [
      "text embeddings",
      "question answering",
      "finance",
      "benchmarks",
      "domain-specific training",
      "public datasets",
      "dataset scale",
      "hard negative mining"
    ],
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2411.07142",
    "arx_url": "https://arxiv.org/abs/2411.07142",
    "authors": [
      "Peter Anderson",
      "Mano Vikash Janardhanan",
      "Jason He",
      "Wei Cheng",
      "Charlie Flanagan"
    ],
    "affiliations_aligned": [
      "Balyasny Asset Management",
      "Balyasny Asset Management",
      "Balyasny Asset Management",
      "Balyasny Asset Management",
      "Balyasny Asset Management"
    ],
    "affiliations": [
      "Balyasny Asset Management"
    ],
    "problem": "challenges for general-purpose text embeddings in finance",
    "solution": "BAM embeddings finetuned on a dataset of query-passage pairs",
    "score": 6,
    "body": {
      "raw": "\n\nFinancial documents are filled with specialized terminology, arcane jargon, and curious acronyms that pose challenges for general-purpose text embeddings. Yet, few text embeddings specialized for finance have been reported in the literature, perhaps in part due to a lack of public datasets and benchmarks. We present BAM embeddings, a set of text embeddings finetuned on a carefully constructed dataset of 14.3M query-passage pairs. Demonstrating the benefits of domain-specific training, BAM embeddings achieve Recall@1 of 62.8% on a held-out test set, vs. only 39.2% for the best general-purpose text embedding from OpenAI. Further, BAM embeddings increase question answering accuracy by 8% on FinanceBench and show increased sensitivity to the finance-specific elements that are found in detailed, forward-looking and company and date-specific queries. To support further research we describe our approach in detail, quantify the importance of hard negative mining and dataset scale.",
      "code": "var Component=(()=>{var p=Object.create;var s=Object.defineProperty;var f=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,h=Object.prototype.hasOwnProperty;var y=(e,a)=>()=>(a||e((a={exports:{}}).exports,a),a.exports),b=(e,a)=>{for(var n in a)s(e,n,{get:a[n],enumerable:!0})},o=(e,a,n,r)=>{if(a&&typeof a==\"object\"||typeof a==\"function\")for(let t of u(a))!h.call(e,t)&&t!==n&&s(e,t,{get:()=>a[t],enumerable:!(r=f(a,t))||r.enumerable});return e};var x=(e,a,n)=>(n=e!=null?p(m(e)):{},o(a||!e||!e.__esModule?s(n,\"default\",{value:e,enumerable:!0}):n,e)),M=e=>o(s({},\"__esModule\",{value:!0}),e);var c=y((v,d)=>{d.exports=_jsx_runtime});var B={};b(B,{default:()=>g,frontmatter:()=>A});var i=x(c()),A={title:\"Greenback Bears and Fiscal Hawks: Finance is a Jungle and Text Embeddings Must Adapt\",date:\"2024-11-12\",tags:[\"text embeddings\",\"question answering\",\"finance\",\"benchmarks\",\"domain-specific training\",\"public datasets\",\"dataset scale\",\"hard negative mining\"],categories:[\"cs.CL\"],problem:\"challenges for general-purpose text embeddings in finance\",solution:\"BAM embeddings finetuned on a dataset of query-passage pairs\",pdf_url:\"https://arxiv.org/pdf/2411.07142\",arx_url:\"https://arxiv.org/abs/2411.07142\",score:6,authors:[\"Peter Anderson\",\"Mano Vikash Janardhanan\",\"Jason He\",\"Wei Cheng\",\"Charlie Flanagan\"],affiliations_aligned:[\"Balyasny Asset Management\",\"Balyasny Asset Management\",\"Balyasny Asset Management\",\"Balyasny Asset Management\",\"Balyasny Asset Management\"],affiliations:[\"Balyasny Asset Management\"]};function l(e){let a={p:\"p\",...e.components};return(0,i.jsx)(a.p,{children:\"Financial documents are filled with specialized terminology, arcane jargon, and curious acronyms that pose challenges for general-purpose text embeddings. Yet, few text embeddings specialized for finance have been reported in the literature, perhaps in part due to a lack of public datasets and benchmarks. We present BAM embeddings, a set of text embeddings finetuned on a carefully constructed dataset of 14.3M query-passage pairs. Demonstrating the benefits of domain-specific training, BAM embeddings achieve Recall@1 of 62.8% on a held-out test set, vs. only 39.2% for the best general-purpose text embedding from OpenAI. Further, BAM embeddings increase question answering accuracy by 8% on FinanceBench and show increased sensitivity to the finance-specific elements that are found in detailed, forward-looking and company and date-specific queries. To support further research we describe our approach in detail, quantify the importance of hard negative mining and dataset scale.\"})}function g(e={}){let{wrapper:a}=e.components||{};return a?(0,i.jsx)(a,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}return M(B);})();\n;return Component;"
    },
    "_id": "articles/2411.07142.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.07142.mdx",
      "sourceFileName": "2411.07142.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.07142"
    },
    "type": "Blog",
    "readingTime": {
      "text": "1 min read",
      "minutes": 0.705,
      "time": 42300,
      "words": 141
    },
    "slug": "2411.07142",
    "path": "articles/2411.07142",
    "filePath": "articles/2411.07142.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "Greenback Bears and Fiscal Hawks: Finance is a Jungle and Text Embeddings Must Adapt",
      "datePublished": "2024-11-12T00:00:00.000Z",
      "dateModified": "2024-11-12T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.07142"
    }
  },
  {
    "title": "Multi-Document Financial Question Answering using LLMs",
    "date": "2024-11-13T00:00:00.000Z",
    "tags": [
      "knowledge graph",
      "semantic tagging",
      "knowledge distillation",
      "LLM based scoring",
      "multi-document financial question answering",
      "RAG methods",
      "evaluation metrics"
    ],
    "categories": [
      "cs.IR",
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2411.07264",
    "arx_url": "https://arxiv.org/abs/2411.07264",
    "authors": [
      "Shalin Shah",
      "Srikanth Ryali",
      "Ramasubbu Venkatesh"
    ],
    "affiliations_aligned": [
      "Anvai AI",
      "",
      ""
    ],
    "affiliations": [
      "",
      "Anvai AI"
    ],
    "problem": "multi-document financial question answering",
    "solution": "methods using semantic tagging and knowledge graphs",
    "score": 6,
    "body": {
      "raw": "\n\nWe propose two new methods for multi-document financial question answering. First, a method that uses semantic tagging, and then, queries the index to get the context (RAG_SEM). And second, a Knowledge Graph (KG_RAG) based method that uses semantic tagging, and, retrieves knowledge graph triples from a graph database, as context. KG_RAG uses knowledge graphs constructed using a small model that is fine-tuned using knowledge distillation using a large teacher model. The data consists of 18 10K reports of Apple, Microsoft, Alphabet, NVIDIA, Amazon and Tesla for the years 2021, 2022 and 2023. The list of questions in the data consists of 111 complex questions including many esoteric questions that are difficult to answer and the answers are not completely obvious. As evaluation metrics, we use overall scores as well as segmented scores for measurement including the faithfulness, relevance, correctness, similarity, an LLM based overall score and the rouge scores as well as a similarity of embeddings. We find that both methods outperform plain RAG significantly. KG_RAG outperforms RAG_SEM in four out of nine metrics.",
      "code": "var Component=(()=>{var g=Object.create;var a=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var h=Object.getOwnPropertyNames;var f=Object.getPrototypeOf,p=Object.prototype.hasOwnProperty;var w=(e,t)=>()=>(t||e((t={exports:{}}).exports,t),t.exports),A=(e,t)=>{for(var n in t)a(e,n,{get:t[n],enumerable:!0})},r=(e,t,n,o)=>{if(t&&typeof t==\"object\"||typeof t==\"function\")for(let s of h(t))!p.call(e,s)&&s!==n&&a(e,s,{get:()=>t[s],enumerable:!(o=m(t,s))||o.enumerable});return e};var x=(e,t,n)=>(n=e!=null?g(f(e)):{},r(t||!e||!e.__esModule?a(n,\"default\",{value:e,enumerable:!0}):n,e)),_=e=>r(a({},\"__esModule\",{value:!0}),e);var c=w((M,l)=>{l.exports=_jsx_runtime});var v={};A(v,{default:()=>u,frontmatter:()=>b});var i=x(c()),b={title:\"Multi-Document Financial Question Answering using LLMs\",date:\"2024-11-13\",tags:[\"knowledge graph\",\"semantic tagging\",\"knowledge distillation\",\"LLM based scoring\",\"multi-document financial question answering\",\"RAG methods\",\"evaluation metrics\"],categories:[\"cs.IR\",\"cs.CL\"],problem:\"multi-document financial question answering\",solution:\"methods using semantic tagging and knowledge graphs\",pdf_url:\"https://arxiv.org/pdf/2411.07264\",arx_url:\"https://arxiv.org/abs/2411.07264\",score:6,authors:[\"Shalin Shah\",\"Srikanth Ryali\",\"Ramasubbu Venkatesh\"],affiliations_aligned:[\"Anvai AI\",\"\",\"\"],affiliations:[\"\",\"Anvai AI\"]};function d(e){let t={p:\"p\",...e.components};return(0,i.jsx)(t.p,{children:\"We propose two new methods for multi-document financial question answering. First, a method that uses semantic tagging, and then, queries the index to get the context (RAG_SEM). And second, a Knowledge Graph (KG_RAG) based method that uses semantic tagging, and, retrieves knowledge graph triples from a graph database, as context. KG_RAG uses knowledge graphs constructed using a small model that is fine-tuned using knowledge distillation using a large teacher model. The data consists of 18 10K reports of Apple, Microsoft, Alphabet, NVIDIA, Amazon and Tesla for the years 2021, 2022 and 2023. The list of questions in the data consists of 111 complex questions including many esoteric questions that are difficult to answer and the answers are not completely obvious. As evaluation metrics, we use overall scores as well as segmented scores for measurement including the faithfulness, relevance, correctness, similarity, an LLM based overall score and the rouge scores as well as a similarity of embeddings. We find that both methods outperform plain RAG significantly. KG_RAG outperforms RAG_SEM in four out of nine metrics.\"})}function u(e={}){let{wrapper:t}=e.components||{};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}return _(v);})();\n;return Component;"
    },
    "_id": "articles/2411.07264.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.07264.mdx",
      "sourceFileName": "2411.07264.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.07264"
    },
    "type": "Blog",
    "readingTime": {
      "text": "1 min read",
      "minutes": 0.87,
      "time": 52200,
      "words": 174
    },
    "slug": "2411.07264",
    "path": "articles/2411.07264",
    "filePath": "articles/2411.07264.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "Multi-Document Financial Question Answering using LLMs",
      "datePublished": "2024-11-13T00:00:00.000Z",
      "dateModified": "2024-11-13T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.07264"
    }
  },
  {
    "title": "SetLexSem Challenge: Using Set Operations to Evaluate the Lexical and Semantic Robustness of Language Models",
    "date": "2024-11-13T00:00:00.000Z",
    "tags": [
      "lexical variations",
      "NLP tasks",
      "semantic variations",
      "algorithmic tasks",
      "instruction-following abilities",
      "set operations",
      "set theory",
      "robustness evaluation",
      "synthetic benchmark",
      "language models"
    ],
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2411.07336",
    "arx_url": "https://arxiv.org/abs/2411.07336",
    "authors": [
      "Bardiya Akhbari",
      "Manish Gawali",
      "Nicholas A. Dronen"
    ],
    "affiliations_aligned": [
      "Amazon",
      "Amazon",
      "Amazon"
    ],
    "affiliations": [
      "Amazon"
    ],
    "problem": "poor robustness of language models to variation in set operations and operands",
    "solution": "SetLexSem Challenge benchmark for evaluating language model performance",
    "score": 6,
    "body": {
      "raw": "\nSet theory is foundational to mathematics and, when sets are finite, to reasoning about the world. An intelligent system should perform set operations consistently, regardless of superficial variations in the operands. Initially designed for semantically-oriented NLP tasks, large language models (LLMs) are now being evaluated on algorithmic tasks. Because sets are comprised of arbitrary symbols (e.g. numbers, words), they provide an opportunity to test, systematically, the invariance of LLMs' algorithmic abilities under simple lexical or semantic variations. To this end, we present the SetLexSem Challenge, a synthetic benchmark that evaluates the performance of LLMs on set operations. SetLexSem assesses the robustness of LLMs' instruction-following abilities under various conditions, focusing on the set operations and the nature and construction of the set members. Evaluating seven LLMs with SetLexSem, we find that they exhibit poor robustness to variation in both operation and operands. We show -- via the framework's systematic sampling of set members along lexical and semantic dimensions -- that LLMs are not only not robust to variation along these dimensions but demonstrate unique failure modes in particular, easy-to-create semantic groupings of \"deceptive\" sets. We find that rigorously measuring language model robustness to variation in frequency and length is challenging and present an analysis that measures them independently. The code for reproducing the results of this paper, and for generating the SetLexSem Challenge dataset, is available at https://github.com/amazon-science/SetLexSem-Challenge.\n",
      "code": "var Component=(()=>{var u=Object.create;var o=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var g=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var b=(e,t)=>()=>(t||e((t={exports:{}}).exports,t),t.exports),L=(e,t)=>{for(var a in t)o(e,a,{get:t[a],enumerable:!0})},r=(e,t,a,i)=>{if(t&&typeof t==\"object\"||typeof t==\"function\")for(let s of g(t))!f.call(e,s)&&s!==a&&o(e,s,{get:()=>t[s],enumerable:!(i=d(t,s))||i.enumerable});return e};var x=(e,t,a)=>(a=e!=null?u(p(e)):{},r(t||!e||!e.__esModule?o(a,\"default\",{value:e,enumerable:!0}):a,e)),y=e=>r(o({},\"__esModule\",{value:!0}),e);var m=b((M,l)=>{l.exports=_jsx_runtime});var S={};L(S,{default:()=>c,frontmatter:()=>v});var n=x(m()),v={title:\"SetLexSem Challenge: Using Set Operations to Evaluate the Lexical and Semantic Robustness of Language Models\",date:\"2024-11-13\",tags:[\"lexical variations\",\"NLP tasks\",\"semantic variations\",\"algorithmic tasks\",\"instruction-following abilities\",\"set operations\",\"set theory\",\"robustness evaluation\",\"synthetic benchmark\",\"language models\"],categories:[\"cs.CL\"],problem:\"poor robustness of language models to variation in set operations and operands\",solution:\"SetLexSem Challenge benchmark for evaluating language model performance\",pdf_url:\"https://arxiv.org/pdf/2411.07336\",arx_url:\"https://arxiv.org/abs/2411.07336\",score:6,authors:[\"Bardiya Akhbari\",\"Manish Gawali\",\"Nicholas A. Dronen\"],affiliations_aligned:[\"Amazon\",\"Amazon\",\"Amazon\"],affiliations:[\"Amazon\"]};function h(e){let t={a:\"a\",p:\"p\",...e.components};return(0,n.jsxs)(t.p,{children:[`Set theory is foundational to mathematics and, when sets are finite, to reasoning about the world. An intelligent system should perform set operations consistently, regardless of superficial variations in the operands. Initially designed for semantically-oriented NLP tasks, large language models (LLMs) are now being evaluated on algorithmic tasks. Because sets are comprised of arbitrary symbols (e.g. numbers, words), they provide an opportunity to test, systematically, the invariance of LLMs' algorithmic abilities under simple lexical or semantic variations. To this end, we present the SetLexSem Challenge, a synthetic benchmark that evaluates the performance of LLMs on set operations. SetLexSem assesses the robustness of LLMs' instruction-following abilities under various conditions, focusing on the set operations and the nature and construction of the set members. Evaluating seven LLMs with SetLexSem, we find that they exhibit poor robustness to variation in both operation and operands. We show -- via the framework's systematic sampling of set members along lexical and semantic dimensions -- that LLMs are not only not robust to variation along these dimensions but demonstrate unique failure modes in particular, easy-to-create semantic groupings of \"deceptive\" sets. We find that rigorously measuring language model robustness to variation in frequency and length is challenging and present an analysis that measures them independently. The code for reproducing the results of this paper, and for generating the SetLexSem Challenge dataset, is available at `,(0,n.jsx)(t.a,{href:\"https://github.com/amazon-science/SetLexSem-Challenge\",children:\"https://github.com/amazon-science/SetLexSem-Challenge\"}),\".\"]})}function c(e={}){let{wrapper:t}=e.components||{};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(h,{...e})}):h(e)}return y(S);})();\n;return Component;"
    },
    "_id": "articles/2411.07336.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.07336.mdx",
      "sourceFileName": "2411.07336.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.07336"
    },
    "type": "Blog",
    "readingTime": {
      "text": "2 min read",
      "minutes": 1.135,
      "time": 68100,
      "words": 227
    },
    "slug": "2411.07336",
    "path": "articles/2411.07336",
    "filePath": "articles/2411.07336.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "SetLexSem Challenge: Using Set Operations to Evaluate the Lexical and Semantic Robustness of Language Models",
      "datePublished": "2024-11-13T00:00:00.000Z",
      "dateModified": "2024-11-13T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.07336"
    }
  },
  {
    "title": "IdentifyMe: A Challenging Long-Context Mention Resolution Benchmark",
    "date": "2024-11-13T00:00:00.000Z",
    "tags": [
      "coreference resolution",
      "LLMs",
      "model performance",
      "pronominal mentions",
      "nominal mentions",
      "multiple-choice question format",
      "nested structures",
      "long narratives",
      "mention resolution",
      "evaluation metrics"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2411.07466",
    "arx_url": "https://arxiv.org/abs/2411.07466",
    "authors": [
      "Kawshik Manikantan",
      "Makarand Tapaswi",
      "Vineet Gandhi",
      "Shubham Toshniwal"
    ],
    "affiliations_aligned": [
      "CVIT, IIIT Hyderabad",
      "CVIT, IIIT Hyderabad",
      "CVIT, IIIT Hyderabad",
      "NVIDIA"
    ],
    "affiliations": [
      "CVIT, IIIT Hyderabad",
      "NVIDIA"
    ],
    "problem": "limitations of traditional output formats and evaluation metrics in capturing referential understanding",
    "solution": "IdentifyMe benchmark for mention resolution",
    "score": 6,
    "body": {
      "raw": "\n\nRecent evaluations of LLMs on coreference resolution have revealed that traditional output formats and evaluation metrics do not fully capture the models' referential understanding. To address this, we introduce IdentifyMe, a new benchmark for mention resolution presented in a multiple-choice question (MCQ) format, commonly used for evaluating LLMs. IdentifyMe features long narratives and employs heuristics to exclude easily identifiable mentions, creating a more challenging task. The benchmark also consists of a curated mixture of different mention types and corresponding entities, allowing for a fine-grained analysis of model performance. We evaluate both closed- and open source LLMs on IdentifyMe and observe a significant performance gap (20-30%) between the state-of-the-art sub-10B open models vs. closed ones. We observe that pronominal mentions, which have limited surface information, are typically much harder for models to resolve than nominal mentions. Additionally, we find that LLMs often confuse entities when their mentions overlap in nested structures. The highest-scoring model, GPT-4o, achieves 81.9% accuracy, highlighting the strong referential capabilities of state-of-the-art LLMs while also indicating room for further improvement.",
      "code": "var Component=(()=>{var f=Object.create;var a=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var h=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var I=(e,n)=>()=>(n||e((n={exports:{}}).exports,n),n.exports),v=(e,n)=>{for(var t in n)a(e,t,{get:n[t],enumerable:!0})},s=(e,n,t,r)=>{if(n&&typeof n==\"object\"||typeof n==\"function\")for(let o of h(n))!g.call(e,o)&&o!==t&&a(e,o,{get:()=>n[o],enumerable:!(r=u(n,o))||r.enumerable});return e};var y=(e,n,t)=>(t=e!=null?f(p(e)):{},s(n||!e||!e.__esModule?a(t,\"default\",{value:e,enumerable:!0}):t,e)),M=e=>s(a({},\"__esModule\",{value:!0}),e);var c=I((x,l)=>{l.exports=_jsx_runtime});var L={};v(L,{default:()=>m,frontmatter:()=>b});var i=y(c()),b={title:\"IdentifyMe: A Challenging Long-Context Mention Resolution Benchmark\",date:\"2024-11-13\",tags:[\"coreference resolution\",\"LLMs\",\"model performance\",\"pronominal mentions\",\"nominal mentions\",\"multiple-choice question format\",\"nested structures\",\"long narratives\",\"mention resolution\",\"evaluation metrics\"],categories:[\"cs.LG\",\"cs.AI\",\"cs.CL\"],problem:\"limitations of traditional output formats and evaluation metrics in capturing referential understanding\",solution:\"IdentifyMe benchmark for mention resolution\",pdf_url:\"https://arxiv.org/pdf/2411.07466\",arx_url:\"https://arxiv.org/abs/2411.07466\",score:6,authors:[\"Kawshik Manikantan\",\"Makarand Tapaswi\",\"Vineet Gandhi\",\"Shubham Toshniwal\"],affiliations_aligned:[\"CVIT, IIIT Hyderabad\",\"CVIT, IIIT Hyderabad\",\"CVIT, IIIT Hyderabad\",\"NVIDIA\"],affiliations:[\"CVIT, IIIT Hyderabad\",\"NVIDIA\"]};function d(e){let n={p:\"p\",...e.components};return(0,i.jsx)(n.p,{children:\"Recent evaluations of LLMs on coreference resolution have revealed that traditional output formats and evaluation metrics do not fully capture the models' referential understanding. To address this, we introduce IdentifyMe, a new benchmark for mention resolution presented in a multiple-choice question (MCQ) format, commonly used for evaluating LLMs. IdentifyMe features long narratives and employs heuristics to exclude easily identifiable mentions, creating a more challenging task. The benchmark also consists of a curated mixture of different mention types and corresponding entities, allowing for a fine-grained analysis of model performance. We evaluate both closed- and open source LLMs on IdentifyMe and observe a significant performance gap (20-30%) between the state-of-the-art sub-10B open models vs. closed ones. We observe that pronominal mentions, which have limited surface information, are typically much harder for models to resolve than nominal mentions. Additionally, we find that LLMs often confuse entities when their mentions overlap in nested structures. The highest-scoring model, GPT-4o, achieves 81.9% accuracy, highlighting the strong referential capabilities of state-of-the-art LLMs while also indicating room for further improvement.\"})}function m(e={}){let{wrapper:n}=e.components||{};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}return M(L);})();\n;return Component;"
    },
    "_id": "articles/2411.07466.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.07466.mdx",
      "sourceFileName": "2411.07466.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.07466"
    },
    "type": "Blog",
    "readingTime": {
      "text": "1 min read",
      "minutes": 0.86,
      "time": 51600,
      "words": 172
    },
    "slug": "2411.07466",
    "path": "articles/2411.07466",
    "filePath": "articles/2411.07466.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "IdentifyMe: A Challenging Long-Context Mention Resolution Benchmark",
      "datePublished": "2024-11-13T00:00:00.000Z",
      "dateModified": "2024-11-13T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.07466"
    }
  },
  {
    "title": "SparrowVQE: Visual Question Explanation for Course Content Understanding",
    "date": "2024-11-13T00:00:00.000Z",
    "tags": [
      "benchmark VQA datasets",
      "Visual Question Answering",
      "Phi-2 language model",
      "feature alignment",
      "domain fine-tuning",
      "multimodal model",
      "machine learning course",
      "MLVQE dataset",
      "SigLIP model",
      "instruction tuning",
      "training mechanism",
      "Visual Question Explanation",
      "MLP adapter"
    ],
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "pdf_url": "https://arxiv.org/pdf/2411.07516",
    "arx_url": "https://arxiv.org/abs/2411.07516",
    "authors": [
      "Jialu Li",
      "Manish Kumar Thota",
      "Ruslan Gokhman",
      "Radek Holik",
      "Youshan Zhang"
    ],
    "affiliations_aligned": [
      "Artificial Intelligence, Graduate Computer Science and Engineering Department, Yeshiva University, NY, USA",
      "Artificial Intelligence, Graduate Computer Science and Engineering Department, Yeshiva University, NY, USA",
      "Artificial Intelligence, Graduate Computer Science and Engineering Department, Yeshiva University, NY, USA",
      "Artificial Intelligence, Graduate Computer Science and Engineering Department, Yeshiva University, NY, USA",
      "Artificial Intelligence, Graduate Computer Science and Engineering Department, Yeshiva University, NY, USA"
    ],
    "affiliations": [
      "Artificial Intelligence, Graduate Computer Science and Engineering Department, Yeshiva University, NY, USA"
    ],
    "problem": "overly simplistic and short answers in VQA methods",
    "solution": "SparrowVQE multimodal model",
    "score": 8,
    "body": {
      "raw": "\nVisual Question Answering (VQA) research seeks to create AI systems to answer natural language questions in images, yet VQA methods often yield overly simplistic and short answers. This paper aims to advance the field by introducing Visual Question Explanation (VQE), which enhances the ability of VQA to provide detailed explanations rather than brief responses and address the need for more complex interaction with visual content. We first created an MLVQE dataset from a 14-week streamed video machine learning course, including 885 slide images, 110,407 words of transcripts, and 9,416 designed question-answer (QA) pairs. Next, we proposed a novel SparrowVQE, a small 3 billion parameters multimodal model. We trained our model with a three-stage training mechanism consisting of multimodal pre-training (slide images and transcripts feature alignment), instruction tuning (tuning the pre-trained model with transcripts and QA pairs), and domain fine-tuning (fine-tuning slide image and QA pairs). Eventually, our SparrowVQE can understand and connect visual information using the SigLIP model with transcripts using the Phi-2 language model with an MLP adapter. Experimental results demonstrate that our SparrowVQE achieves better performance in our developed MLVQE dataset and outperforms state-of-the-art methods in the other five benchmark VQA datasets. The source code is available at https://github.com/YoushanZhang/SparrowVQE.\n",
      "code": "var Component=(()=>{var m=Object.create;var r=Object.defineProperty;var h=Object.getOwnPropertyDescriptor;var g=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var v=(e,n)=>()=>(n||e((n={exports:{}}).exports,n),n.exports),Q=(e,n)=>{for(var t in n)r(e,t,{get:n[t],enumerable:!0})},o=(e,n,t,s)=>{if(n&&typeof n==\"object\"||typeof n==\"function\")for(let a of g(n))!f.call(e,a)&&a!==t&&r(e,a,{get:()=>n[a],enumerable:!(s=h(n,a))||s.enumerable});return e};var A=(e,n,t)=>(t=e!=null?m(p(e)):{},o(n||!e||!e.__esModule?r(t,\"default\",{value:e,enumerable:!0}):t,e)),w=e=>o(r({},\"__esModule\",{value:!0}),e);var d=v((x,l)=>{l.exports=_jsx_runtime});var E={};Q(E,{default:()=>c,frontmatter:()=>V});var i=A(d()),V={title:\"SparrowVQE: Visual Question Explanation for Course Content Understanding\",date:\"2024-11-13\",tags:[\"benchmark VQA datasets\",\"Visual Question Answering\",\"Phi-2 language model\",\"feature alignment\",\"domain fine-tuning\",\"multimodal model\",\"machine learning course\",\"MLVQE dataset\",\"SigLIP model\",\"instruction tuning\",\"training mechanism\",\"Visual Question Explanation\",\"MLP adapter\"],categories:[\"cs.CL\",\"cs.CV\"],problem:\"overly simplistic and short answers in VQA methods\",solution:\"SparrowVQE multimodal model\",pdf_url:\"https://arxiv.org/pdf/2411.07516\",arx_url:\"https://arxiv.org/abs/2411.07516\",score:8,authors:[\"Jialu Li\",\"Manish Kumar Thota\",\"Ruslan Gokhman\",\"Radek Holik\",\"Youshan Zhang\"],affiliations_aligned:[\"Artificial Intelligence, Graduate Computer Science and Engineering Department, Yeshiva University, NY, USA\",\"Artificial Intelligence, Graduate Computer Science and Engineering Department, Yeshiva University, NY, USA\",\"Artificial Intelligence, Graduate Computer Science and Engineering Department, Yeshiva University, NY, USA\",\"Artificial Intelligence, Graduate Computer Science and Engineering Department, Yeshiva University, NY, USA\",\"Artificial Intelligence, Graduate Computer Science and Engineering Department, Yeshiva University, NY, USA\"],affiliations:[\"Artificial Intelligence, Graduate Computer Science and Engineering Department, Yeshiva University, NY, USA\"]};function u(e){let n={a:\"a\",p:\"p\",...e.components};return(0,i.jsxs)(n.p,{children:[\"Visual Question Answering (VQA) research seeks to create AI systems to answer natural language questions in images, yet VQA methods often yield overly simplistic and short answers. This paper aims to advance the field by introducing Visual Question Explanation (VQE), which enhances the ability of VQA to provide detailed explanations rather than brief responses and address the need for more complex interaction with visual content. We first created an MLVQE dataset from a 14-week streamed video machine learning course, including 885 slide images, 110,407 words of transcripts, and 9,416 designed question-answer (QA) pairs. Next, we proposed a novel SparrowVQE, a small 3 billion parameters multimodal model. We trained our model with a three-stage training mechanism consisting of multimodal pre-training (slide images and transcripts feature alignment), instruction tuning (tuning the pre-trained model with transcripts and QA pairs), and domain fine-tuning (fine-tuning slide image and QA pairs). Eventually, our SparrowVQE can understand and connect visual information using the SigLIP model with transcripts using the Phi-2 language model with an MLP adapter. Experimental results demonstrate that our SparrowVQE achieves better performance in our developed MLVQE dataset and outperforms state-of-the-art methods in the other five benchmark VQA datasets. The source code is available at \",(0,i.jsx)(n.a,{href:\"https://github.com/YoushanZhang/SparrowVQE\",children:\"https://github.com/YoushanZhang/SparrowVQE\"}),\".\"]})}function c(e={}){let{wrapper:n}=e.components||{};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(u,{...e})}):u(e)}return w(E);})();\n;return Component;"
    },
    "_id": "articles/2411.07516.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.07516.mdx",
      "sourceFileName": "2411.07516.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.07516"
    },
    "type": "Blog",
    "readingTime": {
      "text": "1 min read",
      "minutes": 1.005,
      "time": 60300,
      "words": 201
    },
    "slug": "2411.07516",
    "path": "articles/2411.07516",
    "filePath": "articles/2411.07516.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "SparrowVQE: Visual Question Explanation for Course Content Understanding",
      "datePublished": "2024-11-13T00:00:00.000Z",
      "dateModified": "2024-11-13T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.07516"
    }
  },
  {
    "title": "Enhancing Multimodal Query Representation via Visual Dialogues for End-to-End Knowledge Retrieval",
    "date": "2024-11-14T00:00:00.000Z",
    "tags": [
      "fine-tuning scenarios",
      "end-to-end retrieval system",
      "image comprehension",
      "partial convolution mechanism",
      "multimodal query representations",
      "dynamic modality interaction",
      "visual information",
      "Visual Dialogue-to-Retrieval dataset",
      "caption generators",
      "information retrieval tasks",
      "zero-shot settings",
      "object detectors",
      "multimodal retrieval systems"
    ],
    "categories": [
      "cs.IR",
      "cs.MM",
      "cs.AI",
      "cs.CV"
    ],
    "pdf_url": "https://arxiv.org/pdf/2411.08334",
    "arx_url": "https://arxiv.org/abs/2411.08334",
    "authors": [
      "Yeong-Joon Ju",
      "Ho-Joong Kim",
      "Seong-Whan Lee"
    ],
    "affiliations_aligned": [
      "Department of Electrical Engineering, Pohang University of Science and Technology, Pohang, South Korea",
      "Department of Electrical Engineering, Pohang University of Science and Technology, Pohang, South Korea",
      "Department of Computer Science and Engineering, Pohang University of Science and Technology, Pohang, South Korea"
    ],
    "affiliations": [
      "Department of Electrical Engineering, Pohang University of Science and Technology, Pohang, South Korea",
      "Department of Computer Science and Engineering, Pohang University of Science and Technology, Pohang, South Korea"
    ],
    "problem": "disjointed models for image comprehension",
    "solution": "end-to-end retrieval system Ret-XKnow",
    "score": 6,
    "body": {
      "raw": "\n\nExisting multimodal retrieval systems often rely on disjointed models for image comprehension, such as object detectors and caption generators, leading to cumbersome implementations and training processes. To overcome this limitation, we propose an end-to-end retrieval system, Ret-XKnow, to endow a text retriever with the ability to understand multimodal queries via dynamic modality interaction. Ret-XKnow leverages a partial convolution mechanism to focus on visual information relevant to the given textual query, thereby enhancing multimodal query representations. To effectively learn multimodal interaction, we also introduce the Visual Dialogue-to-Retrieval (ViD2R) dataset automatically constructed from visual dialogue datasets. Our dataset construction process ensures that the dialogues are transformed into suitable information retrieval tasks using a text retriever. We demonstrate that our approach not only significantly improves retrieval performance in zero-shot settings but also achieves substantial improvements in fine-tuning scenarios. Our code is publicly available: https://github.com/yeongjoonJu/Ret_XKnow.",
      "code": "var Component=(()=>{var d=Object.create;var a=Object.defineProperty;var g=Object.getOwnPropertyDescriptor;var h=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var v=(e,t)=>()=>(t||e((t={exports:{}}).exports,t),t.exports),y=(e,t)=>{for(var n in t)a(e,n,{get:t[n],enumerable:!0})},s=(e,t,n,r)=>{if(t&&typeof t==\"object\"||typeof t==\"function\")for(let i of h(t))!f.call(e,i)&&i!==n&&a(e,i,{get:()=>t[i],enumerable:!(r=g(t,i))||r.enumerable});return e};var x=(e,t,n)=>(n=e!=null?d(p(e)):{},s(t||!e||!e.__esModule?a(n,\"default\",{value:e,enumerable:!0}):n,e)),b=e=>s(a({},\"__esModule\",{value:!0}),e);var c=v((j,l)=>{l.exports=_jsx_runtime});var E={};y(E,{default:()=>u,frontmatter:()=>S});var o=x(c()),S={title:\"Enhancing Multimodal Query Representation via Visual Dialogues for End-to-End Knowledge Retrieval\",date:\"2024-11-14\",tags:[\"fine-tuning scenarios\",\"end-to-end retrieval system\",\"image comprehension\",\"partial convolution mechanism\",\"multimodal query representations\",\"dynamic modality interaction\",\"visual information\",\"Visual Dialogue-to-Retrieval dataset\",\"caption generators\",\"information retrieval tasks\",\"zero-shot settings\",\"object detectors\",\"multimodal retrieval systems\"],categories:[\"cs.IR\",\"cs.MM\",\"cs.AI\",\"cs.CV\"],problem:\"disjointed models for image comprehension\",solution:\"end-to-end retrieval system Ret-XKnow\",pdf_url:\"https://arxiv.org/pdf/2411.08334\",arx_url:\"https://arxiv.org/abs/2411.08334\",score:6,authors:[\"Yeong-Joon Ju\",\"Ho-Joong Kim\",\"Seong-Whan Lee\"],affiliations_aligned:[\"Department of Electrical Engineering, Pohang University of Science and Technology, Pohang, South Korea\",\"Department of Electrical Engineering, Pohang University of Science and Technology, Pohang, South Korea\",\"Department of Computer Science and Engineering, Pohang University of Science and Technology, Pohang, South Korea\"],affiliations:[\"Department of Electrical Engineering, Pohang University of Science and Technology, Pohang, South Korea\",\"Department of Computer Science and Engineering, Pohang University of Science and Technology, Pohang, South Korea\"]};function m(e){let t={a:\"a\",p:\"p\",...e.components};return(0,o.jsxs)(t.p,{children:[\"Existing multimodal retrieval systems often rely on disjointed models for image comprehension, such as object detectors and caption generators, leading to cumbersome implementations and training processes. To overcome this limitation, we propose an end-to-end retrieval system, Ret-XKnow, to endow a text retriever with the ability to understand multimodal queries via dynamic modality interaction. Ret-XKnow leverages a partial convolution mechanism to focus on visual information relevant to the given textual query, thereby enhancing multimodal query representations. To effectively learn multimodal interaction, we also introduce the Visual Dialogue-to-Retrieval (ViD2R) dataset automatically constructed from visual dialogue datasets. Our dataset construction process ensures that the dialogues are transformed into suitable information retrieval tasks using a text retriever. We demonstrate that our approach not only significantly improves retrieval performance in zero-shot settings but also achieves substantial improvements in fine-tuning scenarios. Our code is publicly available: \",(0,o.jsx)(t.a,{href:\"https://github.com/yeongjoonJu/Ret_XKnow\",children:\"https://github.com/yeongjoonJu/Ret_XKnow\"}),\".\"]})}function u(e={}){let{wrapper:t}=e.components||{};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(m,{...e})}):m(e)}return b(E);})();\n;return Component;"
    },
    "_id": "articles/2411.08334.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.08334.mdx",
      "sourceFileName": "2411.08334.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.08334"
    },
    "type": "Blog",
    "readingTime": {
      "text": "1 min read",
      "minutes": 0.705,
      "time": 42300,
      "words": 141
    },
    "slug": "2411.08334",
    "path": "articles/2411.08334",
    "filePath": "articles/2411.08334.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "Enhancing Multimodal Query Representation via Visual Dialogues for End-to-End Knowledge Retrieval",
      "datePublished": "2024-11-14T00:00:00.000Z",
      "dateModified": "2024-11-14T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.08334"
    }
  },
  {
    "title": "CorrSynth -- A Correlated Sampling Method for Diverse Dataset Generation from LLMs",
    "date": "2024-11-14T00:00:00.000Z",
    "tags": [
      "bias in data",
      "data synthesis",
      "student model",
      "diversity",
      "large language models",
      "correlated sampling",
      "prompt adherence",
      "dataset generation",
      "decoding-time guidance",
      "intrinsic evaluation"
    ],
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2411.08553",
    "arx_url": "https://arxiv.org/abs/2411.08553",
    "authors": [
      "Suhas S Kowshik",
      "Abhishek Divekar",
      "Vijit Malik"
    ],
    "affiliations_aligned": [
      "Amazon",
      "Amazon",
      "Amazon"
    ],
    "affiliations": [
      "Amazon"
    ],
    "problem": "lack of diversity in generated data",
    "solution": "CorrSynth method for diverse dataset generation",
    "score": 6,
    "body": {
      "raw": "\n\nLarge language models (LLMs) have demonstrated remarkable performance in diverse tasks using zero-shot and few-shot prompting. Even though their capabilities of data synthesis have been studied well in recent years, the generated data suffers from a lack of diversity, less adherence to the prompt, and potential biases that creep into the data from the generator model. In this work, we tackle the challenge of generating datasets with high diversity, upon which a student model is trained for downstream tasks. Taking the route of decoding-time guidance-based approaches, we propose CorrSynth, which generates data that is more diverse and faithful to the input prompt using a correlated sampling strategy. Further, our method overcomes the complexity drawbacks of some other guidance-based techniques like classifier-based guidance. With extensive experiments, we show the effectiveness of our approach and substantiate our claims. In particular, we perform intrinsic evaluation to show the improvements in diversity. Our experiments show that CorrSynth improves both student metrics and intrinsic metrics upon competitive baselines across four datasets, showing the innate advantage of our method.",
      "code": "var Component=(()=>{var l=Object.create;var i=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var v=(e,t)=>()=>(t||e((t={exports:{}}).exports,t),t.exports),w=(e,t)=>{for(var a in t)i(e,a,{get:t[a],enumerable:!0})},s=(e,t,a,n)=>{if(t&&typeof t==\"object\"||typeof t==\"function\")for(let r of u(t))!f.call(e,r)&&r!==a&&i(e,r,{get:()=>t[r],enumerable:!(n=p(t,r))||n.enumerable});return e};var b=(e,t,a)=>(a=e!=null?l(g(e)):{},s(t||!e||!e.__esModule?i(a,\"default\",{value:e,enumerable:!0}):a,e)),y=e=>s(i({},\"__esModule\",{value:!0}),e);var h=v((_,d)=>{d.exports=_jsx_runtime});var x={};w(x,{default:()=>m,frontmatter:()=>k});var o=b(h()),k={title:\"CorrSynth -- A Correlated Sampling Method for Diverse Dataset Generation from LLMs\",date:\"2024-11-14\",tags:[\"bias in data\",\"data synthesis\",\"student model\",\"diversity\",\"large language models\",\"correlated sampling\",\"prompt adherence\",\"dataset generation\",\"decoding-time guidance\",\"intrinsic evaluation\"],categories:[\"cs.CL\"],problem:\"lack of diversity in generated data\",solution:\"CorrSynth method for diverse dataset generation\",pdf_url:\"https://arxiv.org/pdf/2411.08553\",arx_url:\"https://arxiv.org/abs/2411.08553\",score:6,authors:[\"Suhas S Kowshik\",\"Abhishek Divekar\",\"Vijit Malik\"],affiliations_aligned:[\"Amazon\",\"Amazon\",\"Amazon\"],affiliations:[\"Amazon\"]};function c(e){let t={p:\"p\",...e.components};return(0,o.jsx)(t.p,{children:\"Large language models (LLMs) have demonstrated remarkable performance in diverse tasks using zero-shot and few-shot prompting. Even though their capabilities of data synthesis have been studied well in recent years, the generated data suffers from a lack of diversity, less adherence to the prompt, and potential biases that creep into the data from the generator model. In this work, we tackle the challenge of generating datasets with high diversity, upon which a student model is trained for downstream tasks. Taking the route of decoding-time guidance-based approaches, we propose CorrSynth, which generates data that is more diverse and faithful to the input prompt using a correlated sampling strategy. Further, our method overcomes the complexity drawbacks of some other guidance-based techniques like classifier-based guidance. With extensive experiments, we show the effectiveness of our approach and substantiate our claims. In particular, we perform intrinsic evaluation to show the improvements in diversity. Our experiments show that CorrSynth improves both student metrics and intrinsic metrics upon competitive baselines across four datasets, showing the innate advantage of our method.\"})}function m(e={}){let{wrapper:t}=e.components||{};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}return y(x);})();\n;return Component;"
    },
    "_id": "articles/2411.08553.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.08553.mdx",
      "sourceFileName": "2411.08553.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.08553"
    },
    "type": "Blog",
    "readingTime": {
      "text": "1 min read",
      "minutes": 0.865,
      "time": 51900,
      "words": 173
    },
    "slug": "2411.08553",
    "path": "articles/2411.08553",
    "filePath": "articles/2411.08553.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "CorrSynth -- A Correlated Sampling Method for Diverse Dataset Generation from LLMs",
      "datePublished": "2024-11-14T00:00:00.000Z",
      "dateModified": "2024-11-14T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.08553"
    }
  },
  {
    "title": "QCG-Rerank: Chunks Graph Rerank with Query Expansion in Retrieval-Augmented LLMs for Tourism Domain",
    "date": "2024-11-14T00:00:00.000Z",
    "tags": [
      "query expansion",
      "Retrieval-Augmented Generation",
      "similarity scores",
      "chunks graph",
      "Large Language Models",
      "evaluation datasets",
      "transition probabilities",
      "information retrieval techniques",
      "tourism domain"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2411.08724",
    "arx_url": "https://arxiv.org/abs/2411.08724",
    "authors": [
      "Qikai Wei",
      "Mingzhi Yang",
      "Chunlong Han",
      "Jingfu Wei",
      "Minghao Zhang",
      "Feifei Shi",
      "Huansheng Ning"
    ],
    "affiliations_aligned": [
      "a,b",
      "c",
      "a,b",
      "a,b",
      "a,b",
      "a,b",
      "a,b"
    ],
    "affiliations": [
      "a,b",
      "c"
    ],
    "problem": "irrelevant or contradictory information in retrieval-augmented generation",
    "solution": "QCG-Rerank model",
    "score": 6,
    "body": {
      "raw": "\n\nRetrieval-Augmented Generation (RAG) mitigates the issue of hallucination in Large Language Models (LLMs) by integrating information retrieval techniques. However, in the tourism domain, since the query is usually brief and the content in the database is diverse, existing RAG may contain a significant amount of irrelevant or contradictory information contents after retrieval. To address this challenge, we propose the QCG-Rerank model. This model first performs an initial retrieval to obtain candidate chunks and then enhances semantics by extracting critical information to expand the original query. Next, we utilize the expanded query and candidate chunks to calculate similarity scores as the initial transition probability and construct the chunks graph. Subsequently, We iteratively compute the transition probabilities based on an initial estimate until convergence. The chunks with the highest score are selected and input into the LLMs to generate responses. We evaluate the model on Cultour, IIRC, StrategyQA, HotpotQA, SQuAD, and MuSiQue datasets. The experimental results demonstrate the effectiveness and superiority of the QCG-Rerank method.",
      "code": "var Component=(()=>{var d=Object.create;var a=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var g=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var b=(e,t)=>()=>(t||e((t={exports:{}}).exports,t),t.exports),v=(e,t)=>{for(var n in t)a(e,n,{get:t[n],enumerable:!0})},s=(e,t,n,o)=>{if(t&&typeof t==\"object\"||typeof t==\"function\")for(let i of g(t))!f.call(e,i)&&i!==n&&a(e,i,{get:()=>t[i],enumerable:!(o=m(t,i))||o.enumerable});return e};var y=(e,t,n)=>(n=e!=null?d(p(e)):{},s(t||!e||!e.__esModule?a(n,\"default\",{value:e,enumerable:!0}):n,e)),x=e=>s(a({},\"__esModule\",{value:!0}),e);var l=b((M,u)=>{u.exports=_jsx_runtime});var L={};v(L,{default:()=>h,frontmatter:()=>k});var r=y(l()),k={title:\"QCG-Rerank: Chunks Graph Rerank with Query Expansion in Retrieval-Augmented LLMs for Tourism Domain\",date:\"2024-11-14\",tags:[\"query expansion\",\"Retrieval-Augmented Generation\",\"similarity scores\",\"chunks graph\",\"Large Language Models\",\"evaluation datasets\",\"transition probabilities\",\"information retrieval techniques\",\"tourism domain\"],categories:[\"cs.CL\",\"cs.AI\"],problem:\"irrelevant or contradictory information in retrieval-augmented generation\",solution:\"QCG-Rerank model\",pdf_url:\"https://arxiv.org/pdf/2411.08724\",arx_url:\"https://arxiv.org/abs/2411.08724\",score:6,authors:[\"Qikai Wei\",\"Mingzhi Yang\",\"Chunlong Han\",\"Jingfu Wei\",\"Minghao Zhang\",\"Feifei Shi\",\"Huansheng Ning\"],affiliations_aligned:[\"a,b\",\"c\",\"a,b\",\"a,b\",\"a,b\",\"a,b\",\"a,b\"],affiliations:[\"a,b\",\"c\"]};function c(e){let t={p:\"p\",...e.components};return(0,r.jsx)(t.p,{children:\"Retrieval-Augmented Generation (RAG) mitigates the issue of hallucination in Large Language Models (LLMs) by integrating information retrieval techniques. However, in the tourism domain, since the query is usually brief and the content in the database is diverse, existing RAG may contain a significant amount of irrelevant or contradictory information contents after retrieval. To address this challenge, we propose the QCG-Rerank model. This model first performs an initial retrieval to obtain candidate chunks and then enhances semantics by extracting critical information to expand the original query. Next, we utilize the expanded query and candidate chunks to calculate similarity scores as the initial transition probability and construct the chunks graph. Subsequently, We iteratively compute the transition probabilities based on an initial estimate until convergence. The chunks with the highest score are selected and input into the LLMs to generate responses. We evaluate the model on Cultour, IIRC, StrategyQA, HotpotQA, SQuAD, and MuSiQue datasets. The experimental results demonstrate the effectiveness and superiority of the QCG-Rerank method.\"})}function h(e={}){let{wrapper:t}=e.components||{};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}return x(L);})();\n;return Component;"
    },
    "_id": "articles/2411.08724.mdx",
    "_raw": {
      "sourceFilePath": "articles/2411.08724.mdx",
      "sourceFileName": "2411.08724.mdx",
      "sourceFileDir": "articles",
      "contentType": "mdx",
      "flattenedPath": "articles/2411.08724"
    },
    "type": "Blog",
    "readingTime": {
      "text": "1 min read",
      "minutes": 0.815,
      "time": 48900,
      "words": 163
    },
    "slug": "2411.08724",
    "path": "articles/2411.08724",
    "filePath": "articles/2411.08724.mdx",
    "toc": [],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "QCG-Rerank: Chunks Graph Rerank with Query Expansion in Retrieval-Augmented LLMs for Tourism Domain",
      "datePublished": "2024-11-14T00:00:00.000Z",
      "dateModified": "2024-11-14T00:00:00.000Z",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/2411.08724"
    }
  },
  {
    "title": "Sample .md file",
    "date": "2016-03-08T00:00:00.000Z",
    "tags": [
      "markdown",
      "code",
      "features"
    ],
    "categories": [],
    "draft": false,
    "summary": "Example of a markdown file with code blocks and syntax highlighting",
    "pdf_url": "https://arxiv.org/pdf/2411.00000",
    "arx_url": "https://arxiv.org/abs/2411.00000",
    "authors": [
      "Xin Wang",
      "Xinyi Bai"
    ],
    "affiliations_aligned": [],
    "affiliations": [
      "Henan Institute of Technology, China",
      "Henan Institute of Technology, China"
    ],
    "body": {
      "raw": "\nA sample post with markdown.\n\n## Inline Highlighting\n\nSample of inline highlighting `sum = parseInt(num1) + parseInt(num2)`\n\n## Code Blocks\n\nSome Javascript code\n\n```javascript\nvar num1, num2, sum\nnum1 = prompt('Enter first number')\nnum2 = prompt('Enter second number')\nsum = parseInt(num1) + parseInt(num2) // \"+\" means \"add\"\nalert('Sum = ' + sum) // \"+\" means combine into a string\n```\n\nSome Python code ð\n\n```python\ndef fib():\n    a, b = 0, 1\n    while True:            # First iteration:\n        yield a            # yield 0 to start with and then\n        a, b = b, a + b    # a will now be 1, and b will also be 1, (0 + 1)\n\nfor index, fibonacci_number in zip(range(10), fib()):\n     print('{i:3}: {f:3}'.format(i=index, f=fibonacci_number))\n```\n",
      "code": "var Component=(()=>{var h=Object.create;var l=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var N=Object.getPrototypeOf,k=Object.prototype.hasOwnProperty;var g=(e,n)=>()=>(n||e((n={exports:{}}).exports,n),n.exports),f=(e,n)=>{for(var s in n)l(e,s,{get:n[s],enumerable:!0})},i=(e,n,s,t)=>{if(n&&typeof n==\"object\"||typeof n==\"function\")for(let c of u(n))!k.call(e,c)&&c!==s&&l(e,c,{get:()=>n[c],enumerable:!(t=m(n,c))||t.enumerable});return e};var w=(e,n,s)=>(s=e!=null?h(N(e)):{},i(n||!e||!e.__esModule?l(s,\"default\",{value:e,enumerable:!0}):s,e)),b=e=>i(l({},\"__esModule\",{value:!0}),e);var r=g((_,o)=>{o.exports=_jsx_runtime});var y={};f(y,{default:()=>d,frontmatter:()=>x});var a=w(r()),x={title:\"Sample .md file\",date:\"2016-03-08\",tags:[\"markdown\",\"code\",\"features\"],draft:!1,pdf_url:\"https://arxiv.org/pdf/2411.00000\",arx_url:\"https://arxiv.org/abs/2411.00000\",authors:[\"Xin Wang\",\"Xinyi Bai\"],affiliations:[\"Henan Institute of Technology, China\",\"Henan Institute of Technology, China\"],summary:\"Example of a markdown file with code blocks and syntax highlighting\"};function p(e){let n={a:\"a\",code:\"code\",h2:\"h2\",p:\"p\",path:\"path\",pre:\"pre\",span:\"span\",svg:\"svg\",...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.p,{children:\"A sample post with markdown.\"}),(0,a.jsxs)(n.h2,{className:\"content-header\",id:\"inline-highlighting\",children:[(0,a.jsx)(n.a,{\"aria-hidden\":\"true\",href:\"#inline-highlighting\",tabIndex:\"-1\",children:(0,a.jsx)(a.Fragment,{children:(0,a.jsx)(n.span,{className:\"content-header-link\",children:(0,a.jsxs)(n.svg,{className:\"h-5 linkicon w-5\",fill:\"currentColor\",viewBox:\"0 0 20 20\",xmlns:\"http://www.w3.org/2000/svg\",children:[(0,a.jsx)(n.path,{d:\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}),(0,a.jsx)(n.path,{d:\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"})]})})})}),\"Inline Highlighting\"]}),(0,a.jsxs)(n.p,{children:[\"Sample of inline highlighting \",(0,a.jsx)(n.code,{children:\"sum = parseInt(num1) + parseInt(num2)\"})]}),(0,a.jsxs)(n.h2,{className:\"content-header\",id:\"code-blocks\",children:[(0,a.jsx)(n.a,{\"aria-hidden\":\"true\",href:\"#code-blocks\",tabIndex:\"-1\",children:(0,a.jsx)(a.Fragment,{children:(0,a.jsx)(n.span,{className:\"content-header-link\",children:(0,a.jsxs)(n.svg,{className:\"h-5 linkicon w-5\",fill:\"currentColor\",viewBox:\"0 0 20 20\",xmlns:\"http://www.w3.org/2000/svg\",children:[(0,a.jsx)(n.path,{d:\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}),(0,a.jsx)(n.path,{d:\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"})]})})})}),\"Code Blocks\"]}),(0,a.jsx)(n.p,{children:\"Some Javascript code\"}),(0,a.jsx)(n.pre,{className:\"language-javascript\",children:(0,a.jsxs)(n.code,{className:\"code-highlight language-javascript\",children:[(0,a.jsxs)(n.span,{className:\"code-line\",children:[(0,a.jsx)(n.span,{className:\"token keyword\",children:\"var\"}),\" num1\",(0,a.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" num2\",(0,a.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),` sum\n`]}),(0,a.jsxs)(n.span,{className:\"code-line\",children:[\"num1 \",(0,a.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" \",(0,a.jsx)(n.span,{className:\"token function\",children:\"prompt\"}),(0,a.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,a.jsx)(n.span,{className:\"token string\",children:\"'Enter first number'\"}),(0,a.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,a.jsxs)(n.span,{className:\"code-line\",children:[\"num2 \",(0,a.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" \",(0,a.jsx)(n.span,{className:\"token function\",children:\"prompt\"}),(0,a.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,a.jsx)(n.span,{className:\"token string\",children:\"'Enter second number'\"}),(0,a.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,a.jsxs)(n.span,{className:\"code-line\",children:[\"sum \",(0,a.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" \",(0,a.jsx)(n.span,{className:\"token function\",children:\"parseInt\"}),(0,a.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"num1\",(0,a.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),\" \",(0,a.jsx)(n.span,{className:\"token operator\",children:\"+\"}),\" \",(0,a.jsx)(n.span,{className:\"token function\",children:\"parseInt\"}),(0,a.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"num2\",(0,a.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),\" \",(0,a.jsx)(n.span,{className:\"token comment\",children:'// \"+\" means \"add\"'}),`\n`]}),(0,a.jsxs)(n.span,{className:\"code-line\",children:[(0,a.jsx)(n.span,{className:\"token function\",children:\"alert\"}),(0,a.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,a.jsx)(n.span,{className:\"token string\",children:\"'Sum = '\"}),\" \",(0,a.jsx)(n.span,{className:\"token operator\",children:\"+\"}),\" sum\",(0,a.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),\" \",(0,a.jsx)(n.span,{className:\"token comment\",children:'// \"+\" means combine into a string'}),`\n`]})]})}),(0,a.jsx)(n.p,{children:\"Some Python code \\u{1F40D}\"}),(0,a.jsx)(n.pre,{className:\"language-python\",children:(0,a.jsxs)(n.code,{className:\"code-highlight language-python\",children:[(0,a.jsxs)(n.span,{className:\"code-line\",children:[(0,a.jsx)(n.span,{className:\"token keyword\",children:\"def\"}),\" \",(0,a.jsx)(n.span,{className:\"token function\",children:\"fib\"}),(0,a.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,a.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,a.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),`\n`]}),(0,a.jsxs)(n.span,{className:\"code-line\",children:[\"    a\",(0,a.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" b \",(0,a.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" \",(0,a.jsx)(n.span,{className:\"token number\",children:\"0\"}),(0,a.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,a.jsx)(n.span,{className:\"token number\",children:\"1\"}),`\n`]}),(0,a.jsxs)(n.span,{className:\"code-line\",children:[\"    \",(0,a.jsx)(n.span,{className:\"token keyword\",children:\"while\"}),\" \",(0,a.jsx)(n.span,{className:\"token boolean\",children:\"True\"}),(0,a.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),\"            \",(0,a.jsx)(n.span,{className:\"token comment\",children:\"# First iteration:\"}),`\n`]}),(0,a.jsxs)(n.span,{className:\"code-line\",children:[\"        \",(0,a.jsx)(n.span,{className:\"token keyword\",children:\"yield\"}),\" a            \",(0,a.jsx)(n.span,{className:\"token comment\",children:\"# yield 0 to start with and then\"}),`\n`]}),(0,a.jsxs)(n.span,{className:\"code-line\",children:[\"        a\",(0,a.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" b \",(0,a.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" b\",(0,a.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" a \",(0,a.jsx)(n.span,{className:\"token operator\",children:\"+\"}),\" b    \",(0,a.jsx)(n.span,{className:\"token comment\",children:\"# a will now be 1, and b will also be 1, (0 + 1)\"}),`\n`]}),(0,a.jsx)(n.span,{className:\"code-line\",children:`\n`}),(0,a.jsxs)(n.span,{className:\"code-line\",children:[(0,a.jsx)(n.span,{className:\"token keyword\",children:\"for\"}),\" index\",(0,a.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" fibonacci_number \",(0,a.jsx)(n.span,{className:\"token keyword\",children:\"in\"}),\" \",(0,a.jsx)(n.span,{className:\"token builtin\",children:\"zip\"}),(0,a.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,a.jsx)(n.span,{className:\"token builtin\",children:\"range\"}),(0,a.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,a.jsx)(n.span,{className:\"token number\",children:\"10\"}),(0,a.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,a.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" fib\",(0,a.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,a.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,a.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,a.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),`\n`]}),(0,a.jsxs)(n.span,{className:\"code-line\",children:[\"     \",(0,a.jsx)(n.span,{className:\"token keyword\",children:\"print\"}),(0,a.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,a.jsx)(n.span,{className:\"token string\",children:\"'{i:3}: {f:3}'\"}),(0,a.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),(0,a.jsx)(n.span,{className:\"token builtin\",children:\"format\"}),(0,a.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"i\",(0,a.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\"index\",(0,a.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" f\",(0,a.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\"fibonacci_number\",(0,a.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,a.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]})]})})]})}function d(e={}){let{wrapper:n}=e.components||{};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(p,{...e})}):p(e)}return b(y);})();\n;return Component;"
    },
    "_id": "articles/nested-route/code-sample.mdx",
    "_raw": {
      "sourceFilePath": "articles/nested-route/code-sample.mdx",
      "sourceFileName": "code-sample.mdx",
      "sourceFileDir": "articles/nested-route",
      "contentType": "mdx",
      "flattenedPath": "articles/nested-route/code-sample"
    },
    "type": "Blog",
    "readingTime": {
      "text": "1 min read",
      "minutes": 0.595,
      "time": 35700,
      "words": 119
    },
    "slug": "nested-route/code-sample",
    "path": "articles/nested-route/code-sample",
    "filePath": "articles/nested-route/code-sample.mdx",
    "toc": [
      {
        "value": "Inline Highlighting",
        "url": "#inline-highlighting",
        "depth": 2
      },
      {
        "value": "Code Blocks",
        "url": "#code-blocks",
        "depth": 2
      }
    ],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "Sample .md file",
      "datePublished": "2016-03-08T00:00:00.000Z",
      "dateModified": "2016-03-08T00:00:00.000Z",
      "description": "Example of a markdown file with code blocks and syntax highlighting",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/nested-route/code-sample"
    }
  },
  {
    "title": "Introducing Multi-part Posts with Nested Routing",
    "date": "2021-05-02T00:00:00.000Z",
    "tags": [
      "multi-author",
      "next-js",
      "feature"
    ],
    "categories": [],
    "draft": false,
    "summary": "The blog template supports posts in nested sub-folders. This can be used to group posts of similar content e.g. a multi-part course. This post is itself an example of a nested route!",
    "pdf_url": "https://arxiv.org/pdf/2411.03163",
    "arx_url": "https://arxiv.org/abs/2411.03163",
    "authors": [],
    "affiliations_aligned": [],
    "affiliations": [],
    "body": {
      "raw": "\n# Nested Routes\n\nThe blog template supports posts in nested sub-folders. This helps in organisation and can be used to group posts of similar content e.g. a multi-part series. This post is itself an example of a nested route! It's located in the `/data/blog/nested-route` folder.\n\n## How\n\nSimplify create multiple folders inside the main `/data/articles` folder and add your `.md`/`.mdx` files to them. You can even create something like `/data/articles/nested-route/deeply-nested-route/my-post.md`\n\nWe use Next.js catch all routes to handle the routing and path creations.\n\n## Use Cases\n\nHere are some reasons to use nested routes\n\n- More logical content organisation (blogs will still be displayed based on the created date)\n- Multi-part posts\n- Different sub-routes for each author\n- Internationalization (though it would be recommended to use [Next.js built-in i8n routing](https://nextjs.org/docs/advanced-features/i18n-routing))\n\n## Note\n\n- The previous/next post links at bottom of the template are currently sorted by date. One could explore modifying the template to refer the reader to the previous/next post in the series, rather than by date.\n",
      "code": "var Component=(()=>{var u=Object.create;var r=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var f=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var x=(a,e)=>()=>(e||a((e={exports:{}}).exports,e),e.exports),w=(a,e)=>{for(var n in e)r(a,n,{get:e[n],enumerable:!0})},i=(a,e,n,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let l of m(e))!g.call(a,l)&&l!==n&&r(a,l,{get:()=>e[l],enumerable:!(s=p(e,l))||s.enumerable});return a};var b=(a,e,n)=>(n=a!=null?u(f(a)):{},i(e||!a||!a.__esModule?r(n,\"default\",{value:a,enumerable:!0}):n,a)),v=a=>i(r({},\"__esModule\",{value:!0}),a);var d=x((k,o)=>{o.exports=_jsx_runtime});var M={};w(M,{default:()=>c,frontmatter:()=>N});var t=b(d()),N={title:\"Introducing Multi-part Posts with Nested Routing\",date:\"2021-05-02\",tags:[\"multi-author\",\"next-js\",\"feature\"],draft:!1,pdf_url:\"https://arxiv.org/pdf/2411.03163\",arx_url:\"https://arxiv.org/abs/2411.03163\",summary:\"The blog template supports posts in nested sub-folders. This can be used to group posts of similar content e.g. a multi-part course. This post is itself an example of a nested route!\"};function h(a){let e={a:\"a\",code:\"code\",h1:\"h1\",h2:\"h2\",li:\"li\",p:\"p\",path:\"path\",span:\"span\",svg:\"svg\",ul:\"ul\",...a.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(e.h1,{className:\"content-header\",id:\"nested-routes\",children:[(0,t.jsx)(e.a,{href:\"#nested-routes\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,t.jsx)(t.Fragment,{children:(0,t.jsx)(e.span,{className:\"content-header-link\",children:(0,t.jsxs)(e.svg,{className:\"h-5 linkicon w-5\",fill:\"currentColor\",viewBox:\"0 0 20 20\",xmlns:\"http://www.w3.org/2000/svg\",children:[(0,t.jsx)(e.path,{d:\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}),(0,t.jsx)(e.path,{d:\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"})]})})})}),\"Nested Routes\"]}),(0,t.jsxs)(e.p,{children:[\"The blog template supports posts in nested sub-folders. This helps in organisation and can be used to group posts of similar content e.g. a multi-part series. This post is itself an example of a nested route! It's located in the \",(0,t.jsx)(e.code,{children:\"/data/blog/nested-route\"}),\" folder.\"]}),(0,t.jsxs)(e.h2,{className:\"content-header\",id:\"how\",children:[(0,t.jsx)(e.a,{href:\"#how\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,t.jsx)(t.Fragment,{children:(0,t.jsx)(e.span,{className:\"content-header-link\",children:(0,t.jsxs)(e.svg,{className:\"h-5 linkicon w-5\",fill:\"currentColor\",viewBox:\"0 0 20 20\",xmlns:\"http://www.w3.org/2000/svg\",children:[(0,t.jsx)(e.path,{d:\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}),(0,t.jsx)(e.path,{d:\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"})]})})})}),\"How\"]}),(0,t.jsxs)(e.p,{children:[\"Simplify create multiple folders inside the main \",(0,t.jsx)(e.code,{children:\"/data/articles\"}),\" folder and add your \",(0,t.jsx)(e.code,{children:\".md\"}),\"/\",(0,t.jsx)(e.code,{children:\".mdx\"}),\" files to them. You can even create something like \",(0,t.jsx)(e.code,{children:\"/data/articles/nested-route/deeply-nested-route/my-post.md\"})]}),(0,t.jsx)(e.p,{children:\"We use Next.js catch all routes to handle the routing and path creations.\"}),(0,t.jsxs)(e.h2,{className:\"content-header\",id:\"use-cases\",children:[(0,t.jsx)(e.a,{href:\"#use-cases\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,t.jsx)(t.Fragment,{children:(0,t.jsx)(e.span,{className:\"content-header-link\",children:(0,t.jsxs)(e.svg,{className:\"h-5 linkicon w-5\",fill:\"currentColor\",viewBox:\"0 0 20 20\",xmlns:\"http://www.w3.org/2000/svg\",children:[(0,t.jsx)(e.path,{d:\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}),(0,t.jsx)(e.path,{d:\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"})]})})})}),\"Use Cases\"]}),(0,t.jsx)(e.p,{children:\"Here are some reasons to use nested routes\"}),(0,t.jsxs)(e.ul,{children:[(0,t.jsx)(e.li,{children:\"More logical content organisation (blogs will still be displayed based on the created date)\"}),(0,t.jsx)(e.li,{children:\"Multi-part posts\"}),(0,t.jsx)(e.li,{children:\"Different sub-routes for each author\"}),(0,t.jsxs)(e.li,{children:[\"Internationalization (though it would be recommended to use \",(0,t.jsx)(e.a,{href:\"https://nextjs.org/docs/advanced-features/i18n-routing\",children:\"Next.js built-in i8n routing\"}),\")\"]})]}),(0,t.jsxs)(e.h2,{className:\"content-header\",id:\"note\",children:[(0,t.jsx)(e.a,{href:\"#note\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,t.jsx)(t.Fragment,{children:(0,t.jsx)(e.span,{className:\"content-header-link\",children:(0,t.jsxs)(e.svg,{className:\"h-5 linkicon w-5\",fill:\"currentColor\",viewBox:\"0 0 20 20\",xmlns:\"http://www.w3.org/2000/svg\",children:[(0,t.jsx)(e.path,{d:\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}),(0,t.jsx)(e.path,{d:\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"})]})})})}),\"Note\"]}),(0,t.jsx)(e.ul,{children:(0,t.jsx)(e.li,{children:\"The previous/next post links at bottom of the template are currently sorted by date. One could explore modifying the template to refer the reader to the previous/next post in the series, rather than by date.\"})})]})}function c(a={}){let{wrapper:e}=a.components||{};return e?(0,t.jsx)(e,{...a,children:(0,t.jsx)(h,{...a})}):h(a)}return v(M);})();\n;return Component;"
    },
    "_id": "articles/nested-route/introducing-multi-part-posts-with-nested-routing.mdx",
    "_raw": {
      "sourceFilePath": "articles/nested-route/introducing-multi-part-posts-with-nested-routing.mdx",
      "sourceFileName": "introducing-multi-part-posts-with-nested-routing.mdx",
      "sourceFileDir": "articles/nested-route",
      "contentType": "mdx",
      "flattenedPath": "articles/nested-route/introducing-multi-part-posts-with-nested-routing"
    },
    "type": "Blog",
    "readingTime": {
      "text": "1 min read",
      "minutes": 0.845,
      "time": 50700,
      "words": 169
    },
    "slug": "nested-route/introducing-multi-part-posts-with-nested-routing",
    "path": "articles/nested-route/introducing-multi-part-posts-with-nested-routing",
    "filePath": "articles/nested-route/introducing-multi-part-posts-with-nested-routing.mdx",
    "toc": [
      {
        "value": "Nested Routes",
        "url": "#nested-routes",
        "depth": 1
      },
      {
        "value": "How",
        "url": "#how",
        "depth": 2
      },
      {
        "value": "Use Cases",
        "url": "#use-cases",
        "depth": 2
      },
      {
        "value": "Note",
        "url": "#note",
        "depth": 2
      }
    ],
    "structuredData": {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "Introducing Multi-part Posts with Nested Routing",
      "datePublished": "2021-05-02T00:00:00.000Z",
      "dateModified": "2021-05-02T00:00:00.000Z",
      "description": "The blog template supports posts in nested sub-folders. This can be used to group posts of similar content e.g. a multi-part course. This post is itself an example of a nested route!",
      "image": "/static/images/twitter-card.png",
      "url": "https://ai-heap.com/articles/nested-route/introducing-multi-part-posts-with-nested-routing"
    }
  }
]